{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e740123",
   "metadata": {},
   "source": [
    "# Experimentos: TranAD+ y VLT-Anomaly — Preprocesamiento y EDA (Salida preservada)\n",
    "\n",
    "**Autor:** Javier Moreno  \n",
    "\n",
    "**Semilla global:** 42  \n",
    "\n",
    "**Fecha de exportación:** 2025-10-20 17:28 UTC\n",
    "\n",
    "Esta versión conserva las **salidas originales** de las celdas relevantes. \n",
    "Se eliminaron únicamente importaciones duplicadas y pruebas exploratorias redundantes (e.g., `head()`, `shape`) repetidas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d12d9d",
   "metadata": {},
   "source": [
    "### Semillas y dispositivo de ejecución\n",
    "\n",
    "Se fijan semillas globales para garantizar reproducibilidad (*seed = 42*). Además, se detecta el dispositivo disponible en esta máquina (CPU / **Apple Silicon – MPS** / GPU) para ajustar la ejecución sin modificar el resto del cuaderno.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60031105",
   "metadata": {},
   "source": [
    "**Ejecución auxiliar.**\n",
    "\n",
    "Celda de apoyo para operaciones intermedias del flujo experimental.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe57fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijación de semillas y detección de dispositivo (robusto a entornos sin PyTorch)\n",
    "SEED = 42\n",
    "\n",
    "# Semillas en librerías estándar\n",
    "try:\n",
    "    import random\n",
    "    random.seed(SEED)\n",
    "except Exception as e:\n",
    "    print(\"random no disponible:\", e)\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    np.random.seed(SEED)\n",
    "except Exception as e:\n",
    "    print(\"numpy no disponible:\", e)\n",
    "\n",
    "# Semillas/Dispositivo en PyTorch (si está disponible)\n",
    "device = \"cpu\"\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"   # Apple Silicon (M1/M2/M3/M4)\n",
    "    print(\"PyTorch:\", torch.__version__, \"| Dispositivo:\", device)\n",
    "except Exception as e:\n",
    "    print(\"PyTorch no disponible:\", e)\n",
    "    print(\"Dispositivo:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e129dc",
   "metadata": {},
   "source": [
    "## 1. Configuración del entorno y dependencias\n",
    "\n",
    "*Importaciones, versión de librerías y dispositivo (Apple Silicon M4 si aplica).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e237d",
   "metadata": {},
   "source": [
    "## 2. Carga de datos y descripción del dataset\n",
    "\n",
    "*Estructura y tamaños sin exponer rutas internas.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fde5c1",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento y reducción de características\n",
    "\n",
    "*Normalización, ventanas temporales y selección/reducción de features.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a694d",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento y evaluación: TranAD+ (pruebas base)\n",
    "\n",
    "*Métricas clave (F1, Precision, Recall, AUC) y notas de configuración.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd4653",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento y evaluación: VLT-Anomaly (pruebas base)\n",
    "\n",
    "*Comparación frente a TranAD+ con el mismo split/semilla.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8454d7",
   "metadata": {},
   "source": [
    "## 6. Resultados resumidos y observaciones\n",
    "\n",
    "*Hallazgos clave y referencia a tablas extendidas (Anexo V).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061a305-6f1d-4731-b221-5a7136ba681b",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78adf7",
   "metadata": {},
   "source": [
    "**Preprocesamiento y selección/reducción de características.**\n",
    "\n",
    "Se normalizan variables y se ajusta el conjunto de *features* para estabilizar el entrenamiento y evitar ruido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8082c353-b20a-4d05-9e7f-8a063e18d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Original\n",
    "#     │\n",
    "#     ├─► Separar columnas metadata: ['Timestamp', 'Label', etc.]\n",
    "#     │\n",
    "#     ├─► Procesamiento de features (limpieza, escalado, correlación)\n",
    "#     │\n",
    "#     └─► Reconstruir dataset final:\n",
    "#           [Timestamp] + [y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c5813f",
   "metadata": {},
   "source": [
    "**Carga de datos y verificación inicial.**\n",
    "\n",
    "Se cargan los datasets de trabajo y se inspeccionan estructuras básicas para confirmar formato y tamaños.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35da5261-37f2-43c0-a58e-67478246eedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Configurando rutas y cargando archivos CSV de CIC-IDS2017 ---\n",
      "Cargando Monday-WorkingHours.pcap_ISCX.csv...\n",
      "Cargando Tuesday-WorkingHours.pcap_ISCX.csv...\n",
      "Cargando Wednesday-workingHours.pcap_ISCX.csv...\n",
      "Cargando Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv...\n",
      "Cargando Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv...\n",
      "Cargando Friday-WorkingHours-Morning.pcap_ISCX.csv...\n",
      "Cargando Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv...\n",
      "Cargando Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv...\n",
      "Dataset CIC-IDS2017 cargado. Filas totales iniciales: 3119345\n",
      "\n",
      "--- 2. Limpieza y Preprocesamiento de Datos ---\n",
      "Filas eliminadas por NaN/Inf: 291469\n",
      "Filas eliminadas por duplicados: 199\n",
      "Filas eliminadas por conversión fallida de Timestamp: 0\n",
      "\n",
      "Dataset después de limpieza: Filas=2827677, Columnas=87\n",
      "Distribución de etiquetas final:\n",
      "Label\n",
      "BENIGN                        2271122\n",
      "DoS Hulk                       230123\n",
      "PortScan                       158804\n",
      "DDoS                           128025\n",
      "DoS GoldenEye                   10293\n",
      "FTP-Patator                      7935\n",
      "SSH-Patator                      5897\n",
      "DoS slowloris                    5796\n",
      "DoS Slowhttptest                 5499\n",
      "Bot                              1956\n",
      "Web Attack - Brute Force         1507\n",
      "Web Attack - XSS                  652\n",
      "Infiltration                       36\n",
      "Web Attack - Sql Injection         21\n",
      "Heartbleed                         11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- 3. Preparación de DataFrames para Fine-tuning ---\n",
      "df_context creado con 2827677 filas.\n",
      "df_features_scaled creado con 2827677 filas.\n",
      "\n",
      "--- 4. Calculando Estadísticas Base (BENIGN) ---\n",
      "Estadísticas de tráfico BENIGN calculadas.\n",
      "\n",
      "--- 5. Automatizando la selección de KEY_FEATURES_BY_ATTACK_TYPE ---\n",
      "Procesando tipo de ataque: FTP-Patator\n",
      "Procesando tipo de ataque: SSH-Patator\n",
      "Procesando tipo de ataque: DoS slowloris\n",
      "Procesando tipo de ataque: DoS Slowhttptest\n",
      "Procesando tipo de ataque: DoS Hulk\n",
      "Procesando tipo de ataque: DoS GoldenEye\n",
      "Procesando tipo de ataque: Heartbleed\n",
      "Procesando tipo de ataque: Web Attack - Brute Force\n",
      "Procesando tipo de ataque: Web Attack - XSS\n",
      "Procesando tipo de ataque: Web Attack - Sql Injection\n",
      "Procesando tipo de ataque: Infiltration\n",
      "Procesando tipo de ataque: Bot\n",
      "Procesando tipo de ataque: PortScan\n",
      "Procesando tipo de ataque: DDoS\n",
      "\n",
      "Diccionario KEY_FEATURES_BY_ATTACK_TYPE actualizado automáticamente:\n",
      "  FTP-Patator: 40 features\n",
      "  SSH-Patator: 40 features\n",
      "  DoS slowloris: 40 features\n",
      "  DoS Slowhttptest: 40 features\n",
      "  DoS Hulk: 40 features\n",
      "  DoS GoldenEye: 40 features\n",
      "  Heartbleed: 40 features\n",
      "  Web Attack - Brute Force: 40 features\n",
      "  Web Attack - XSS: 40 features\n",
      "  Web Attack - Sql Injection: 40 features\n",
      "  Infiltration: 40 features\n",
      "  Bot: 40 features\n",
      "  PortScan: 40 features\n",
      "  DDoS: 40 features\n",
      "  BENIGN: 0 features\n",
      "  Label: 0 features\n",
      "\n",
      "--- 6. Definiendo la función para generar el resumen numérico ---\n",
      "Función 'get_numeric_summary_for_incident' definida.\n",
      "\n",
      "--- 7. Ejemplo de Uso ---\n",
      "Generando resumen numérico para un incidente de tipo: PortScan\n",
      "{\n",
      "  \"PSH_Flag_Count\": \"Elevated (1.00), higher than normal (0.26).\",\n",
      "  \"Bwd_Packets/s\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Bwd_Packet_Length_Mean\": \"Near normal (0.05), typical for benign traffic (0.03).\",\n",
      "  \"Bwd_Packet_Length_Min\": \"Near normal (0.00), typical for benign traffic (0.02).\",\n",
      "  \"Total_Fwd_Packets\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Avg_Bwd_Segment_Size\": \"Near normal (0.05), typical for benign traffic (0.03).\",\n",
      "  \"Packet_Length_Mean\": \"Near normal (0.05), typical for benign traffic (0.03).\",\n",
      "  \"Packet_Length_Std\": \"Near normal (0.07), typical for benign traffic (0.03).\",\n",
      "  \"Subflow_Fwd_Bytes\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Init_Win_bytes_forward\": \"Elevated (0.45), higher than normal (0.11).\",\n",
      "  \"Subflow_Fwd_Packets\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Bwd_Packet_Length_Max\": \"Near normal (0.05), typical for benign traffic (0.02).\",\n",
      "  \"ACK_Flag_Count\": \"Near normal (0.00), typical for benign traffic (0.29).\",\n",
      "  \"Fwd_Packets/s\": \"Near normal (0.00), typical for benign traffic (0.02).\",\n",
      "  \"act_data_pkt_fwd\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Flow_Bytes/s\": \"Near normal (0.11), typical for benign traffic (0.11).\",\n",
      "  \"Average_Packet_Size\": \"Near normal (0.05), typical for benign traffic (0.03).\",\n",
      "  \"Fwd_Packet_Length_Mean\": \"Near normal (0.02), typical for benign traffic (0.01).\",\n",
      "  \"Total_Backward_Packets\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Fwd_Packet_Length_Max\": \"Near normal (0.01), typical for benign traffic (0.01).\",\n",
      "  \"Avg_Fwd_Segment_Size\": \"Near normal (0.02), typical for benign traffic (0.01).\",\n",
      "  \"Subflow_Bwd_Packets\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Destination_Port\": \"Near normal (0.00), typical for benign traffic (0.14).\",\n",
      "  \"Fwd_Packet_Length_Min\": \"Near normal (0.00), typical for benign traffic (0.01).\",\n",
      "  \"URG_Flag_Count\": \"Near normal (0.00), typical for benign traffic (0.12).\",\n",
      "  \"Down/Up_Ratio\": \"Reduced (0.00), lower than normal (0.00).\",\n",
      "  \"Flow_Packets/s\": \"Near normal (0.33), typical for benign traffic (0.34).\",\n",
      "  \"Packet_Length_Variance\": \"Near normal (0.00), typical for benign traffic (0.01).\",\n",
      "  \"Source_Port\": \"Near normal (0.62), typical for benign traffic (0.61).\",\n",
      "  \"Flow_Duration\": \"Near normal (0.04), typical for benign traffic (0.09).\",\n",
      "  \"Init_Win_bytes_backward\": \"Near normal (0.00), typical for benign traffic (0.04).\",\n",
      "  \"Fwd_IAT_Max\": \"Near normal (0.00), typical for benign traffic (0.04).\",\n",
      "  \"Fwd_IAT_Mean\": \"Near normal (0.00), typical for benign traffic (0.02).\",\n",
      "  \"Total_Length_of_Fwd_Packets\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Bwd_IAT_Min\": \"Near normal (0.00), typical for benign traffic (0.01).\",\n",
      "  \"Flow_IAT_Std\": \"Near normal (0.02), typical for benign traffic (0.02).\",\n",
      "  \"Fwd_IAT_Min\": \"Near normal (0.00), typical for benign traffic (0.01).\",\n",
      "  \"Flow_IAT_Max\": \"Near normal (0.04), typical for benign traffic (0.04).\",\n",
      "  \"Max_Packet_Length\": \"Near normal (0.04), typical for benign traffic (0.02).\",\n",
      "  \"Flow_IAT_Min\": \"Near normal (0.00), typical for benign traffic (0.00).\"\n",
      "}\n",
      "\n",
      "--- 8. Guardando artefactos generados ---\n",
      "\n",
      "Datos preprocesados y diccionario de características guardados en: /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/preprocessed_data_for_llm\n",
      "Pipeline completado.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Pipeline Completo: Preparación de CIC-IDS2017 y Análisis de Características\n",
    "# \n",
    "# Este notebook contiene el código unificado para:\n",
    "# 1. Cargar y limpiar los datos del dataset CIC-IDS2017.\n",
    "# 2. Escalar las características numéricas.\n",
    "# 3. Calcular estadísticas base para el tráfico benigno.\n",
    "# 4. Realizar un análisis de importancia de características para identificar las métricas más relevantes por tipo de ataque.\n",
    "# 5. Automatizar la creación del diccionario `KEY_FEATURES_BY_ATTACK_TYPE`.\n",
    "# 6. Definir una función para generar el `key_numeric_summary` para los incidentes.\n",
    "# 7. Guardar los artefactos generados (DataFrames y el diccionario de características) para su uso futuro.\n",
    "\n",
    "# %%\n",
    "# --- 0. Importaciones y Configuración ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suprimir advertencias para una salida más limpia\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Configuración de Rutas y Carga de Archivos ---\n",
    "print(\"--- 1. Configurando rutas y cargando archivos CSV de CIC-IDS2017 ---\")\n",
    "\n",
    "# RUTA A LA UBICACIÓN REAL Y ABSOLUTA DE TU CARPETA CIC_TrafficLabelling\n",
    "cic_base_path = '/Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling'\n",
    "\n",
    "csv_files = [\n",
    "    'Monday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Wednesday-workingHours.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "]\n",
    "\n",
    "all_dfs = []\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(cic_base_path, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Cargando {file_name}...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='latin1', low_memory=False)\n",
    "            df.columns = df.columns.str.strip()\n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar {file_name}: {e}. Saltando.\")\n",
    "    else:\n",
    "        print(f\"Advertencia: Archivo no encontrado en {file_path}. Saltando.\")\n",
    "\n",
    "if not all_dfs:\n",
    "    raise FileNotFoundError(\"No se encontró ningún archivo CSV de CIC-IDS2017. Verifica la ruta y los nombres de archivo.\")\n",
    "\n",
    "df_cic = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"Dataset CIC-IDS2017 cargado. Filas totales iniciales: {len(df_cic)}\")\n",
    "\n",
    "# --- 2. Limpieza y Preprocesamiento de Datos ---\n",
    "print(\"\\n--- 2. Limpieza y Preprocesamiento de Datos ---\")\n",
    "\n",
    "df_cic.columns = df_cic.columns.str.replace(' ', '_')\n",
    "\n",
    "initial_rows = len(df_cic)\n",
    "df_cic.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_cic.dropna(inplace=True)\n",
    "print(f\"Filas eliminadas por NaN/Inf: {initial_rows - len(df_cic)}\")\n",
    "\n",
    "initial_rows = len(df_cic)\n",
    "df_cic.drop_duplicates(inplace=True)\n",
    "print(f\"Filas eliminadas por duplicados: {initial_rows - len(df_cic)}\")\n",
    "\n",
    "if 'Timestamp' in df_cic.columns:\n",
    "    df_cic['Timestamp_DT'] = pd.to_datetime(df_cic['Timestamp'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "    df_cic['Timestamp_DT'] = df_cic['Timestamp_DT'].fillna(\n",
    "        pd.to_datetime(df_cic['Timestamp'], format='%d/%m/%Y %H:%M', errors='coerce')\n",
    "    )\n",
    "    initial_rows_ts = len(df_cic)\n",
    "    df_cic.dropna(subset=['Timestamp_DT'], inplace=True)\n",
    "    print(f\"Filas eliminadas por conversión fallida de Timestamp: {initial_rows_ts - len(df_cic)}\")\n",
    "    df_cic['Timestamp_Unix'] = df_cic['Timestamp_DT'].astype(np.int64) // 10**9\n",
    "else:\n",
    "    print(\"Columna 'Timestamp' no encontrada en el dataset.\")\n",
    "\n",
    "numeric_features_cols = [col for col in df_cic.columns if pd.api.types.is_numeric_dtype(df_cic[col]) and col not in ['Flow_ID', 'Source_IP', 'Destination_IP', 'Timestamp', 'Timestamp_DT', 'Timestamp_Unix', 'Label']]\n",
    "for col in numeric_features_cols:\n",
    "    df_cic[col] = pd.to_numeric(df_cic[col], errors='coerce')\n",
    "    df_cic[col] = df_cic[col].astype(float)\n",
    "df_cic.fillna(0, inplace=True)\n",
    "\n",
    "df_cic['Label'] = df_cic['Label'].astype(str).str.strip()\n",
    "df_cic['Label'] = df_cic['Label'].replace({\n",
    "    'Web Attack \\x96 Brute Force': 'Web Attack - Brute Force',\n",
    "    'Web Attack \\x96 XSS': 'Web Attack - XSS',\n",
    "    'Web Attack \\x96 Sql Injection': 'Web Attack - Sql Injection'\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset después de limpieza: Filas={df_cic.shape[0]}, Columnas={df_cic.shape[1]}\")\n",
    "print(\"Distribución de etiquetas final:\")\n",
    "print(df_cic['Label'].value_counts())\n",
    "\n",
    "# --- 3. Preparación de DataFrames para Fine-tuning ---\n",
    "print(\"\\n--- 3. Preparación de DataFrames para Fine-tuning ---\")\n",
    "\n",
    "CONTEXT_COLS_FINAL = ['Flow_ID', 'Source_IP', 'Destination_IP', 'Timestamp', 'Timestamp_Unix', 'Label']\n",
    "df_context = df_cic[CONTEXT_COLS_FINAL].copy()\n",
    "\n",
    "df_features_to_scale = df_cic[numeric_features_cols].copy()\n",
    "scaler = MinMaxScaler()\n",
    "df_features_scaled = df_features_to_scale.copy()\n",
    "df_features_scaled[numeric_features_cols] = scaler.fit_transform(df_features_to_scale[numeric_features_cols])\n",
    "df_features_scaled['Label'] = df_cic['Label']\n",
    "df_features_scaled['Timestamp_Unix'] = df_cic['Timestamp_Unix']\n",
    "\n",
    "print(f\"df_context creado con {len(df_context)} filas.\")\n",
    "print(f\"df_features_scaled creado con {len(df_features_scaled)} filas.\")\n",
    "\n",
    "# --- 4. Cálculo de Estadísticas Base (BENIGN) ---\n",
    "print(\"\\n--- 4. Calculando Estadísticas Base (BENIGN) ---\")\n",
    "\n",
    "df_benign = df_features_scaled[df_features_scaled['Label'] == 'BENIGN']\n",
    "numeric_cols_for_stats = [col for col in numeric_features_cols if col in df_benign.columns]\n",
    "benign_stats = df_benign[numeric_cols_for_stats].agg(['mean', 'std']).transpose()\n",
    "benign_stats.rename(columns={'mean': 'benign_mean', 'std': 'benign_std'}, inplace=True)\n",
    "print(\"Estadísticas de tráfico BENIGN calculadas.\")\n",
    "\n",
    "# --- 5. Análisis de Importancia de Características y Automatización de KEY_FEATURES_BY_ATTACK_TYPE ---\n",
    "print(\"\\n--- 5. Automatizando la selección de KEY_FEATURES_BY_ATTACK_TYPE ---\")\n",
    "\n",
    "KEY_FEATURES_BY_ATTACK_TYPE = {}\n",
    "attack_types = [at for at in df_features_scaled['Label'].unique() if at != 'BENIGN']\n",
    "NUM_TOP_FEATURES_PER_ATTACK = 40\n",
    "\n",
    "# Muestreo para eficiencia en el entrenamiento de los clasificadores\n",
    "sample_size = min(500000, len(df_features_scaled))\n",
    "df_sample = df_features_scaled.sample(n=sample_size, random_state=42)\n",
    "\n",
    "for attack_type in attack_types:\n",
    "    print(f\"Procesando tipo de ataque: {attack_type}\")\n",
    "    \n",
    "    # Crear un dataset binario para el tipo de ataque actual (ataque vs. el resto)\n",
    "    df_binary = df_sample.copy()\n",
    "    \n",
    "    if attack_type not in df_binary['Label'].unique():\n",
    "        print(f\"  Advertencia: Tipo de ataque '{attack_type}' no encontrado en el sample. Saltando.\")\n",
    "        KEY_FEATURES_BY_ATTACK_TYPE[attack_type] = []\n",
    "        continue\n",
    "\n",
    "    y_binary = (df_binary['Label'] == attack_type).astype(int)\n",
    "    \n",
    "    # Asegurarse de que haya al menos 2 clases en el target\n",
    "    if y_binary.nunique() < 2:\n",
    "        print(f\"  Advertencia: Solo una clase presente para '{attack_type}' en el sample. Saltando.\")\n",
    "        KEY_FEATURES_BY_ATTACK_TYPE[attack_type] = []\n",
    "        continue\n",
    "        \n",
    "    rf_classifier_binary = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf_classifier_binary.fit(df_binary[numeric_features_cols], y_binary)\n",
    "    \n",
    "    feature_importances_binary = pd.DataFrame({\n",
    "        'Feature': numeric_features_cols,\n",
    "        'Importance': rf_classifier_binary.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    top_features = feature_importances_binary['Feature'].head(NUM_TOP_FEATURES_PER_ATTACK).tolist()\n",
    "    KEY_FEATURES_BY_ATTACK_TYPE[attack_type] = top_features\n",
    "\n",
    "KEY_FEATURES_BY_ATTACK_TYPE['BENIGN'] = []\n",
    "KEY_FEATURES_BY_ATTACK_TYPE['Label'] = []\n",
    "\n",
    "print(\"\\nDiccionario KEY_FEATURES_BY_ATTACK_TYPE actualizado automáticamente:\")\n",
    "for attack_type, features in KEY_FEATURES_BY_ATTACK_TYPE.items():\n",
    "    print(f\"  {attack_type}: {len(features)} features\")\n",
    "\n",
    "# --- 6. Definición de la Función para Generar `key_numeric_summary` ---\n",
    "print(\"\\n--- 6. Definiendo la función para generar el resumen numérico ---\")\n",
    "\n",
    "def get_numeric_summary_for_incident(incident_df_row, benign_stats_df, attack_type, num_format='scaled'):\n",
    "    summary = {}\n",
    "    features_to_summarize = KEY_FEATURES_BY_ATTACK_TYPE.get(attack_type, [])\n",
    "    features_to_summarize = [f for f in features_to_summarize if f in incident_df_row and f in benign_stats_df.index]\n",
    "\n",
    "    if not features_to_summarize:\n",
    "        return {\"No significant key features found for this incident.\": \"\"}\n",
    "\n",
    "    for feature in features_to_summarize:\n",
    "        incident_value = incident_df_row[feature]\n",
    "        benign_mean = benign_stats_df.loc[feature, 'benign_mean']\n",
    "        benign_std = benign_stats_df.loc[feature, 'benign_std']\n",
    "        \n",
    "        description = \"\"\n",
    "        if num_format == 'scaled':\n",
    "            if incident_value > benign_mean + 3 * benign_std:\n",
    "                description = f\"Significantly high ({incident_value:.2f}), much higher than normal ({benign_mean:.2f}).\"\n",
    "            elif incident_value < benign_mean - 3 * benign_std:\n",
    "                description = f\"Significantly low ({incident_value:.2f}), much lower than normal ({benign_mean:.2f}).\"\n",
    "            elif incident_value > benign_mean + 1 * benign_std:\n",
    "                description = f\"Elevated ({incident_value:.2f}), higher than normal ({benign_mean:.2f}).\"\n",
    "            elif incident_value < benign_mean - 1 * benign_std:\n",
    "                description = f\"Reduced ({incident_value:.2f}), lower than normal ({benign_mean:.2f}).\"\n",
    "            else:\n",
    "                description = f\"Near normal ({incident_value:.2f}), typical for benign traffic ({benign_mean:.2f}).\"\n",
    "        summary[feature] = description\n",
    "    return summary\n",
    "\n",
    "print(\"Función 'get_numeric_summary_for_incident' definida.\")\n",
    "\n",
    "# --- 7. Ejemplo de Uso de la Función ---\n",
    "print(\"\\n--- 7. Ejemplo de Uso ---\")\n",
    "\n",
    "# Buscar la primera fila que sea un 'PortScan'\n",
    "try:\n",
    "    first_portscan_index = df_features_scaled[df_features_scaled['Label'] == 'PortScan'].index[0]\n",
    "    incident_example_row = df_features_scaled.loc[first_portscan_index]\n",
    "    attack_label = incident_example_row['Label']\n",
    "    print(f\"Generando resumen numérico para un incidente de tipo: {attack_label}\")\n",
    "    numeric_summary = get_numeric_summary_for_incident(incident_example_row, benign_stats, attack_label, num_format='scaled')\n",
    "    print(json.dumps(numeric_summary, indent=2))\n",
    "except IndexError:\n",
    "    print(\"No se encontraron incidentes de tipo 'PortScan' para el ejemplo.\")\n",
    "\n",
    "\n",
    "# --- 8. Guardar Artefactos Generados ---\n",
    "print(\"\\n--- 8. Guardando artefactos generados ---\")\n",
    "\n",
    "output_dir_save = os.path.join(cic_base_path, 'preprocessed_data_for_llm')\n",
    "os.makedirs(output_dir_save, exist_ok=True)\n",
    "\n",
    "# Guardar DataFrames procesados\n",
    "df_context.to_csv(os.path.join(output_dir_save, 'cic_context_data_for_llm.csv'), index=True)\n",
    "df_features_scaled.to_csv(os.path.join(output_dir_save, 'cic_features_scaled_for_llm.csv'), index=True)\n",
    "\n",
    "# Guardar diccionario de características\n",
    "key_features_path = os.path.join(output_dir_save, 'key_features_by_attack_type.json')\n",
    "with open(key_features_path, 'w') as f:\n",
    "    json.dump(KEY_FEATURES_BY_ATTACK_TYPE, f, indent=4)\n",
    "\n",
    "print(f\"\\nDatos preprocesados y diccionario de características guardados en: {output_dir_save}\")\n",
    "print(\"Pipeline completado.\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c4d4c",
   "metadata": {},
   "source": [
    "**Carga de datos y verificación inicial.**\n",
    "\n",
    "Se cargan los datasets de trabajo y se inspeccionan estructuras básicas para confirmar formato y tamaños.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b6feee-0660-4cbb-acca-58bbbf9f7b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Configurando rutas y cargando archivos CSV de CIC-IDS2017 ---\n",
      "Cargando Monday-WorkingHours.pcap_ISCX.csv...\n",
      "Cargando Tuesday-WorkingHours.pcap_ISCX.csv...\n",
      "Cargando Wednesday-workingHours.pcap_ISCX.csv...\n",
      "Cargando Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv...\n",
      "Cargando Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv...\n",
      "Cargando Friday-WorkingHours-Morning.pcap_ISCX.csv...\n",
      "Cargando Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv...\n",
      "Cargando Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv...\n",
      "Dataset CIC-IDS2017 cargado. Filas totales iniciales: 3119345\n",
      "\n",
      "--- 2. Limpieza y Preprocesamiento de Datos ---\n",
      "Filas eliminadas por NaN/Inf: 291469\n",
      "Filas eliminadas por duplicados: 199\n",
      "Filas eliminadas por conversión fallida de Timestamp: 0\n",
      "\n",
      "Dataset después de limpieza: Filas=2827677, Columnas=87\n",
      "Distribución de etiquetas final:\n",
      "Label\n",
      "BENIGN                        2271122\n",
      "DoS Hulk                       230123\n",
      "PortScan                       158804\n",
      "DDoS                           128025\n",
      "DoS GoldenEye                   10293\n",
      "FTP-Patator                      7935\n",
      "SSH-Patator                      5897\n",
      "DoS slowloris                    5796\n",
      "DoS Slowhttptest                 5499\n",
      "Bot                              1956\n",
      "Web Attack - Brute Force         1507\n",
      "Web Attack - XSS                  652\n",
      "Infiltration                       36\n",
      "Web Attack - Sql Injection         21\n",
      "Heartbleed                         11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- 3. Preparación de DataFrames para Fine-tuning ---\n",
      "df_context creado con 2827677 filas.\n",
      "df_features_scaled creado con 2827677 filas.\n",
      "\n",
      "--- 4. Calculando Estadísticas Base (BENIGN) ---\n",
      "Estadísticas de tráfico BENIGN calculadas.\n",
      "\n",
      "--- 5. Automatizando la selección de KEY_FEATURES_BY_ATTACK_TYPE ---\n",
      "Procesando tipo de ataque: FTP-Patator\n",
      "Procesando tipo de ataque: SSH-Patator\n",
      "Procesando tipo de ataque: DoS slowloris\n",
      "Procesando tipo de ataque: DoS Slowhttptest\n",
      "Procesando tipo de ataque: DoS Hulk\n",
      "Procesando tipo de ataque: DoS GoldenEye\n",
      "Procesando tipo de ataque: Heartbleed\n",
      "Procesando tipo de ataque: Web Attack - Brute Force\n",
      "Procesando tipo de ataque: Web Attack - XSS\n",
      "Procesando tipo de ataque: Web Attack - Sql Injection\n",
      "Procesando tipo de ataque: Infiltration\n",
      "Procesando tipo de ataque: Bot\n",
      "Procesando tipo de ataque: PortScan\n",
      "Procesando tipo de ataque: DDoS\n",
      "\n",
      "Diccionario KEY_FEATURES_BY_ATTACK_TYPE actualizado automáticamente:\n",
      "  FTP-Patator: 40 features\n",
      "  SSH-Patator: 40 features\n",
      "  DoS slowloris: 40 features\n",
      "  DoS Slowhttptest: 40 features\n",
      "  DoS Hulk: 40 features\n",
      "  DoS GoldenEye: 40 features\n",
      "  Heartbleed: 40 features\n",
      "  Web Attack - Brute Force: 40 features\n",
      "  Web Attack - XSS: 40 features\n",
      "  Web Attack - Sql Injection: 40 features\n",
      "  Infiltration: 40 features\n",
      "  Bot: 40 features\n",
      "  PortScan: 40 features\n",
      "  DDoS: 40 features\n",
      "  BENIGN: 0 features\n",
      "  Label: 0 features\n",
      "\n",
      "--- 6. Definiendo la función para generar el resumen numérico ---\n",
      "Función 'get_numeric_summary_for_incident' definida.\n",
      "\n",
      "--- 7. Ejemplo de Uso ---\n",
      "Generando resumen numérico para un incidente de tipo: PortScan\n",
      "{\n",
      "  \"PSH_Flag_Count\": \"Elevated (1.00), higher than normal (0.26).\",\n",
      "  \"Bwd_Packets/s\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Bwd_Packet_Length_Mean\": \"Near normal (0.05), typical for benign traffic (0.03).\",\n",
      "  \"Bwd_Packet_Length_Min\": \"Near normal (0.00), typical for benign traffic (0.02).\",\n",
      "  \"Total_Fwd_Packets\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Avg_Bwd_Segment_Size\": \"Near normal (0.05), typical for benign traffic (0.03).\",\n",
      "  \"Packet_Length_Mean\": \"Near normal (0.05), typical for benign traffic (0.03).\",\n",
      "  \"Packet_Length_Std\": \"Near normal (0.07), typical for benign traffic (0.03).\",\n",
      "  \"Subflow_Fwd_Bytes\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Init_Win_bytes_forward\": \"Elevated (0.45), higher than normal (0.11).\",\n",
      "  \"Subflow_Fwd_Packets\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Bwd_Packet_Length_Max\": \"Near normal (0.05), typical for benign traffic (0.02).\",\n",
      "  \"ACK_Flag_Count\": \"Near normal (0.00), typical for benign traffic (0.29).\",\n",
      "  \"Fwd_Packets/s\": \"Near normal (0.00), typical for benign traffic (0.02).\",\n",
      "  \"act_data_pkt_fwd\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Flow_Bytes/s\": \"Near normal (0.11), typical for benign traffic (0.11).\",\n",
      "  \"Average_Packet_Size\": \"Near normal (0.05), typical for benign traffic (0.03).\",\n",
      "  \"Fwd_Packet_Length_Mean\": \"Near normal (0.02), typical for benign traffic (0.01).\",\n",
      "  \"Total_Backward_Packets\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Fwd_Packet_Length_Max\": \"Near normal (0.01), typical for benign traffic (0.01).\",\n",
      "  \"Avg_Fwd_Segment_Size\": \"Near normal (0.02), typical for benign traffic (0.01).\",\n",
      "  \"Subflow_Bwd_Packets\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Destination_Port\": \"Near normal (0.00), typical for benign traffic (0.14).\",\n",
      "  \"Fwd_Packet_Length_Min\": \"Near normal (0.00), typical for benign traffic (0.01).\",\n",
      "  \"URG_Flag_Count\": \"Near normal (0.00), typical for benign traffic (0.12).\",\n",
      "  \"Down/Up_Ratio\": \"Reduced (0.00), lower than normal (0.00).\",\n",
      "  \"Flow_Packets/s\": \"Near normal (0.33), typical for benign traffic (0.34).\",\n",
      "  \"Packet_Length_Variance\": \"Near normal (0.00), typical for benign traffic (0.01).\",\n",
      "  \"Source_Port\": \"Near normal (0.62), typical for benign traffic (0.61).\",\n",
      "  \"Flow_Duration\": \"Near normal (0.04), typical for benign traffic (0.09).\",\n",
      "  \"Init_Win_bytes_backward\": \"Near normal (0.00), typical for benign traffic (0.04).\",\n",
      "  \"Fwd_IAT_Max\": \"Near normal (0.00), typical for benign traffic (0.04).\",\n",
      "  \"Fwd_IAT_Mean\": \"Near normal (0.00), typical for benign traffic (0.02).\",\n",
      "  \"Total_Length_of_Fwd_Packets\": \"Near normal (0.00), typical for benign traffic (0.00).\",\n",
      "  \"Bwd_IAT_Min\": \"Near normal (0.00), typical for benign traffic (0.01).\",\n",
      "  \"Flow_IAT_Std\": \"Near normal (0.02), typical for benign traffic (0.02).\",\n",
      "  \"Fwd_IAT_Min\": \"Near normal (0.00), typical for benign traffic (0.01).\",\n",
      "  \"Flow_IAT_Max\": \"Near normal (0.04), typical for benign traffic (0.04).\",\n",
      "  \"Max_Packet_Length\": \"Near normal (0.04), typical for benign traffic (0.02).\",\n",
      "  \"Flow_IAT_Min\": \"Near normal (0.00), typical for benign traffic (0.00).\"\n",
      "}\n",
      "\n",
      "--- 8. Guardando artefactos generados ---\n",
      "\n",
      "Datos preprocesados y diccionario de características guardados en: /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/preprocessed_data_for_llm\n",
      "Pipeline completado.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Pipeline Completo: Preparación de CIC-IDS2017 y Análisis de Características\n",
    "# \n",
    "# Este notebook contiene el código unificado para:\n",
    "# 1. Cargar y limpiar los datos del dataset CIC-IDS2017.\n",
    "# 2. Escalar las características numéricas.\n",
    "# 3. Calcular estadísticas base para el tráfico benigno.\n",
    "# 4. Realizar un análisis de importancia de características para identificar las métricas más relevantes por tipo de ataque.\n",
    "# 5. Automatizar la creación del diccionario `KEY_FEATURES_BY_ATTACK_TYPE`.\n",
    "# 6. Definir una función para generar el `key_numeric_summary` para los incidentes.\n",
    "# 7. Guardar los artefactos generados (DataFrames y el diccionario de características) para su uso futuro.\n",
    "\n",
    "# %%\n",
    "# --- 0. Importaciones y Configuración ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suprimir advertencias para una salida más limpia\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Configuración de Rutas y Carga de Archivos ---\n",
    "print(\"--- 1. Configurando rutas y cargando archivos CSV de CIC-IDS2017 ---\")\n",
    "\n",
    "# RUTA A LA UBICACIÓN REAL Y ABSOLUTA DE TU CARPETA CIC_TrafficLabelling\n",
    "cic_base_path = '/Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling'\n",
    "\n",
    "csv_files = [\n",
    "    'Monday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Wednesday-workingHours.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "]\n",
    "\n",
    "all_dfs = []\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(cic_base_path, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Cargando {file_name}...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='latin1', low_memory=False)\n",
    "            df.columns = df.columns.str.strip()\n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar {file_name}: {e}. Saltando.\")\n",
    "    else:\n",
    "        print(f\"Advertencia: Archivo no encontrado en {file_path}. Saltando.\")\n",
    "\n",
    "if not all_dfs:\n",
    "    raise FileNotFoundError(\"No se encontró ningún archivo CSV de CIC-IDS2017. Verifica la ruta y los nombres de archivo.\")\n",
    "\n",
    "df_cic = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"Dataset CIC-IDS2017 cargado. Filas totales iniciales: {len(df_cic)}\")\n",
    "\n",
    "# --- 2. Limpieza y Preprocesamiento de Datos ---\n",
    "print(\"\\n--- 2. Limpieza y Preprocesamiento de Datos ---\")\n",
    "\n",
    "df_cic.columns = df_cic.columns.str.replace(' ', '_')\n",
    "\n",
    "initial_rows = len(df_cic)\n",
    "df_cic.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_cic.dropna(inplace=True)\n",
    "print(f\"Filas eliminadas por NaN/Inf: {initial_rows - len(df_cic)}\")\n",
    "\n",
    "initial_rows = len(df_cic)\n",
    "df_cic.drop_duplicates(inplace=True)\n",
    "print(f\"Filas eliminadas por duplicados: {initial_rows - len(df_cic)}\")\n",
    "\n",
    "if 'Timestamp' in df_cic.columns:\n",
    "    df_cic['Timestamp_DT'] = pd.to_datetime(df_cic['Timestamp'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "    df_cic['Timestamp_DT'] = df_cic['Timestamp_DT'].fillna(\n",
    "        pd.to_datetime(df_cic['Timestamp'], format='%d/%m/%Y %H:%M', errors='coerce')\n",
    "    )\n",
    "    initial_rows_ts = len(df_cic)\n",
    "    df_cic.dropna(subset=['Timestamp_DT'], inplace=True)\n",
    "    print(f\"Filas eliminadas por conversión fallida de Timestamp: {initial_rows_ts - len(df_cic)}\")\n",
    "    df_cic['Timestamp_Unix'] = df_cic['Timestamp_DT'].astype(np.int64) // 10**9\n",
    "else:\n",
    "    print(\"Columna 'Timestamp' no encontrada en el dataset.\")\n",
    "\n",
    "numeric_features_cols = [col for col in df_cic.columns if pd.api.types.is_numeric_dtype(df_cic[col]) and col not in ['Flow_ID', 'Source_IP', 'Destination_IP', 'Timestamp', 'Timestamp_DT', 'Timestamp_Unix', 'Label']]\n",
    "for col in numeric_features_cols:\n",
    "    df_cic[col] = pd.to_numeric(df_cic[col], errors='coerce')\n",
    "    df_cic[col] = df_cic[col].astype(float)\n",
    "df_cic.fillna(0, inplace=True)\n",
    "\n",
    "df_cic['Label'] = df_cic['Label'].astype(str).str.strip()\n",
    "df_cic['Label'] = df_cic['Label'].replace({\n",
    "    'Web Attack \\x96 Brute Force': 'Web Attack - Brute Force',\n",
    "    'Web Attack \\x96 XSS': 'Web Attack - XSS',\n",
    "    'Web Attack \\x96 Sql Injection': 'Web Attack - Sql Injection'\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset después de limpieza: Filas={df_cic.shape[0]}, Columnas={df_cic.shape[1]}\")\n",
    "print(\"Distribución de etiquetas final:\")\n",
    "print(df_cic['Label'].value_counts())\n",
    "\n",
    "# --- 3. Preparación de DataFrames para Fine-tuning ---\n",
    "print(\"\\n--- 3. Preparación de DataFrames para Fine-tuning ---\")\n",
    "\n",
    "CONTEXT_COLS_FINAL = ['Flow_ID', 'Source_IP', 'Destination_IP', 'Timestamp', 'Timestamp_Unix', 'Label']\n",
    "df_context = df_cic[CONTEXT_COLS_FINAL].copy()\n",
    "\n",
    "df_features_to_scale = df_cic[numeric_features_cols].copy()\n",
    "scaler = MinMaxScaler()\n",
    "df_features_scaled = df_features_to_scale.copy()\n",
    "df_features_scaled[numeric_features_cols] = scaler.fit_transform(df_features_to_scale[numeric_features_cols])\n",
    "df_features_scaled['Label'] = df_cic['Label']\n",
    "df_features_scaled['Timestamp_Unix'] = df_cic['Timestamp_Unix']\n",
    "\n",
    "print(f\"df_context creado con {len(df_context)} filas.\")\n",
    "print(f\"df_features_scaled creado con {len(df_features_scaled)} filas.\")\n",
    "\n",
    "# --- 4. Cálculo de Estadísticas Base (BENIGN) ---\n",
    "print(\"\\n--- 4. Calculando Estadísticas Base (BENIGN) ---\")\n",
    "\n",
    "df_benign = df_features_scaled[df_features_scaled['Label'] == 'BENIGN']\n",
    "numeric_cols_for_stats = [col for col in numeric_features_cols if col in df_benign.columns]\n",
    "benign_stats = df_benign[numeric_cols_for_stats].agg(['mean', 'std']).transpose()\n",
    "benign_stats.rename(columns={'mean': 'benign_mean', 'std': 'benign_std'}, inplace=True)\n",
    "print(\"Estadísticas de tráfico BENIGN calculadas.\")\n",
    "\n",
    "# --- 5. Análisis de Importancia de Características y Automatización de KEY_FEATURES_BY_ATTACK_TYPE ---\n",
    "print(\"\\n--- 5. Automatizando la selección de KEY_FEATURES_BY_ATTACK_TYPE ---\")\n",
    "\n",
    "KEY_FEATURES_BY_ATTACK_TYPE = {}\n",
    "attack_types = [at for at in df_features_scaled['Label'].unique() if at != 'BENIGN']\n",
    "NUM_TOP_FEATURES_PER_ATTACK = 40\n",
    "\n",
    "# Muestreo para eficiencia en el entrenamiento de los clasificadores\n",
    "sample_size = min(500000, len(df_features_scaled))\n",
    "df_sample = df_features_scaled.sample(n=sample_size, random_state=42)\n",
    "\n",
    "for attack_type in attack_types:\n",
    "    print(f\"Procesando tipo de ataque: {attack_type}\")\n",
    "    \n",
    "    # Crear un dataset binario para el tipo de ataque actual (ataque vs. el resto)\n",
    "    df_binary = df_sample.copy()\n",
    "    \n",
    "    if attack_type not in df_binary['Label'].unique():\n",
    "        print(f\"  Advertencia: Tipo de ataque '{attack_type}' no encontrado en el sample. Saltando.\")\n",
    "        KEY_FEATURES_BY_ATTACK_TYPE[attack_type] = []\n",
    "        continue\n",
    "\n",
    "    y_binary = (df_binary['Label'] == attack_type).astype(int)\n",
    "    \n",
    "    # Asegurarse de que haya al menos 2 clases en el target\n",
    "    if y_binary.nunique() < 2:\n",
    "        print(f\"  Advertencia: Solo una clase presente para '{attack_type}' en el sample. Saltando.\")\n",
    "        KEY_FEATURES_BY_ATTACK_TYPE[attack_type] = []\n",
    "        continue\n",
    "        \n",
    "    rf_classifier_binary = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf_classifier_binary.fit(df_binary[numeric_features_cols], y_binary)\n",
    "    \n",
    "    feature_importances_binary = pd.DataFrame({\n",
    "        'Feature': numeric_features_cols,\n",
    "        'Importance': rf_classifier_binary.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    top_features = feature_importances_binary['Feature'].head(NUM_TOP_FEATURES_PER_ATTACK).tolist()\n",
    "    KEY_FEATURES_BY_ATTACK_TYPE[attack_type] = top_features\n",
    "\n",
    "KEY_FEATURES_BY_ATTACK_TYPE['BENIGN'] = []\n",
    "KEY_FEATURES_BY_ATTACK_TYPE['Label'] = []\n",
    "\n",
    "print(\"\\nDiccionario KEY_FEATURES_BY_ATTACK_TYPE actualizado automáticamente:\")\n",
    "for attack_type, features in KEY_FEATURES_BY_ATTACK_TYPE.items():\n",
    "    print(f\"  {attack_type}: {len(features)} features\")\n",
    "\n",
    "# --- 6. Definición de la Función para Generar `key_numeric_summary` ---\n",
    "print(\"\\n--- 6. Definiendo la función para generar el resumen numérico ---\")\n",
    "\n",
    "def get_numeric_summary_for_incident(incident_df_row, benign_stats_df, attack_type, num_format='scaled'):\n",
    "    summary = {}\n",
    "    features_to_summarize = KEY_FEATURES_BY_ATTACK_TYPE.get(attack_type, [])\n",
    "    features_to_summarize = [f for f in features_to_summarize if f in incident_df_row and f in benign_stats_df.index]\n",
    "\n",
    "    if not features_to_summarize:\n",
    "        return {\"No significant key features found for this incident.\": \"\"}\n",
    "\n",
    "    for feature in features_to_summarize:\n",
    "        incident_value = incident_df_row[feature]\n",
    "        benign_mean = benign_stats_df.loc[feature, 'benign_mean']\n",
    "        benign_std = benign_stats_df.loc[feature, 'benign_std']\n",
    "        \n",
    "        description = \"\"\n",
    "        if num_format == 'scaled':\n",
    "            if incident_value > benign_mean + 3 * benign_std:\n",
    "                description = f\"Significantly high ({incident_value:.2f}), much higher than normal ({benign_mean:.2f}).\"\n",
    "            elif incident_value < benign_mean - 3 * benign_std:\n",
    "                description = f\"Significantly low ({incident_value:.2f}), much lower than normal ({benign_mean:.2f}).\"\n",
    "            elif incident_value > benign_mean + 1 * benign_std:\n",
    "                description = f\"Elevated ({incident_value:.2f}), higher than normal ({benign_mean:.2f}).\"\n",
    "            elif incident_value < benign_mean - 1 * benign_std:\n",
    "                description = f\"Reduced ({incident_value:.2f}), lower than normal ({benign_mean:.2f}).\"\n",
    "            else:\n",
    "                description = f\"Near normal ({incident_value:.2f}), typical for benign traffic ({benign_mean:.2f}).\"\n",
    "        summary[feature] = description\n",
    "    return summary\n",
    "\n",
    "print(\"Función 'get_numeric_summary_for_incident' definida.\")\n",
    "\n",
    "# --- 7. Ejemplo de Uso de la Función ---\n",
    "print(\"\\n--- 7. Ejemplo de Uso ---\")\n",
    "\n",
    "# Buscar la primera fila que sea un 'PortScan'\n",
    "try:\n",
    "    first_portscan_index = df_features_scaled[df_features_scaled['Label'] == 'PortScan'].index[0]\n",
    "    incident_example_row = df_features_scaled.loc[first_portscan_index]\n",
    "    attack_label = incident_example_row['Label']\n",
    "    print(f\"Generando resumen numérico para un incidente de tipo: {attack_label}\")\n",
    "    numeric_summary = get_numeric_summary_for_incident(incident_example_row, benign_stats, attack_label, num_format='scaled')\n",
    "    print(json.dumps(numeric_summary, indent=2))\n",
    "except IndexError:\n",
    "    print(\"No se encontraron incidentes de tipo 'PortScan' para el ejemplo.\")\n",
    "\n",
    "\n",
    "# --- 8. Guardar Artefactos Generados ---\n",
    "print(\"\\n--- 8. Guardando artefactos generados ---\")\n",
    "\n",
    "output_dir_save = os.path.join(cic_base_path, 'preprocessed_data_for_llm')\n",
    "os.makedirs(output_dir_save, exist_ok=True)\n",
    "\n",
    "# Guardar DataFrames procesados\n",
    "df_context.to_csv(os.path.join(output_dir_save, 'cic_context_data_for_llm.csv'), index=True)\n",
    "df_features_scaled.to_csv(os.path.join(output_dir_save, 'cic_features_scaled_for_llm.csv'), index=True)\n",
    "\n",
    "# Guardar diccionario de características\n",
    "key_features_path = os.path.join(output_dir_save, 'key_features_by_attack_type.json')\n",
    "with open(key_features_path, 'w') as f:\n",
    "    json.dump(KEY_FEATURES_BY_ATTACK_TYPE, f, indent=4)\n",
    "\n",
    "print(f\"\\nDatos preprocesados y diccionario de características guardados en: {output_dir_save}\")\n",
    "print(\"Pipeline completado.\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23e6da",
   "metadata": {},
   "source": [
    "**Importaciones y configuración de dependencias.**\n",
    "\n",
    "Se cargan librerías y utilidades requeridas para preprocesamiento, entrenamiento y evaluación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e1272e6-6fc8-42bf-8dc6-522a836c2cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPARACIÓN TEMPORAL ROBUSTA PARA VLT ANOMALY / TRANAD PLUS ===\n",
      "\n",
      "--- FASE 0: Carga Robusta desde CSV Originales ---\n",
      "Archivos CSV encontrados: 8\n",
      "  - Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "  - Monday-WorkingHours.pcap_ISCX.csv\n",
      "  - Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "  - Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "  - Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "  - Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "  - Wednesday-workingHours.pcap_ISCX.csv\n",
      "  - Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "\n",
      "--- Cargando archivos CSV con encoding robusto ---\n",
      "Procesando: Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "    Intentando encoding: utf-8\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "    ✓ Éxito con utf-8: 288,395 registros\n",
      "Procesando: Monday-WorkingHours.pcap_ISCX.csv\n",
      "    Intentando encoding: utf-8\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Monday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Monday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Monday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Monday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Monday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Monday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Monday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Monday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Monday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Monday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Monday-WorkingHours.pcap_ISCX.csv\n",
      "    ✓ Éxito con utf-8: 529,481 registros\n",
      "Procesando: Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "    Intentando encoding: utf-8\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "    ✓ Éxito con utf-8: 190,911 registros\n",
      "Procesando: Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "    Intentando encoding: utf-8\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "    ✓ Éxito con utf-8: 286,096 registros\n",
      "Procesando: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "    Intentando encoding: utf-8\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "    ✓ Éxito con utf-8: 225,711 registros\n",
      "Procesando: Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "    Intentando encoding: utf-8\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "    ✓ Éxito con utf-8: 445,645 registros\n",
      "Procesando: Wednesday-workingHours.pcap_ISCX.csv\n",
      "    Intentando encoding: utf-8\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Wednesday-workingHours.pcap_ISCX.csv\n",
      "    ✓ Éxito con utf-8: 691,406 registros\n",
      "Procesando: Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "    Intentando encoding: utf-8\n",
      "    ❌ Fallo con encoding utf-8\n",
      "    Intentando encoding: latin1\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "    ⚠️ Columna 'Timestamp' no encontrada en /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "    ✓ Éxito con latin1: 170,231 registros\n",
      "\n",
      "Resultado de carga:\n",
      "  ✓ Archivos cargados exitosamente: 8\n",
      "  ❌ Archivos fallidos: 0\n",
      "\n",
      "--- Combinando 8 archivos ---\n",
      "Dataset combinado: (2827876, 87)\n",
      "\n",
      "--- FASE 1: Limpieza y Validación Temporal ---\n",
      "Timestamps válidos: 2,827,876 de 2,827,876 (100.0%)\n",
      "Registros con timestamp válido: 2,827,876\n",
      "Rango temporal: 2017-07-03 09:00:00 a 2017-07-11 09:25:02\n",
      "⚠️ Timestamps fuera del rango esperado: 469,384 registros\n",
      "Manteniendo todos los registros para análisis completo\n",
      "Registros después de eliminar duplicados: 2,827,876\n",
      "✓ Dataset ordenado temporalmente\n",
      "❌ Columna 'Label' no encontrada\n",
      "Columnas candidatas para etiquetas: ['_Label']\n",
      "Usando '_Label' como columna de etiquetas\n",
      "\n",
      "--- FASE 2: Filtrado de Clases Relevantes ---\n",
      "Top 10 clases por frecuencia:\n",
      "Label\n",
      "BENIGN              2271320\n",
      "DoS Hulk             230124\n",
      "PortScan             158804\n",
      "DDoS                 128025\n",
      "DoS GoldenEye         10293\n",
      "FTP-Patator            7935\n",
      "SSH-Patator            5897\n",
      "DoS slowloris          5796\n",
      "DoS Slowhttptest       5499\n",
      "Bot                    1956\n",
      "Name: count, dtype: int64\n",
      "Clases seleccionadas: ['BENIGN', 'DoS Hulk', 'PortScan', 'DDoS']\n",
      "Dataset filtrado: (2788273, 88)\n",
      "Distribución filtrada:\n",
      "Label\n",
      "BENIGN      2271320\n",
      "DoS Hulk     230124\n",
      "PortScan     158804\n",
      "DDoS         128025\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución temporal por clase:\n",
      "  BENIGN: 2,271,320 muestras, 2017-07-03 09:00:00 a 2017-07-11 09:25:02\n",
      "  DoS Hulk: 230,124 muestras, 2017-07-04 05:47:40 a 2017-07-07 04:57:12\n",
      "  PortScan: 158,804 muestras, 2017-07-03 09:24:23 a 2017-07-06 14:31:28\n",
      "  DDoS: 128,025 muestras, 2017-07-03 14:14:43 a 2017-07-05 15:52:22\n",
      "\n",
      "--- FASE 3: Ingeniería de Features Temporales ---\n",
      "✓ Features temporales creadas:\n",
      "  - hour_sin: rango [-1.000, 1.000]\n",
      "  - hour_cos: rango [-1.000, 1.000]\n",
      "  - day_sin: rango [-0.975, 0.975]\n",
      "  - day_cos: rango [-0.901, 1.000]\n",
      "  - minute_sin: rango [-1.000, 1.000]\n",
      "  - minute_cos: rango [-1.000, 1.000]\n",
      "  - timestamp_normalized: rango [0.000, 1.000]\n",
      "  - hours_elapsed: rango [0.000, 192.417]\n",
      "\n",
      "--- FASE 4: Preparación de Features para Modelado ---\n",
      "Features identificadas: 93\n",
      "Columnas de metadatos: 7\n",
      "Dataset para modelado: (2788273, 93)\n",
      "\n",
      "--- FASE 5: Identificación, Limpieza y Preservación de Metadatos ---\n",
      "Clasificación de features:\n",
      "  Temporal: 23\n",
      "  Flags: 12\n",
      "  Ports: 2\n",
      "  Packets: 38\n",
      "  Ratios: 8\n",
      "  Flow: 12\n",
      "\n",
      "🔍 PRESERVANDO METADATOS CRÍTICOS PARA INTERPRETACIÓN:\n",
      "  ✓ Preservando _Source_IP\n",
      "  ✓ Preservando _Destination_IP\n",
      "  ✓ Preservando Flow_ID\n",
      "  ✓ Preservando _Timestamp\n",
      "📋 Metadatos preservados: 8 columnas\n",
      "\n",
      "📊 SEPARACIÓN FEATURES vs METADATOS:\n",
      "  Features numéricas para entrenamiento: 88\n",
      "  Columnas preservadas como metadatos: 5\n",
      "  Metadatos: ['_Label', '_Timestamp', '_Source_IP', 'Flow_ID', '_Destination_IP']\n",
      "Features eliminadas por baja varianza (<0.01): 12\n",
      "Calculando matriz de correlación para 76 features...\n",
      "Features eliminadas por alta correlación (>0.98): 21\n",
      "Features finales para normalización: 55\n",
      "\n",
      "--- FASE 6: Normalización Estratégica ---\n",
      "Features por grupo final:\n",
      "  Temporal: 19\n",
      "  Flags: 5\n",
      "  Packets: 20\n",
      "  Ratios: 5\n",
      "  Ports: 2\n",
      "  Flow: 7\n",
      "  Otros: 7\n",
      "  ✓ flags: 5 features normalizadas con minmax\n",
      "  ✓ ratios: 5 features normalizadas con standard\n",
      "  ✓ packets: 20 features normalizadas con robust\n",
      "  ✓ ports: 2 features normalizadas con standard\n",
      "  ✓ flow: 7 features normalizadas con standard\n",
      "  ✓ temporal: 12 features normalizadas con standard\n",
      "  ✓ others: 7 features normalizadas con standard\n",
      "✓ Normalización completada: 7 grupos procesados\n",
      "\n",
      "=== FASE 7: División Estratificada Temporal ===\n",
      "Distribución binaria total: {0: 2271320, 1: 516953}\n",
      "Tasa global de anomalías: 18.5%\n",
      "\n",
      "=== ANÁLISIS TEMPORAL POR TIPO DE ATAQUE ===\n",
      "\n",
      "BENIGN:\n",
      "  Muestras: 2,271,320\n",
      "  Período: 2017-07-03 09:00:00 → 2017-07-11 09:25:02\n",
      "  Duración: 8 days 00:25:02\n",
      "\n",
      "DDoS:\n",
      "  Muestras: 128,025\n",
      "  Período: 2017-07-03 14:14:43 → 2017-07-05 15:52:22\n",
      "  Duración: 2 days 01:37:39\n",
      "\n",
      "DoS Hulk:\n",
      "  Muestras: 230,124\n",
      "  Período: 2017-07-04 05:47:40 → 2017-07-07 04:57:12\n",
      "  Duración: 2 days 23:09:32\n",
      "\n",
      "PortScan:\n",
      "  Muestras: 158,804\n",
      "  Período: 2017-07-03 09:24:23 → 2017-07-06 14:31:28\n",
      "  Duración: 3 days 05:07:05\n",
      "\n",
      "=== DIVISIÓN ESTRATIFICADA TEMPORAL ===\n",
      "\n",
      "📊 BENIGN: 2,271,320 muestras\n",
      "   Período: 2017-07-03 09:00:00 → 2017-07-11 09:25:02\n",
      "   ✓ Train: 1,589,924 muestras (07-03 09:00 - 07-07 02:39)\n",
      "   ✓ Val:   340,698 muestras (07-07 02:39 - 07-08 11:03)\n",
      "   ✓ Test:  340,698 muestras (07-08 11:03 - 07-11 09:25)\n",
      "\n",
      "📊 DoS Hulk: 230,124 muestras\n",
      "   Período: 2017-07-04 05:47:40 → 2017-07-07 04:57:12\n",
      "   ✓ Train: 161,086 muestras (07-04 05:47 - 07-06 05:17)\n",
      "   ✓ Val:   34,519 muestras (07-06 05:17 - 07-06 15:54)\n",
      "   ✓ Test:  34,519 muestras (07-06 15:54 - 07-07 04:57)\n",
      "\n",
      "📊 PortScan: 158,804 muestras\n",
      "   Período: 2017-07-03 09:24:23 → 2017-07-06 14:31:28\n",
      "   ✓ Train: 111,162 muestras (07-03 09:24 - 07-05 17:05)\n",
      "   ✓ Val:   23,821 muestras (07-05 17:05 - 07-06 00:03)\n",
      "   ✓ Test:  23,821 muestras (07-06 00:03 - 07-06 14:31)\n",
      "\n",
      "📊 DDoS: 128,025 muestras\n",
      "   Período: 2017-07-03 14:14:43 → 2017-07-05 15:52:22\n",
      "   ✓ Train: 89,617 muestras (07-03 14:14 - 07-05 00:10)\n",
      "   ✓ Val:   19,204 muestras (07-05 00:10 - 07-05 07:39)\n",
      "   ✓ Test:  19,204 muestras (07-05 07:39 - 07-05 15:52)\n",
      "\n",
      "=== COMBINANDO SPLITS CON ORDEN TEMPORAL GLOBAL ===\n",
      "\n",
      "🎯 RESULTADO FINAL DE DIVISIÓN ESTRATIFICADA:\n",
      "=================================================================================\n",
      "\n",
      "📊 TRAIN SET:\n",
      "   Muestras: 1,951,789\n",
      "   Período: 2017-07-03 09:00:00 → 2017-07-07 02:39:30\n",
      "   Tasa anomalías: 18.5%\n",
      "      BENIGN: 1,589,924 (81.5%)\n",
      "      DoS Hulk: 161,086 (8.3%)\n",
      "      PortScan: 111,162 (5.7%)\n",
      "      DDoS: 89,617 (4.6%)\n",
      "\n",
      "📊 VALIDATION SET:\n",
      "   Muestras: 418,242\n",
      "   Período: 2017-07-05 00:10:33 → 2017-07-08 11:03:49\n",
      "   Tasa anomalías: 18.5%\n",
      "      BENIGN: 340,698 (81.5%)\n",
      "      DoS Hulk: 34,519 (8.3%)\n",
      "      PortScan: 23,821 (5.7%)\n",
      "      DDoS: 19,204 (4.6%)\n",
      "\n",
      "📊 TEST SET:\n",
      "   Muestras: 418,242\n",
      "   Período: 2017-07-05 07:39:35 → 2017-07-11 09:25:02\n",
      "   Tasa anomalías: 18.5%\n",
      "      BENIGN: 340,698 (81.5%)\n",
      "      DoS Hulk: 34,519 (8.3%)\n",
      "      PortScan: 23,821 (5.7%)\n",
      "      DDoS: 19,204 (4.6%)\n",
      "\n",
      "🏁 DIVISIÓN ESTRATIFICADA TEMPORAL COMPLETADA\n",
      "\n",
      "--- FASE 8: Guardado con Metadatos Interpretativos ---\n",
      "✅ Train guardado: 1,951,789 muestras + metadatos interpretativos\n",
      "✅ Val guardado: 418,242 muestras + metadatos interpretativos\n",
      "✅ Test guardado: 418,242 muestras + metadatos interpretativos\n",
      "\n",
      "==========================================================================================\n",
      "🎉 PREPARACIÓN COMPLETA CON METADATOS INTERPRETATIVOS\n",
      "==========================================================================================\n",
      "✅ Features para modelado: 55\n",
      "✅ Metadatos preservados: 8 tipos\n",
      "✅ Splits balanceados temporalmente\n",
      "✅ Interpretabilidad post-detección garantizada\n",
      "\n",
      "Archivos clave:\n",
      " 🤖 Entrenamiento: train.csv (55 features)\n",
      " 🔍 Interpretación: *_interpretation_metadata.csv\n",
      " 📖 Guía: INTERPRETATION_GUIDE.md\n",
      " ⚙️ Config: interpretation_guide.json\n",
      "\n",
      "[INTERPRETABLE_DATA_READY] ✅\n",
      "Datos listos para VLT/TranAD Plus con interpretabilidad completa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de rutas\n",
    "raw_data_path = \"/Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/TrafficLabelling\"\n",
    "output_path = \"/Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/outputs/data\"\n",
    "artifacts_path = \"/Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/outputs/artifacts\"\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(artifacts_path, exist_ok=True)\n",
    "\n",
    "print(\"=== PREPARACIÓN TEMPORAL ROBUSTA PARA VLT ANOMALY / TRANAD PLUS ===\")\n",
    "\n",
    "# FASE 0: Carga Robusta desde CSV Originales\n",
    "print(\"\\n--- FASE 0: Carga Robusta desde CSV Originales ---\")\n",
    "\n",
    "csv_files = glob.glob(os.path.join(raw_data_path, \"*.csv\"))\n",
    "csv_files = [f for f in csv_files if any(day in f for day in [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]) or \"CIC\" in f or \"pcap\" in f]\n",
    "\n",
    "if not csv_files:\n",
    "    for root, dirs, files in os.walk(raw_data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv') and any(keyword in file for keyword in [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"CIC\", \"pcap\"]):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Archivos CSV encontrados: {len(csv_files)}\")\n",
    "for file in csv_files:\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "# Función robusta para cargar CSV con diferentes encodings\n",
    "def load_csv_robust(csv_file):\n",
    "    \"\"\"Cargar CSV probando diferentes encodings\"\"\"\n",
    "    encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252', 'utf-16']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"    Intentando encoding: {encoding}\")\n",
    "            \n",
    "            chunk_list = []\n",
    "            chunk_size = 50000\n",
    "            \n",
    "            for chunk in pd.read_csv(csv_file, encoding=encoding, chunksize=chunk_size, low_memory=False):\n",
    "                chunk.columns = chunk.columns.str.replace(' ', '_').str.strip()\n",
    "                chunk['source_file'] = os.path.basename(csv_file)\n",
    "                \n",
    "                if 'Timestamp' in chunk.columns:\n",
    "                    chunk['Timestamp_parsed'] = pd.to_datetime(\n",
    "                        chunk['Timestamp'], \n",
    "                        format='%d/%m/%Y %H:%M:%S', \n",
    "                        errors='coerce'\n",
    "                    )\n",
    "                    \n",
    "                    mask_null = chunk['Timestamp_parsed'].isnull()\n",
    "                    if mask_null.any():\n",
    "                        chunk.loc[mask_null, 'Timestamp_parsed'] = pd.to_datetime(\n",
    "                            chunk.loc[mask_null, 'Timestamp'], \n",
    "                            format='%d/%m/%Y %H:%M', \n",
    "                            errors='coerce'\n",
    "                        )\n",
    "                    \n",
    "                    mask_null = chunk['Timestamp_parsed'].isnull()\n",
    "                    if mask_null.any():\n",
    "                        chunk.loc[mask_null, 'Timestamp_parsed'] = pd.to_datetime(\n",
    "                            chunk.loc[mask_null, 'Timestamp'], \n",
    "                            errors='coerce'\n",
    "                        )\n",
    "                else:\n",
    "                    print(f\"    ⚠️ Columna 'Timestamp' no encontrada en {csv_file}\")\n",
    "                    base_time = pd.Timestamp('2017-07-03 09:00:00')\n",
    "                    chunk['Timestamp_parsed'] = base_time + pd.to_timedelta(chunk.index, unit='s')\n",
    "                \n",
    "                chunk = chunk.replace([np.inf, -np.inf], np.nan)\n",
    "                chunk = chunk.dropna()\n",
    "                \n",
    "                if len(chunk) > 0:\n",
    "                    chunk_list.append(chunk)\n",
    "            \n",
    "            if chunk_list:\n",
    "                df_combined = pd.concat(chunk_list, ignore_index=True)\n",
    "                print(f\"    ✓ Éxito con {encoding}: {len(df_combined):,} registros\")\n",
    "                return df_combined\n",
    "            \n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"    ❌ Fallo con encoding {encoding}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"    ❌ Error con {encoding}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"    ❌ No se pudo cargar {csv_file} con ningún encoding\")\n",
    "    return None\n",
    "\n",
    "# Cargar todos los archivos\n",
    "dataframes = []\n",
    "failed_files = []\n",
    "\n",
    "print(\"\\n--- Cargando archivos CSV con encoding robusto ---\")\n",
    "for csv_file in csv_files:\n",
    "    print(f\"Procesando: {os.path.basename(csv_file)}\")\n",
    "    df_file = load_csv_robust(csv_file)\n",
    "    \n",
    "    if df_file is not None:\n",
    "        dataframes.append(df_file)\n",
    "    else:\n",
    "        failed_files.append(csv_file)\n",
    "\n",
    "print(f\"\\nResultado de carga:\")\n",
    "print(f\"  ✓ Archivos cargados exitosamente: {len(dataframes)}\")\n",
    "print(f\"  ❌ Archivos fallidos: {len(failed_files)}\")\n",
    "if failed_files:\n",
    "    for failed in failed_files:\n",
    "        print(f\"    - {os.path.basename(failed)}\")\n",
    "\n",
    "if not dataframes:\n",
    "    raise ValueError(\"No se pudo cargar ningún archivo CSV\")\n",
    "\n",
    "# Combinar dataframes\n",
    "print(f\"\\n--- Combinando {len(dataframes)} archivos ---\")\n",
    "df_combined = pd.concat(dataframes, ignore_index=True)\n",
    "print(f\"Dataset combinado: {df_combined.shape}\")\n",
    "\n",
    "# Verificar que existe la columna Timestamp_parsed\n",
    "if 'Timestamp_parsed' not in df_combined.columns:\n",
    "    print(\"❌ Error: No se encontró columna Timestamp_parsed en ningún archivo\")\n",
    "    print(\"Columnas disponibles:\", list(df_combined.columns))\n",
    "    raise ValueError(\"Columna Timestamp_parsed faltante\")\n",
    "\n",
    "# FASE 1: Limpieza y Validación Temporal\n",
    "print(\"\\n--- FASE 1: Limpieza y Validación Temporal ---\")\n",
    "\n",
    "initial_count = len(df_combined)\n",
    "valid_timestamps = df_combined['Timestamp_parsed'].notna().sum()\n",
    "print(f\"Timestamps válidos: {valid_timestamps:,} de {initial_count:,} ({valid_timestamps/initial_count*100:.1f}%)\")\n",
    "\n",
    "df_combined = df_combined.dropna(subset=['Timestamp_parsed'])\n",
    "print(f\"Registros con timestamp válido: {len(df_combined):,}\")\n",
    "\n",
    "if len(df_combined) > 0:\n",
    "    min_time = df_combined['Timestamp_parsed'].min()\n",
    "    max_time = df_combined['Timestamp_parsed'].max()\n",
    "    print(f\"Rango temporal: {min_time} a {max_time}\")\n",
    "    \n",
    "    expected_start = pd.Timestamp('2017-07-01')\n",
    "    expected_end = pd.Timestamp('2017-07-08')\n",
    "    \n",
    "    valid_range_mask = (df_combined['Timestamp_parsed'] >= expected_start) & (df_combined['Timestamp_parsed'] <= expected_end)\n",
    "    if not valid_range_mask.all():\n",
    "        print(f\"⚠️ Timestamps fuera del rango esperado: {(~valid_range_mask).sum():,} registros\")\n",
    "        print(\"Manteniendo todos los registros para análisis completo\")\n",
    "else:\n",
    "    raise ValueError(\"No hay registros con timestamps válidos\")\n",
    "\n",
    "initial_count = len(df_combined)\n",
    "df_combined = df_combined.drop_duplicates()\n",
    "print(f\"Registros después de eliminar duplicados: {len(df_combined):,}\")\n",
    "\n",
    "df_combined = df_combined.sort_values('Timestamp_parsed').reset_index(drop=True)\n",
    "print(f\"✓ Dataset ordenado temporalmente\")\n",
    "\n",
    "if 'Label' in df_combined.columns:\n",
    "    print(f\"\\nDistribución de etiquetas completa:\")\n",
    "    label_dist = df_combined['Label'].value_counts()\n",
    "    print(label_dist.head(15))\n",
    "else:\n",
    "    print(\"❌ Columna 'Label' no encontrada\")\n",
    "    label_columns = [col for col in df_combined.columns if 'label' in col.lower() or 'class' in col.lower()]\n",
    "    print(f\"Columnas candidatas para etiquetas: {label_columns}\")\n",
    "    if label_columns:\n",
    "        label_col = label_columns[0]\n",
    "        df_combined['Label'] = df_combined[label_col]\n",
    "        print(f\"Usando '{label_col}' como columna de etiquetas\")\n",
    "\n",
    "# FASE 2: Filtrado de Clases Relevantes\n",
    "print(\"\\n--- FASE 2: Filtrado de Clases Relevantes ---\")\n",
    "\n",
    "all_labels = df_combined['Label'].value_counts()\n",
    "print(\"Top 10 clases por frecuencia:\")\n",
    "print(all_labels.head(10))\n",
    "\n",
    "min_samples = 1000\n",
    "viable_classes = all_labels[all_labels >= min_samples].index.tolist()\n",
    "\n",
    "target_classes = ['BENIGN', 'DoS Hulk', 'PortScan', 'DDoS']\n",
    "available_targets = [cls for cls in target_classes if cls in viable_classes]\n",
    "\n",
    "if len(available_targets) >= 2:\n",
    "    selected_classes = available_targets\n",
    "else:\n",
    "    selected_classes = viable_classes[:4]\n",
    "\n",
    "print(f\"Clases seleccionadas: {selected_classes}\")\n",
    "\n",
    "mask_relevant = df_combined['Label'].isin(selected_classes)\n",
    "df_filtered = df_combined[mask_relevant].copy()\n",
    "\n",
    "print(f\"Dataset filtrado: {df_filtered.shape}\")\n",
    "print(\"Distribución filtrada:\")\n",
    "filtered_dist = df_filtered['Label'].value_counts()\n",
    "print(filtered_dist)\n",
    "\n",
    "print(f\"\\nDistribución temporal por clase:\")\n",
    "for label in selected_classes:\n",
    "    subset = df_filtered[df_filtered['Label'] == label]\n",
    "    if len(subset) > 0:\n",
    "        time_range = f\"{subset['Timestamp_parsed'].min()} a {subset['Timestamp_parsed'].max()}\"\n",
    "        print(f\"  {label}: {len(subset):,} muestras, {time_range}\")\n",
    "\n",
    "# FASE 3: Ingeniería de Features Temporales\n",
    "print(\"\\n--- FASE 3: Ingeniería de Features Temporales ---\")\n",
    "\n",
    "df_features = df_filtered.copy()\n",
    "\n",
    "if df_features['Timestamp_parsed'].dtype == 'datetime64[ns]':\n",
    "    df_features['hour'] = df_features['Timestamp_parsed'].dt.hour\n",
    "    df_features['day_of_week'] = df_features['Timestamp_parsed'].dt.dayofweek\n",
    "    df_features['minute'] = df_features['Timestamp_parsed'].dt.minute\n",
    "    \n",
    "    df_features['hour_sin'] = np.sin(2 * np.pi * df_features['hour'] / 24)\n",
    "    df_features['hour_cos'] = np.cos(2 * np.pi * df_features['hour'] / 24)\n",
    "    df_features['day_sin'] = np.sin(2 * np.pi * df_features['day_of_week'] / 7)\n",
    "    df_features['day_cos'] = np.cos(2 * np.pi * df_features['day_of_week'] / 7)\n",
    "    df_features['minute_sin'] = np.sin(2 * np.pi * df_features['minute'] / 60)\n",
    "    df_features['minute_cos'] = np.cos(2 * np.pi * df_features['minute'] / 60)\n",
    "    \n",
    "    df_features['timestamp_unix'] = df_features['Timestamp_parsed'].astype(np.int64) // 10**9\n",
    "    timestamp_min = df_features['timestamp_unix'].min()\n",
    "    timestamp_max = df_features['timestamp_unix'].max()\n",
    "    \n",
    "    if timestamp_max > timestamp_min:\n",
    "        df_features['timestamp_normalized'] = (df_features['timestamp_unix'] - timestamp_min) / (timestamp_max - timestamp_min)\n",
    "    else:\n",
    "        df_features['timestamp_normalized'] = 0.5\n",
    "    \n",
    "    df_features['hours_elapsed'] = (df_features['timestamp_unix'] - timestamp_min) / 3600\n",
    "    \n",
    "    temporal_features_created = [\n",
    "        'hour_sin', 'hour_cos', 'day_sin', 'day_cos', \n",
    "        'minute_sin', 'minute_cos', 'timestamp_normalized', 'hours_elapsed'\n",
    "    ]\n",
    "    \n",
    "    print(\"✓ Features temporales creadas:\")\n",
    "    for feat in temporal_features_created:\n",
    "        print(f\"  - {feat}: rango [{df_features[feat].min():.3f}, {df_features[feat].max():.3f}]\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Timestamps no válidos para ingeniería de features\")\n",
    "    temporal_features_created = []\n",
    "\n",
    "# FASE 4: Preparación de Features para Modelado\n",
    "print(\"\\n--- FASE 4: Preparación de Features para Modelado ---\")\n",
    "\n",
    "metadata_cols = [\n",
    "    'Label', 'Timestamp', 'Timestamp_parsed', 'source_file', \n",
    "    'hour', 'day_of_week', 'minute', 'timestamp_unix'\n",
    "]\n",
    "metadata_cols = [col for col in metadata_cols if col in df_features.columns]\n",
    "\n",
    "feature_cols = [col for col in df_features.columns if col not in metadata_cols]\n",
    "print(f\"Features identificadas: {len(feature_cols)}\")\n",
    "print(f\"Columnas de metadatos: {len(metadata_cols)}\")\n",
    "\n",
    "X_all = df_features[feature_cols].copy()\n",
    "y_all = df_features['Label'].copy()\n",
    "timestamps = df_features['Timestamp_parsed'].copy()\n",
    "source_files = df_features['source_file'].copy()\n",
    "\n",
    "print(f\"Dataset para modelado: {X_all.shape}\")\n",
    "\n",
    "# FASE 5: Identificación, Limpieza y Preservación de Metadatos\n",
    "print(\"\\n--- FASE 5: Identificación, Limpieza y Preservación de Metadatos ---\")\n",
    "\n",
    "temporal_features_final = [col for col in feature_cols if any(keyword in col.lower() \n",
    "                          for keyword in ['hour_', 'day_', 'minute_', 'timestamp_', 'time', 'iat', 'elapsed'])]\n",
    "flag_features = [col for col in feature_cols if 'flag' in col.lower()]\n",
    "port_features = [col for col in feature_cols if 'port' in col.lower()]\n",
    "packet_features = [col for col in feature_cols if any(keyword in col.lower() \n",
    "                  for keyword in ['packet', 'bytes', 'length', 'size'])]\n",
    "ratio_features = [col for col in feature_cols if any(keyword in col.lower() \n",
    "                 for keyword in ['ratio', '/s', 'rate', '%'])]\n",
    "flow_features = [col for col in feature_cols if 'flow' in col.lower()]\n",
    "\n",
    "print(f\"Clasificación de features:\")\n",
    "print(f\"  Temporal: {len(temporal_features_final)}\")\n",
    "print(f\"  Flags: {len(flag_features)}\")\n",
    "print(f\"  Ports: {len(port_features)}\")\n",
    "print(f\"  Packets: {len(packet_features)}\")\n",
    "print(f\"  Ratios: {len(ratio_features)}\")\n",
    "print(f\"  Flow: {len(flow_features)}\")\n",
    "\n",
    "# CRÍTICO: Preservar metadatos interpretativos ANTES de limpieza\n",
    "print(f\"\\n🔍 PRESERVANDO METADATOS CRÍTICOS PARA INTERPRETACIÓN:\")\n",
    "\n",
    "interpretation_metadata = {}\n",
    "\n",
    "interpretation_metadata['timestamp_original'] = timestamps.copy()\n",
    "interpretation_metadata['source_file'] = source_files.copy()\n",
    "interpretation_metadata['original_label'] = y_all.copy()\n",
    "interpretation_metadata['processing_index'] = np.arange(len(df_features))\n",
    "\n",
    "# Buscar y preservar IPs originales\n",
    "ip_columns = [col for col in df_features.columns if 'ip' in col.lower()]\n",
    "for col in ip_columns:\n",
    "    if col in df_features.columns:\n",
    "        interpretation_metadata[f'{col}_original'] = df_features[col].astype(str).copy()\n",
    "        print(f\"  ✓ Preservando {col}\")\n",
    "\n",
    "# Buscar y preservar Flow IDs\n",
    "flow_id_columns = [col for col in df_features.columns if 'flow' in col.lower() and 'id' in col.lower()]\n",
    "for col in flow_id_columns:\n",
    "    if col in df_features.columns:\n",
    "        interpretation_metadata[f'{col}_original'] = df_features[col].astype(str).copy()\n",
    "        print(f\"  ✓ Preservando {col}\")\n",
    "\n",
    "# Preservar timestamp original si existe\n",
    "timestamp_columns = [col for col in df_features.columns if 'timestamp' in col.lower() and col != 'timestamp']\n",
    "for col in timestamp_columns:\n",
    "    if col in df_features.columns and df_features[col].dtype == 'object':\n",
    "        interpretation_metadata[f'{col}_original'] = df_features[col].astype(str).copy()\n",
    "        print(f\"  ✓ Preservando {col}\")\n",
    "\n",
    "# Preservar puertos como interpretables\n",
    "for col in ['Source_Port', 'Destination_Port']:\n",
    "    if col in df_features.columns:\n",
    "        interpretation_metadata[f'{col}_original'] = df_features[col].copy()\n",
    "        print(f\"  ✓ Preservando {col}\")\n",
    "\n",
    "print(f\"📋 Metadatos preservados: {len(interpretation_metadata)} columnas\")\n",
    "\n",
    "# Ahora proceder con features numéricas para modelado\n",
    "X_numeric = X_all.select_dtypes(include=[np.number]).copy()\n",
    "non_numeric_cols = set(X_all.columns) - set(X_numeric.columns)\n",
    "\n",
    "print(f\"\\n📊 SEPARACIÓN FEATURES vs METADATOS:\")\n",
    "print(f\"  Features numéricas para entrenamiento: {len(X_numeric.columns)}\")\n",
    "print(f\"  Columnas preservadas como metadatos: {len(non_numeric_cols)}\")\n",
    "if len(non_numeric_cols) <= 10:\n",
    "    print(f\"  Metadatos: {list(non_numeric_cols)}\")\n",
    "else:\n",
    "    print(f\"  Metadatos: {list(non_numeric_cols)[:5]}... (+{len(non_numeric_cols)-5} más)\")\n",
    "\n",
    "# Limpieza de valores problemáticos\n",
    "X_clean = X_numeric.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "nan_counts = X_clean.isnull().sum()\n",
    "problematic_cols = nan_counts[nan_counts > len(X_clean) * 0.5].index.tolist()\n",
    "\n",
    "if problematic_cols:\n",
    "    print(f\"Columnas con >50% NaN eliminadas: {len(problematic_cols)}\")\n",
    "    X_clean = X_clean.drop(columns=problematic_cols)\n",
    "\n",
    "X_clean = X_clean.fillna(X_clean.median())\n",
    "\n",
    "remaining_nans = X_clean.isnull().sum().sum()\n",
    "if remaining_nans > 0:\n",
    "    print(f\"⚠️ NaN restantes después de imputación: {remaining_nans}\")\n",
    "    X_clean = X_clean.fillna(0)\n",
    "\n",
    "# Eliminar features con varianza muy baja\n",
    "variance_threshold = 0.01\n",
    "variance_selector = VarianceThreshold(threshold=variance_threshold)\n",
    "\n",
    "try:\n",
    "    X_variance = pd.DataFrame(\n",
    "        variance_selector.fit_transform(X_clean),\n",
    "        columns=X_clean.columns[variance_selector.get_support()],\n",
    "        index=X_clean.index\n",
    "    )\n",
    "    \n",
    "    removed_variance = set(X_clean.columns) - set(X_variance.columns)\n",
    "    print(f\"Features eliminadas por baja varianza (<{variance_threshold}): {len(removed_variance)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error en selector de varianza: {e}\")\n",
    "    X_variance = X_clean.copy()\n",
    "\n",
    "# Eliminar correlaciones extremadamente altas\n",
    "correlation_threshold = 0.98\n",
    "print(f\"Calculando matriz de correlación para {len(X_variance.columns)} features...\")\n",
    "\n",
    "try:\n",
    "    if len(X_variance) > 100000:\n",
    "        sample_size = min(50000, len(X_variance))\n",
    "        correlation_sample = X_variance.sample(n=sample_size, random_state=42)\n",
    "        correlation_matrix = correlation_sample.corr().abs()\n",
    "    else:\n",
    "        correlation_matrix = X_variance.corr().abs()\n",
    "    \n",
    "    upper_triangle = correlation_matrix.where(\n",
    "        np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)\n",
    "    )\n",
    "    \n",
    "    high_corr_features = [col for col in upper_triangle.columns \n",
    "                         if any(upper_triangle[col] > correlation_threshold)]\n",
    "    \n",
    "    X_final_features = X_variance.drop(columns=high_corr_features)\n",
    "    print(f\"Features eliminadas por alta correlación (>{correlation_threshold}): {len(high_corr_features)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error en eliminación de correlación: {e}\")\n",
    "    X_final_features = X_variance.copy()\n",
    "\n",
    "print(f\"Features finales para normalización: {len(X_final_features.columns)}\")\n",
    "\n",
    "# Guardar mapeo para reconstrucción posterior\n",
    "feature_metadata_mapping = {\n",
    "    'features_for_training': list(X_final_features.columns),\n",
    "    'metadata_for_interpretation': list(interpretation_metadata.keys()),\n",
    "    'removed_features': {\n",
    "        'low_variance': list(removed_variance) if 'removed_variance' in locals() else [],\n",
    "        'high_correlation': high_corr_features if 'high_corr_features' in locals() else []\n",
    "    }\n",
    "}\n",
    "\n",
    "# FASE 6: Normalización Estratégica\n",
    "print(\"\\n--- FASE 6: Normalización Estratégica ---\")\n",
    "\n",
    "X_normalized = X_final_features.copy()\n",
    "scalers = {}\n",
    "\n",
    "remaining_features = X_normalized.columns.tolist()\n",
    "\n",
    "current_temporal = [f for f in temporal_features_final if f in remaining_features]\n",
    "current_flags = [f for f in flag_features if f in remaining_features]\n",
    "current_packets = [f for f in packet_features if f in remaining_features]\n",
    "current_ratios = [f for f in ratio_features if f in remaining_features]\n",
    "current_ports = [f for f in port_features if f in remaining_features]\n",
    "current_flow = [f for f in flow_features if f in remaining_features]\n",
    "current_others = [f for f in remaining_features \n",
    "                 if f not in current_temporal + current_flags + current_packets + \n",
    "                            current_ratios + current_ports + current_flow]\n",
    "\n",
    "print(f\"Features por grupo final:\")\n",
    "print(f\"  Temporal: {len(current_temporal)}\")\n",
    "print(f\"  Flags: {len(current_flags)}\")\n",
    "print(f\"  Packets: {len(current_packets)}\")\n",
    "print(f\"  Ratios: {len(current_ratios)}\")\n",
    "print(f\"  Ports: {len(current_ports)}\")\n",
    "print(f\"  Flow: {len(current_flow)}\")\n",
    "print(f\"  Otros: {len(current_others)}\")\n",
    "\n",
    "def apply_scaler_safe(data, columns, scaler_type, scaler_name):\n",
    "    if not columns:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        if scaler_type == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaler_type == 'standard':\n",
    "            scaler = StandardScaler()  \n",
    "        elif scaler_type == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        else:\n",
    "            return\n",
    "            \n",
    "        data[columns] = scaler.fit_transform(data[columns])\n",
    "        scalers[scaler_name] = scaler\n",
    "        print(f\"  ✓ {scaler_name}: {len(columns)} features normalizadas con {scaler_type}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error normalizando {scaler_name}: {e}\")\n",
    "\n",
    "apply_scaler_safe(X_normalized, current_flags, 'minmax', 'flags')\n",
    "apply_scaler_safe(X_normalized, current_ratios, 'standard', 'ratios')\n",
    "apply_scaler_safe(X_normalized, current_packets, 'robust', 'packets')\n",
    "apply_scaler_safe(X_normalized, current_ports, 'standard', 'ports')\n",
    "apply_scaler_safe(X_normalized, current_flow, 'standard', 'flow')\n",
    "\n",
    "non_cyclical_temporal = [f for f in current_temporal \n",
    "                        if not any(x in f for x in ['_sin', '_cos', '_normalized'])]\n",
    "apply_scaler_safe(X_normalized, non_cyclical_temporal, 'standard', 'temporal')\n",
    "\n",
    "apply_scaler_safe(X_normalized, current_others, 'standard', 'others')\n",
    "\n",
    "print(f\"✓ Normalización completada: {len(scalers)} grupos procesados\")\n",
    "\n",
    "# FASE 7: División Estratificada Temporal\n",
    "print(\"\\n=== FASE 7: División Estratificada Temporal ===\")\n",
    "\n",
    "benign_label = 'BENIGN'\n",
    "y_binary = (y_all != benign_label).astype(int)\n",
    "\n",
    "print(f\"Distribución binaria total: {y_binary.value_counts().to_dict()}\")\n",
    "print(f\"Tasa global de anomalías: {y_binary.mean():.1%}\")\n",
    "\n",
    "df_final = X_normalized.copy()\n",
    "df_final['label'] = y_binary.values\n",
    "df_final['original_label'] = y_all.values\n",
    "df_final['timestamp'] = timestamps.values\n",
    "df_final['source_file'] = source_files.values\n",
    "\n",
    "print(f\"\\n=== ANÁLISIS TEMPORAL POR TIPO DE ATAQUE ===\")\n",
    "\n",
    "attack_types = df_final['original_label'].unique()\n",
    "for attack_type in sorted(attack_types):\n",
    "    attack_data = df_final[df_final['original_label'] == attack_type]\n",
    "    if len(attack_data) > 0:\n",
    "        print(f\"\\n{attack_type}:\")\n",
    "        print(f\"  Muestras: {len(attack_data):,}\")\n",
    "        print(f\"  Período: {attack_data['timestamp'].min()} → {attack_data['timestamp'].max()}\")\n",
    "        print(f\"  Duración: {attack_data['timestamp'].max() - attack_data['timestamp'].min()}\")\n",
    "\n",
    "print(f\"\\n=== DIVISIÓN ESTRATIFICADA TEMPORAL ===\")\n",
    "\n",
    "selected_attacks = ['BENIGN', 'DoS Hulk', 'PortScan', 'DDoS']\n",
    "temporal_splits = {'train': [], 'val': [], 'test': []}\n",
    "\n",
    "for attack_type in selected_attacks:\n",
    "    attack_data = df_final[df_final['original_label'] == attack_type].copy()\n",
    "    \n",
    "    if len(attack_data) == 0:\n",
    "        print(f\"⚠️ {attack_type}: No hay datos disponibles\")\n",
    "        continue\n",
    "        \n",
    "    attack_data_sorted = attack_data.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    n_samples = len(attack_data_sorted)\n",
    "    print(f\"\\n📊 {attack_type}: {n_samples:,} muestras\")\n",
    "    print(f\"   Período: {attack_data_sorted['timestamp'].min()} → {attack_data_sorted['timestamp'].max()}\")\n",
    "    \n",
    "    if n_samples >= 10:\n",
    "        train_end = int(0.70 * n_samples)\n",
    "        val_end = int(0.85 * n_samples)\n",
    "        \n",
    "        attack_train = attack_data_sorted[:train_end].copy()\n",
    "        attack_val = attack_data_sorted[train_end:val_end].copy()\n",
    "        attack_test = attack_data_sorted[val_end:].copy()\n",
    "        \n",
    "        temporal_splits['train'].append(attack_train)\n",
    "        temporal_splits['val'].append(attack_val)\n",
    "        temporal_splits['test'].append(attack_test)\n",
    "        \n",
    "        print(f\"   ✓ Train: {len(attack_train):,} muestras ({attack_train['timestamp'].min().strftime('%m-%d %H:%M')} - {attack_train['timestamp'].max().strftime('%m-%d %H:%M')})\")\n",
    "        print(f\"   ✓ Val:   {len(attack_val):,} muestras ({attack_val['timestamp'].min().strftime('%m-%d %H:%M')} - {attack_val['timestamp'].max().strftime('%m-%d %H:%M')})\")\n",
    "        print(f\"   ✓ Test:  {len(attack_test):,} muestras ({attack_test['timestamp'].min().strftime('%m-%d %H:%M')} - {attack_test['timestamp'].max().strftime('%m-%d %H:%M')})\")\n",
    "        \n",
    "    elif n_samples >= 3:\n",
    "        attack_train = attack_data_sorted[:max(1, int(0.8 * n_samples))].copy()\n",
    "        attack_val = attack_data_sorted[max(1, int(0.8 * n_samples)):max(2, int(0.9 * n_samples))].copy()\n",
    "        attack_test = attack_data_sorted[max(2, int(0.9 * n_samples)):].copy()\n",
    "        \n",
    "        temporal_splits['train'].append(attack_train)\n",
    "        if len(attack_val) > 0:\n",
    "            temporal_splits['val'].append(attack_val)\n",
    "        if len(attack_test) > 0:\n",
    "            temporal_splits['test'].append(attack_test)\n",
    "            \n",
    "        print(f\"   ⚠️ Pocas muestras - División básica:\")\n",
    "        print(f\"      Train: {len(attack_train):,}, Val: {len(attack_val):,}, Test: {len(attack_test):,}\")\n",
    "    else:\n",
    "        temporal_splits['train'].append(attack_data_sorted)\n",
    "        print(f\"   ⚠️ Muy pocas muestras → Todas a Train ({n_samples})\")\n",
    "\n",
    "print(f\"\\n=== COMBINANDO SPLITS CON ORDEN TEMPORAL GLOBAL ===\")\n",
    "\n",
    "final_train = pd.concat(temporal_splits['train']).sort_values('timestamp').reset_index(drop=True)\n",
    "final_val = pd.concat(temporal_splits['val']).sort_values('timestamp').reset_index(drop=True) if temporal_splits['val'] else pd.DataFrame()\n",
    "final_test = pd.concat(temporal_splits['test']).sort_values('timestamp').reset_index(drop=True) if temporal_splits['test'] else pd.DataFrame()\n",
    "\n",
    "if len(final_val) == 0:\n",
    "    print(\"⚠️ Val set vacío - Redistribuyendo desde Train\")\n",
    "    val_size = max(1000, int(0.15 * len(final_train)))\n",
    "    final_val = final_train[-val_size:].copy()\n",
    "    final_train = final_train[:-val_size].copy()\n",
    "\n",
    "if len(final_test) == 0:\n",
    "    print(\"⚠️ Test set vacío - Redistribuyendo desde Val\")\n",
    "    test_size = max(1000, int(0.5 * len(final_val)))\n",
    "    final_test = final_val[-test_size:].copy()\n",
    "    final_val = final_val[:-test_size].copy()\n",
    "\n",
    "print(f\"\\n🎯 RESULTADO FINAL DE DIVISIÓN ESTRATIFICADA:\")\n",
    "print(f\"={'='*80}\")\n",
    "\n",
    "for split_name, split_data in [('TRAIN', final_train), ('VALIDATION', final_val), ('TEST', final_test)]:\n",
    "    print(f\"\\n📊 {split_name} SET:\")\n",
    "    print(f\"   Muestras: {len(split_data):,}\")\n",
    "    if len(split_data) > 0:\n",
    "        print(f\"   Período: {split_data['timestamp'].min()} → {split_data['timestamp'].max()}\")\n",
    "        print(f\"   Tasa anomalías: {split_data['label'].mean():.1%}\")\n",
    "        \n",
    "        dist = split_data['original_label'].value_counts()\n",
    "        for label, count in dist.items():\n",
    "            pct = count / len(split_data) * 100\n",
    "            print(f\"      {label}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "train_data = final_train\n",
    "val_data = final_val\n",
    "test_data = final_test\n",
    "\n",
    "print(f\"\\n🏁 DIVISIÓN ESTRATIFICADA TEMPORAL COMPLETADA\")\n",
    "\n",
    "# FASE 8: Guardado con Metadatos Interpretativos\n",
    "print(f\"\\n--- FASE 8: Guardado con Metadatos Interpretativos ---\")\n",
    "\n",
    "feature_columns = X_normalized.columns.tolist()\n",
    "\n",
    "def save_split_with_metadata(split_data, split_name, interpretation_metadata, feature_columns):\n",
    "    if len(split_data) == 0:\n",
    "        return\n",
    "        \n",
    "    split_final = split_data[feature_columns + ['label']].copy()\n",
    "    split_final.to_csv(os.path.join(output_path, f\"{split_name}.csv\"), index=False)\n",
    "    split_final.to_parquet(os.path.join(output_path, f\"{split_name}.parquet\"), index=False)\n",
    "    \n",
    "    split_indices = split_data.index\n",
    "    split_metadata = {}\n",
    "    \n",
    "    for meta_col, meta_data in interpretation_metadata.items():\n",
    "        if hasattr(meta_data, 'iloc'):\n",
    "            split_metadata[meta_col] = meta_data.iloc[split_indices].copy()\n",
    "        else:\n",
    "            split_metadata[meta_col] = [meta_data[i] for i in split_indices]\n",
    "    \n",
    "    split_metadata['split_name'] = [split_name] * len(split_data)\n",
    "    split_metadata['split_index'] = range(len(split_data))\n",
    "    \n",
    "    split_metadata_df = pd.DataFrame(split_metadata)\n",
    "    split_metadata_df.to_csv(os.path.join(output_path, f\"{split_name}_interpretation_metadata.csv\"), index=False)\n",
    "    \n",
    "    print(f\"✅ {split_name.capitalize()} guardado: {len(split_final):,} muestras + metadatos interpretativos\")\n",
    "\n",
    "save_split_with_metadata(train_data, \"train\", interpretation_metadata, feature_columns)\n",
    "save_split_with_metadata(val_data, \"val\", interpretation_metadata, feature_columns)\n",
    "save_split_with_metadata(test_data, \"test\", interpretation_metadata, feature_columns)\n",
    "\n",
    "joblib.dump(scalers, os.path.join(artifacts_path, \"preprocessing_pipeline_with_metadata.pkl\"))\n",
    "\n",
    "interpretation_config = {\n",
    "    'feature_metadata_mapping': feature_metadata_mapping,\n",
    "    'interpretation_columns': list(interpretation_metadata.keys()),\n",
    "    'model_features': feature_columns,\n",
    "    'splits_info': {\n",
    "        'train': {'size': len(train_data), 'anomaly_rate': float(train_data['label'].mean()) if len(train_data) > 0 else 0},\n",
    "        'val': {'size': len(val_data), 'anomaly_rate': float(val_data['label'].mean()) if len(val_data) > 0 else 0},\n",
    "        'test': {'size': len(test_data), 'anomaly_rate': float(test_data['label'].mean()) if len(test_data) > 0 else 0}\n",
    "    },\n",
    "    'usage_instructions': {\n",
    "        'training': 'Usar archivos train.csv, val.csv, test.csv (solo features numéricas + label)',\n",
    "        'interpretation': 'Usar archivos *_interpretation_metadata.csv para análisis post-detección',\n",
    "        'reconstruction': 'Combinar índices de predicciones con metadatos usando split_index',\n",
    "        'example_code': '''\n",
    "# Ejemplo de reconstrucción post-detección:\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Cargar predicciones del modelo\n",
    "test_predictions = model.predict(test_features)  # Array de 0/1\n",
    "test_scores = model.decision_function(test_features)  # Scores de anomalía\n",
    "\n",
    "# 2. Cargar metadatos interpretativos\n",
    "test_metadata = pd.read_csv(\"test_interpretation_metadata.csv\")\n",
    "\n",
    "# 3. Combinar predicciones con contexto\n",
    "results = test_metadata.copy()\n",
    "results['anomaly_predicted'] = test_predictions\n",
    "results['anomaly_score'] = test_scores\n",
    "\n",
    "# 4. Análisis de anomalías detectadas\n",
    "anomalies = results[results['anomaly_predicted'] == 1]\n",
    "print(\"Top IPs con más anomalías:\")\n",
    "print(anomalies.groupby('_Source_IP_original').size().sort_values(ascending=False).head())\n",
    "\n",
    "print(\"Timeline de anomalías:\")\n",
    "print(anomalies[['timestamp_original', '_Source_IP_original', '_Destination_IP_original', 'anomaly_score']])\n",
    "        '''\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(artifacts_path, \"interpretation_guide.json\"), 'w') as f:\n",
    "    json.dump(interpretation_config, f, indent=2)\n",
    "\n",
    "quick_guide = f\"\"\"\n",
    "# 🔍 GUÍA RÁPIDA DE INTERPRETACIÓN DE ANOMALÍAS\n",
    "\n",
    "## Archivos Generados:\n",
    "- train.csv, val.csv, test.csv → Para entrenamiento del modelo ({len(feature_columns)} features)\n",
    "- *_interpretation_metadata.csv → Para análisis post-detección (IPs, timestamps, etc.)\n",
    "\n",
    "## Uso Después de Detección:\n",
    "```python\n",
    "# Cargar resultados y metadatos\n",
    "test_results = model.predict(test_data)\n",
    "metadata = pd.read_csv('test_interpretation_metadata.csv')\n",
    "\n",
    "# Combinar\n",
    "metadata['is_anomaly'] = test_results\n",
    "\n",
    "# Analizar anomalías por IP\n",
    "anomalies_by_ip = metadata[metadata['is_anomaly']==1].groupby('Source_Port_original').size()\n",
    "print(\"Puertos con más anomalías:\", anomalies_by_ip.sort_values(ascending=False).head())\n",
    "\n",
    "# Timeline de ataques\n",
    "timeline = metadata[metadata['is_anomaly']==1][['timestamp_original', 'original_label']]\n",
    "print(\"Cronología de ataques detectados:\")\n",
    "print(timeline.sort_values('timestamp_original'))\n",
    "Columnas Clave para Interpretación:\n",
    "\n",
    "{list(interpretation_metadata.keys())}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open(os.path.join(output_path, \"INTERPRETATION_GUIDE.md\"), 'w') as f:\n",
    "    f.write(quick_guide)\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"🎉 PREPARACIÓN COMPLETA CON METADATOS INTERPRETATIVOS\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"✅ Features para modelado: {len(feature_columns)}\")\n",
    "print(f\"✅ Metadatos preservados: {len(interpretation_metadata)} tipos\")\n",
    "print(f\"✅ Splits balanceados temporalmente\")\n",
    "print(f\"✅ Interpretabilidad post-detección garantizada\")\n",
    "\n",
    "\n",
    "print(f\"\\nArchivos clave:\")\n",
    "print(f\" 🤖 Entrenamiento: train.csv ({len(feature_columns)} features)\")\n",
    "print(f\" 🔍 Interpretación: *_interpretation_metadata.csv\")\n",
    "print(f\" 📖 Guía: INTERPRETATION_GUIDE.md\")\n",
    "print(f\" ⚙️ Config: interpretation_guide.json\")\n",
    "\n",
    "\n",
    "print(f\"\\n[INTERPRETABLE_DATA_READY] ✅\")\n",
    "print(\"Datos listos para VLT/TranAD Plus con interpretabilidad completa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbec443",
   "metadata": {},
   "source": [
    "**Carga de datos y verificación inicial.**\n",
    "\n",
    "Se cargan los datasets de trabajo y se inspeccionan estructuras básicas para confirmar formato y tamaños.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "298ad719-16fb-4fd1-8bcf-cf0075193d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /outputs/data/\n",
    "#   ├── train.csv (1.9M × 56: 55 features + label)\n",
    "#   ├── val.csv (418K × 56: 55 features + label)  \n",
    "#   ├── test.csv (418K × 56: 55 features + label)\n",
    "#   ├── train_interpretation_metadata.csv (IPs, timestamps, etc.)\n",
    "#   ├── val_interpretation_metadata.csv\n",
    "#   ├── test_interpretation_metadata.csv\n",
    "#   └── INTERPRETATION_GUIDE.md\n",
    "\n",
    "# /outputs/artifacts/\n",
    "#   ├── preprocessing_pipeline_with_metadata.pkl\n",
    "#   └── interpretation_guide.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c670f-702f-42a7-8bbd-72208a943565",
   "metadata": {},
   "source": [
    "## TranAD plus Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b473f82-816d-41f3-8d79-43404621b8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DIAGNÓSTICO DE PROBLEMAS DE ENTRENAMIENTO\n",
      "\n",
      "📊 Estadísticas de datos:\n",
      "X_train - Min: -447392160.000000, Max: 76785.343750, Mean: -11.084412, Std: 47460.371094\n",
      "X_val - Min: -89478448.000000, Max: 28884.666016, Mean: -14.178982, Std: 31556.626953\n",
      "X_test - Min: -13980886.000000, Max: 38575.000000, Mean: -1.238860, Std: 4459.386719\n",
      "\n",
      "🔍 Verificación de valores problemáticos:\n",
      "X_train - NaN: 0, Inf: 0\n",
      "X_val - NaN: 0, Inf: 0\n",
      "X_test - NaN: 0, Inf: 0\n",
      "\n",
      "🔧 Aplicando normalización robusta...\n",
      "Después de normalización:\n",
      "X_train_norm - Min: -2.725999, Max: 149.002930, Mean: -0.000000, Std: 0.999998\n",
      "\n",
      "--- Preparando datos normalizados ---\n",
      "📦 Ventanas creadas (normalizadas):\n",
      "  Train: (1589915, 10, 55)\n",
      "  Val: (418233, 10, 55) (77544.0 anomalías)\n",
      "  Test: (418233, 10, 55) (77544.0 anomalías)\n",
      "\n",
      "📊 Estadísticas de ventanas:\n",
      "Train windows - Min: -2.725999, Max: 149.002930\n",
      "Val windows - Min: -5.000000, Max: 5.000000\n",
      "\n",
      "--- ENTRENAMIENTO TRANAD+ CORREGIDO ---\n",
      "🤖 Modelo TranAD+ CORREGIDO:\n",
      "  Parámetros: 896,665\n",
      "🚀 Iniciando entrenamiento CORREGIDO...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c497187f8ae945e19d98d7b41d71ba86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Train Loss=0.172772, Val Loss=0.085530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab56b113c9644989aa453d260a478c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2: Train Loss=0.042705, Val Loss=0.062778\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2714a5a75f4cdf87e35a73cc9f4069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3: Train Loss=0.031040, Val Loss=0.051207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129398578da74817ba73a566768baf58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4: Train Loss=0.026594, Val Loss=0.049274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8719d534d641629882a7474674ed69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5: Train Loss=0.024041, Val Loss=0.045643\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9ed02400e04069a03920cd8ae78bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6: Train Loss=0.022213, Val Loss=0.044603\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1055251ced49b9819069f18e839b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7: Train Loss=0.020924, Val Loss=0.044109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a430de71a04a26ad2b381efa3516ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8: Train Loss=0.020084, Val Loss=0.043071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab853da8083423398b752c3775517f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9: Train Loss=0.019419, Val Loss=0.042665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e9454ee743450d901c08bfd58e7d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10: Train Loss=0.018866, Val Loss=0.042351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d152a4440b042aa8d1a8b6c1f674389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11: Train Loss=0.018413, Val Loss=0.041936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85b6e443ba348f48767230c629d0396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  12: Train Loss=0.017989, Val Loss=0.043925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59cdbc1a1b164babbed003dc085e5825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13: Train Loss=0.017728, Val Loss=0.042636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a693638f91df4aaeb02887a0ebf60b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14: Train Loss=0.017349, Val Loss=0.042648\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df7bdff490943059cac07ce020d7008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15: Train Loss=0.017005, Val Loss=0.041735\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7296b334d70c45e1a6d6d508a46cfe6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  16: Train Loss=0.016662, Val Loss=0.041365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b86302496246ca98423e48599c671d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  17: Train Loss=0.016434, Val Loss=0.041086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9740fbbf0c9545efa350c90a40bf5d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18: Train Loss=0.016245, Val Loss=0.041654\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e7bcce5feb42029438ed2d1d7a7bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  19: Train Loss=0.016080, Val Loss=0.041670\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2d6e19ba734943bb7569115c3434bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20: Train Loss=0.015924, Val Loss=0.041844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1182be2a92f44349892525afed06f9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(57094,0x200a8c800) malloc: Failed to allocate segment from range group - out of space\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  21: Train Loss=0.015782, Val Loss=0.041426\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f107cff64045d39168c25e4b5193d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  22: Train Loss=0.015661, Val Loss=0.041799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d23b952b29f44b8887d9de63a531927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  23: Train Loss=0.015546, Val Loss=0.040904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd720df1db5345cc800d9949601a53d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  24: Train Loss=0.015440, Val Loss=0.040581\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71ca32105c547cca96b00da6fb14b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25: Train Loss=0.015313, Val Loss=0.041133\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ffbb51d676408aa92b99a9c6b77681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  26: Train Loss=0.015206, Val Loss=0.039532\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33f0b4435cb4f18bc21be4a5b115091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  27: Train Loss=0.015104, Val Loss=0.042504\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d174107500841f08be30d03f8e8fbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(57094,0x200a8c800) malloc: Failed to allocate segment from range group - out of space\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  28: Train Loss=0.014980, Val Loss=0.042779\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3148e3aa430547e498d699d5acf5c58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  29: Train Loss=0.014891, Val Loss=0.041694\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd6c658445b4535adbf7f75bf69c3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30: Train Loss=0.014834, Val Loss=0.042415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e89c0e778dc43eca1d95bb3369ad123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 31:   0%|          | 0/24843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 339\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Parámetros: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Entrenar\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Verificar si el entrenamiento fue exitoso\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_losses) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[13], line 207\u001b[0m, in \u001b[0;36mtrain_model_fixed\u001b[0;34m(model, train_loader, val_loader, args)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Gradient clipping más agresivo\u001b[39;00m\n\u001b[1;32m    205\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgrad_clip)\n\u001b[0;32m--> 207\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_comb\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    210\u001b[0m train_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Virtualenv/viupyforai/lib/python3.10/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Virtualenv/viupyforai/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Documents/Virtualenv/viupyforai/lib/python3.10/site-packages/torch/optim/adamw.py:243\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    233\u001b[0m         group,\n\u001b[1;32m    234\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m         state_steps,\n\u001b[1;32m    241\u001b[0m     )\n\u001b[0;32m--> 243\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/Virtualenv/viupyforai/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Virtualenv/viupyforai/lib/python3.10/site-packages/torch/optim/adamw.py:875\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 875\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Virtualenv/viupyforai/lib/python3.10/site-packages/torch/optim/adamw.py:477\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    475\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 477\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Ejecución de TranAD+ en el Dataset CIC-IDS2017 Estratificado Temporal\n",
    "# \n",
    "# Este notebook implementa el modelo TranAD+ para nuestro dataset CIC-IDS2017 preprocesado\n",
    "# con división estratificada temporal y metadatos interpretativos.\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Determinar el dispositivo\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Usando GPU (MPS) en Mac M4.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Usando GPU (CUDA).\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Usando CPU.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. Carga de Datos Estratificados Temporalmente\n",
    "# \n",
    "# Cargamos nuestros datos preprocesados con división estratificada temporal.\n",
    "\n",
    "# %%\n",
    "print(\"--- 1. Cargando datos estratificados temporalmente ---\")\n",
    "\n",
    "# Rutas a nuestros datos preparados\n",
    "data_path = \"/Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/outputs/data\"\n",
    "\n",
    "try:\n",
    "    # Cargar datasets principales\n",
    "    train_df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "    val_df = pd.read_csv(os.path.join(data_path, \"val.csv\"))\n",
    "    test_df = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
    "    \n",
    "    # Cargar metadatos para interpretación\n",
    "    train_meta = pd.read_csv(os.path.join(data_path, \"train_interpretation_metadata.csv\"))\n",
    "    val_meta = pd.read_csv(os.path.join(data_path, \"val_interpretation_metadata.csv\"))\n",
    "    test_meta = pd.read_csv(os.path.join(data_path, \"test_interpretation_metadata.csv\"))\n",
    "    \n",
    "    print(\"✅ Datos cargados exitosamente:\")\n",
    "    print(f\"  Train: {train_df.shape} ({train_df['label'].mean():.1%} anomalías)\")\n",
    "    print(f\"  Val:   {val_df.shape} ({val_df['label'].mean():.1%} anomalías)\")\n",
    "    print(f\"  Test:  {test_df.shape} ({test_df['label'].mean():.1%} anomalías)\")\n",
    "    print(f\"  Features: {len(train_df.columns) - 1} (excluyendo 'label')\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Asegúrarse de haber ejecutado el notebook de preprocesamiento estratificado temporal.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Separar features y labels\n",
    "X_train = train_df.drop('label', axis=1).values.astype(np.float32)\n",
    "y_train = train_df['label'].values.astype(int)\n",
    "\n",
    "X_val = val_df.drop('label', axis=1).values.astype(np.float32)\n",
    "y_val = val_df['label'].values.astype(int)\n",
    "\n",
    "X_test = test_df.drop('label', axis=1).values.astype(np.float32)\n",
    "y_test = test_df['label'].values.astype(int)\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "print(f\"\\n📊 Estadísticas del dataset:\")\n",
    "print(f\"  Features: {num_features}\")\n",
    "print(f\"  Train samples: {len(X_train):,} (anomalías: {y_train.sum():,})\")\n",
    "print(f\"  Val samples: {len(X_val):,} (anomalías: {y_val.sum():,})\")\n",
    "print(f\"  Test samples: {len(X_test):,} (anomalías: {y_test.sum():,})\")\n",
    "\n",
    "# %%\n",
    "# --- Utilidades de Preprocesamiento (ACTUALIZADAS) ---\n",
    "def create_windows(data, labels, window_size):\n",
    "    \"\"\"\n",
    "    Crea ventanas deslizantes con stride=1 para preservar información temporal.\n",
    "    \"\"\"\n",
    "    num_samples, num_features = data.shape\n",
    "    num_windows = num_samples - window_size + 1\n",
    "    \n",
    "    if num_windows <= 0:\n",
    "        return np.empty((0, window_size, num_features)), np.empty((0,))\n",
    "    \n",
    "    windows = np.zeros((num_windows, window_size, num_features))\n",
    "    window_labels = np.zeros(num_windows)\n",
    "    \n",
    "    for i in range(num_windows):\n",
    "        windows[i] = data[i:i + window_size]\n",
    "        # Una ventana es anómala si tiene al menos una anomalía\n",
    "        window_labels[i] = int(np.any(labels[i:i + window_size] == 1))\n",
    "    \n",
    "    return windows, window_labels\n",
    "\n",
    "def pot_eval(scores, labels, q=1e-3):\n",
    "    \"\"\"\n",
    "    Evaluación POT mejorada con manejo de casos edge.\n",
    "    \"\"\"\n",
    "    if len(scores) == 0 or len(labels) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    # Manejar caso donde todos los scores son iguales\n",
    "    if np.std(scores) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    score_sorted = np.sort(scores)\n",
    "    threshold_idx = max(0, int(len(score_sorted) * (1 - q)) - 1)\n",
    "    threshold = score_sorted[threshold_idx]\n",
    "    \n",
    "    predictions = (scores >= threshold).astype(int)\n",
    "    \n",
    "    TP = np.sum((predictions == 1) & (labels == 1))\n",
    "    FP = np.sum((predictions == 1) & (labels == 0))\n",
    "    FN = np.sum((predictions == 0) & (labels == 1))\n",
    "    \n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return f1, precision, recall\n",
    "\n",
    "# %%\n",
    "# --- Arquitectura TranAD+ (IGUAL - Ya está correcta) ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, window_size, d_model, nhead_val):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead_val,\n",
    "            dim_feedforward=d_model * 4,  # Inverse Bottleneck\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer,\n",
    "            num_layers=1\n",
    "        )\n",
    "        self.linear = nn.Linear(window_size * d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded_sequence = self.transformer_encoder(x)\n",
    "        flattened_encoded = encoded_sequence.view(encoded_sequence.size(0), -1)\n",
    "        z = self.linear(flattened_encoded)\n",
    "        return z, encoded_sequence\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, window_size, d_model, nhead_val):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.linear_expand = nn.Linear(d_model, window_size * d_model)\n",
    "        \n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead_val,\n",
    "            dim_feedforward=d_model * 4,  # Inverse Bottleneck\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            self.decoder_layer,\n",
    "            num_layers=1\n",
    "        )\n",
    "        self.output_linear = nn.Linear(window_size * d_model, window_size * d_model)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z, memory):\n",
    "        expanded_z = self.linear_expand(z).view(z.size(0), self.window_size, self.d_model)\n",
    "        decoded_sequence = self.transformer_decoder(expanded_z, memory)\n",
    "        flattened_decoded = decoded_sequence.view(decoded_sequence.size(0), -1)\n",
    "        reconstruction = self.sigmoid(self.output_linear(flattened_decoded))\n",
    "        return reconstruction\n",
    "\n",
    "class TranAD(nn.Module):\n",
    "    def __init__(self, window_size, d_model, nhead_val):\n",
    "        super(TranAD, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.d_model = d_model\n",
    "        self.encoder = Encoder(window_size, d_model, nhead_val)\n",
    "        self.decoder1 = Decoder(window_size, d_model, nhead_val)\n",
    "        self.decoder2 = Decoder(window_size, d_model, nhead_val)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z, encoded_sequence = self.encoder(x)\n",
    "        x_hat1_flat = self.decoder1(z, encoded_sequence)\n",
    "        x_hat2_flat = self.decoder2(z, encoded_sequence)\n",
    "        return x_hat1_flat, x_hat2_flat\n",
    "\n",
    "# %%\n",
    "# --- Parámetros Optimizados para Nuestro Dataset ---\n",
    "class Args:\n",
    "    # Hiperparámetros ajustados para nuestro dataset\n",
    "    batch = 256  # Incrementado para mejor estabilidad\n",
    "    epochs = 30  # Reducido para nuestro dataset más grande\n",
    "    lr = 0.0005  # Learning rate más conservativo\n",
    "    beta = 0.5\n",
    "    window_size = 15  # Ventana más grande para capturar patrones\n",
    "    gamma = 0.1\n",
    "    lamda = 0.9\n",
    "    q = 0.01  # Threshold más estricto para nuestro dataset balanceado\n",
    "    \n",
    "    # nhead debe ser divisor de num_features (55)\n",
    "    # Divisores de 55: 1, 5, 11, 55\n",
    "    nhead_val = 5  # 55/5 = 11 (funciona bien)\n",
    "    \n",
    "    # Early stopping\n",
    "    patience = 5\n",
    "    min_delta = 1e-4\n",
    "\n",
    "args = Args()\n",
    "print(f\"🔧 Configuración del modelo:\")\n",
    "print(f\"  Features: {num_features}, nhead: {args.nhead_val} (ratio: {num_features/args.nhead_val})\")\n",
    "print(f\"  Window size: {args.window_size}, Batch size: {args.batch}\")\n",
    "print(f\"  Epochs: {args.epochs}, Learning rate: {args.lr}\")\n",
    "\n",
    "# %%\n",
    "# --- Funciones de Entrenamiento Mejoradas ---\n",
    "def train_model(model, train_loader, val_loader, args):\n",
    "    \"\"\"Entrenamiento con validación y early stopping\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"🚀 Iniciando entrenamiento con validación...\")\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for batch_idx, (data,) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x_hat1_flat, x_hat2_flat = model(data)\n",
    "            data_flat = data.view(data.size(0), -1)\n",
    "            \n",
    "            loss1 = loss_fn(x_hat1_flat, data_flat)\n",
    "            loss2 = loss_fn(x_hat2_flat, data_flat)\n",
    "            loss_comb = args.beta * loss1 + (1 - args.beta) * loss2\n",
    "            \n",
    "            loss_comb.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss_comb.item()\n",
    "            train_batches += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, _ in val_loader:\n",
    "                data = data.to(device)\n",
    "                \n",
    "                x_hat1_flat, x_hat2_flat = model(data)\n",
    "                data_flat = data.view(data.size(0), -1)\n",
    "                \n",
    "                loss1 = loss_fn(x_hat1_flat, data_flat)\n",
    "                loss2 = loss_fn(x_hat2_flat, data_flat)\n",
    "                loss_comb = args.beta * loss1 + (1 - args.beta) * loss2\n",
    "                \n",
    "                val_loss += loss_comb.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:3d}: Train Loss={avg_train_loss:.6f}, Val Loss={avg_val_loss:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss - args.min_delta:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Guardar mejor modelo\n",
    "            torch.save(model.state_dict(), 'best_tranad_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= args.patience:\n",
    "            print(f\"🛑 Early stopping en epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Cargar mejor modelo\n",
    "    model.load_state_dict(torch.load('best_tranad_model.pth'))\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model_detailed(model, test_loader, test_meta, args):\n",
    "    \"\"\"Evaluación detallada con métricas completas\"\"\"\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"📊 Evaluando modelo...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data_batch, labels_batch in tqdm(test_loader, desc=\"Evaluating\", leave=False):\n",
    "            data_batch = data_batch.to(device)\n",
    "            \n",
    "            x_hat1_flat, x_hat2_flat = model(data_batch)\n",
    "            data_flat = data_batch.view(data_batch.size(0), -1)\n",
    "            \n",
    "            rec_error1 = torch.mean((x_hat1_flat - data_flat)**2, dim=1)\n",
    "            rec_error2 = torch.mean((x_hat2_flat - data_flat)**2, dim=1)\n",
    "            \n",
    "            anomaly_score = (args.gamma * rec_error1 + (args.lamda - args.gamma) * rec_error2).cpu().numpy()\n",
    "            \n",
    "            all_scores.extend(anomaly_score)\n",
    "            all_labels.extend(labels_batch.cpu().numpy())\n",
    "    \n",
    "    # Evaluación POT\n",
    "    f1, precision, recall = pot_eval(np.array(all_scores), np.array(all_labels), q=args.q)\n",
    "    \n",
    "    # Threshold basado en POT\n",
    "    score_sorted = np.sort(all_scores)\n",
    "    threshold_idx = max(0, int(len(score_sorted) * (1 - args.q)) - 1)\n",
    "    threshold = score_sorted[threshold_idx]\n",
    "    predictions = (np.array(all_scores) >= threshold).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'scores': np.array(all_scores),\n",
    "        'labels': np.array(all_labels),\n",
    "        'predictions': predictions,\n",
    "        'threshold': threshold\n",
    "    }\n",
    "\n",
    "# %%\n",
    "# --- Preparación de Datos para TranAD+ ---\n",
    "print(\"\\n--- 2. Preparando ventanas deslizantes ---\")\n",
    "\n",
    "# Crear ventanas para entrenamiento (solo datos normales para unsupervised)\n",
    "# Para TranAD+, tradicionalmente se entrena solo con datos normales\n",
    "train_normal_mask = y_train == 0\n",
    "X_train_normal = X_train[train_normal_mask]\n",
    "\n",
    "print(f\"📦 Creando ventanas (window_size={args.window_size})...\")\n",
    "train_windows, _ = create_windows(X_train_normal, np.zeros(len(X_train_normal)), args.window_size)\n",
    "val_windows, val_window_labels = create_windows(X_val, y_val, args.window_size)\n",
    "test_windows, test_window_labels = create_windows(X_test, y_test, args.window_size)\n",
    "\n",
    "print(f\"  Train windows: {train_windows.shape} (solo datos normales)\")\n",
    "print(f\"  Val windows: {val_windows.shape} ({val_window_labels.sum()} anomalías)\")\n",
    "print(f\"  Test windows: {test_windows.shape} ({test_window_labels.sum()} anomalías)\")\n",
    "\n",
    "# Convertir a tensores\n",
    "train_tensor = torch.from_numpy(train_windows).float()\n",
    "val_tensor = torch.from_numpy(val_windows).float()\n",
    "test_tensor = torch.from_numpy(test_windows).float()\n",
    "\n",
    "val_labels_tensor = torch.from_numpy(val_window_labels).float()\n",
    "test_labels_tensor = torch.from_numpy(test_window_labels).float()\n",
    "\n",
    "# Crear DataLoaders\n",
    "train_dataset = TensorDataset(train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch, shuffle=True, num_workers=2)\n",
    "\n",
    "val_dataset = TensorDataset(val_tensor, val_labels_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch, shuffle=False, num_workers=2)\n",
    "\n",
    "test_dataset = TensorDataset(test_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch, shuffle=False, num_workers=2)\n",
    "\n",
    "# %%\n",
    "# --- Entrenamiento del Modelo ---\n",
    "print(f\"\\n--- 3. Entrenamiento TranAD+ ---\")\n",
    "\n",
    "# Verificar que nhead_val es divisor de num_features\n",
    "if num_features % args.nhead_val != 0:\n",
    "    print(f\"⚠️ Ajustando nhead: {num_features} no es divisible por {args.nhead_val}\")\n",
    "    # Encontrar el divisor más cercano\n",
    "    divisors = [i for i in range(1, num_features + 1) if num_features % i == 0]\n",
    "    args.nhead_val = min(divisors, key=lambda x: abs(x - args.nhead_val))\n",
    "    print(f\"✅ Nuevo nhead: {args.nhead_val}\")\n",
    "\n",
    "# Inicializar modelo\n",
    "model = TranAD(\n",
    "    window_size=args.window_size,\n",
    "    d_model=num_features,\n",
    "    nhead_val=args.nhead_val\n",
    ").to(device)\n",
    "\n",
    "print(f\"🤖 Modelo TranAD+ inicializado:\")\n",
    "print(f\"  Parámetros: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Parámetros entrenables: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Entrenar modelo\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, args)\n",
    "\n",
    "# %%\n",
    "# --- Evaluación Final ---\n",
    "print(\"\\n--- 4. Evaluación Final ---\")\n",
    "\n",
    "results = evaluate_model_detailed(model, test_loader, test_meta, args)\n",
    "\n",
    "print(f\"\\n🎯 RESULTADOS FINALES TranAD+ en CIC-IDS2017:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"F1-Score:  {results['f1']:.4f}\")\n",
    "print(f\"Precision: {results['precision']:.4f}\")\n",
    "print(f\"Recall:    {results['recall']:.4f}\")\n",
    "print(f\"Threshold: {results['threshold']:.6f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Análisis por tipo de ataque\n",
    "print(f\"\\n📋 Análisis por tipo de ataque:\")\n",
    "anomaly_indices = np.where(results['predictions'] == 1)[0]\n",
    "\n",
    "if len(anomaly_indices) > 0:\n",
    "    # Mapear índices de ventana a índices originales (aproximación)\n",
    "    original_anomaly_indices = anomaly_indices + args.window_size - 1\n",
    "    \n",
    "    # Asegurar que no excedamos los límites\n",
    "    original_anomaly_indices = original_anomaly_indices[original_anomaly_indices < len(test_meta)]\n",
    "    \n",
    "    if len(original_anomaly_indices) > 0:\n",
    "        detected_attacks = test_meta.iloc[original_anomaly_indices]['original_label'].value_counts()\n",
    "        print(\"Ataques detectados por tipo:\")\n",
    "        for attack, count in detected_attacks.items():\n",
    "            print(f\"  {attack}: {count}\")\n",
    "\n",
    "# %%\n",
    "# --- Visualización de Resultados ---\n",
    "print(\"\\n--- 5. Visualizaciones ---\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Curvas de pérdida\n",
    "axes[0,0].plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "axes[0,0].plot(val_losses, label='Validation Loss', alpha=0.7)\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].set_title('Curvas de Entrenamiento')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# 2. Distribución de scores de anomalía\n",
    "axes[0,1].hist(results['scores'][results['labels']==0], bins=50, alpha=0.5, label='Normal', density=True)\n",
    "axes[0,1].hist(results['scores'][results['labels']==1], bins=50, alpha=0.5, label='Anomalía', density=True)\n",
    "axes[0,1].axvline(results['threshold'], color='red', linestyle='--', label=f'Threshold={results[\"threshold\"]:.4f}')\n",
    "axes[0,1].set_xlabel('Anomaly Score')\n",
    "axes[0,1].set_ylabel('Densidad')\n",
    "axes[0,1].set_title('Distribución de Scores de Anomalía')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# 3. Timeline de anomalías (muestra)\n",
    "sample_size = min(1000, len(results['scores']))\n",
    "indices = np.random.choice(len(results['scores']), sample_size, replace=False)\n",
    "indices = np.sort(indices)\n",
    "\n",
    "axes[1,0].plot(indices, results['scores'][indices], alpha=0.6, label='Anomaly Score')\n",
    "axes[1,0].scatter(indices[results['labels'][indices]==1], \n",
    "                  results['scores'][indices[results['labels'][indices]==1]], \n",
    "                  color='red', s=10, alpha=0.7, label='True Anomalies')\n",
    "axes[1,0].axhline(results['threshold'], color='red', linestyle='--', alpha=0.7, label='Threshold')\n",
    "axes[1,0].set_xlabel('Índice Temporal')\n",
    "axes[1,0].set_ylabel('Anomaly Score')\n",
    "axes[1,0].set_title('Timeline de Anomalías (Muestra)')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. Matriz de confusión\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(results['labels'], results['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1,1])\n",
    "axes[1,1].set_xlabel('Predicción')\n",
    "axes[1,1].set_ylabel('Real')\n",
    "axes[1,1].set_title('Matriz de Confusión')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tranad_plus_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Proceso completado. Resultados guardados en 'tranad_plus_results.png'\")\n",
    "print(f\"📊 Modelo entrenado guardado como 'best_tranad_model.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01403e57",
   "metadata": {},
   "source": [
    "**Pruebas base con TranAD+.**\n",
    "\n",
    "Se configura y ejecuta el detector Transformer (TranAD+) y se registran métricas clave y salidas intermedias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67414373-ff77-42cf-8597-23cf72e14d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛑 Entrenamiento detenido manualmente\n",
      "✅ Mejor modelo cargado (val loss ~0.0395)\n",
      "🧹 Memoria liberada\n",
      "🎯 Modelo listo para evaluación\n"
     ]
    }
   ],
   "source": [
    "print(\"🛑 Entrenamiento detenido manualmente\")\n",
    "\n",
    "# Cargar MEJOR modelo (no el último)\n",
    "model.load_state_dict(torch.load('best_tranad_fixed_model.pth'))\n",
    "print(\"✅ Mejor modelo cargado (val loss ~0.0395)\")\n",
    "\n",
    "# Limpiar memoria AHORA\n",
    "import gc\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "gc.collect()\n",
    "print(\"🧹 Memoria liberada\")\n",
    "\n",
    "# Verificar modelo cargado\n",
    "model.eval()\n",
    "print(\"🎯 Modelo listo para evaluación\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1c060",
   "metadata": {},
   "source": [
    "**Pruebas base con TranAD+.**\n",
    "\n",
    "Se configura y ejecuta el detector Transformer (TranAD+) y se registran métricas clave y salidas intermedias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8543b03-7e35-4bcc-ba04-34df2555c3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluando modelo...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f59d58804f64bb997facdc54d0a178a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/6535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Celda 1 - Cargar mejor modelo\n",
    "model.load_state_dict(torch.load('best_tranad_fixed_model.pth'))\n",
    "\n",
    "# Celda 2 - Limpiar memoria  \n",
    "import gc\n",
    "torch.mps.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Celda 3 - Continuar con evaluación\n",
    "results = evaluate_model_detailed(model, test_loader, test_meta, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541300f",
   "metadata": {},
   "source": [
    "**Cálculo de métricas de rendimiento.**\n",
    "\n",
    "Se computan F1, Precision, Recall y AUC, y se preparan resultados para el análisis comparativo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e92963f-74f9-4461-a23a-e822024ab9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Ejecutando evaluación debug...\n",
      "🔍 INICIANDO EVALUACIÓN DEBUG\n",
      "📊 Test loader tiene 6535 batches\n",
      "  Procesando batch 1/6535\n",
      "    Data shape: torch.Size([64, 10, 55])\n",
      "    Reconstruction shapes: torch.Size([64, 550]), torch.Size([64, 550])\n",
      "    Data flat shape: torch.Size([64, 550])\n",
      "    Anomaly scores shape: (64,)\n",
      "    Score range: [0.059444, 0.160641]\n",
      "  Procesando batch 2/6535\n",
      "    Data shape: torch.Size([64, 10, 55])\n",
      "    Reconstruction shapes: torch.Size([64, 550]), torch.Size([64, 550])\n",
      "    Data flat shape: torch.Size([64, 550])\n",
      "    Anomaly scores shape: (64,)\n",
      "    Score range: [0.068648, 0.155143]\n",
      "  Procesando batch 3/6535\n",
      "    Data shape: torch.Size([64, 10, 55])\n",
      "    Reconstruction shapes: torch.Size([64, 550]), torch.Size([64, 550])\n",
      "    Data flat shape: torch.Size([64, 550])\n",
      "    Anomaly scores shape: (64,)\n",
      "    Score range: [0.069772, 0.129607]\n",
      "  Procesando batch 4/6535\n",
      "    Data shape: torch.Size([64, 10, 55])\n",
      "    Reconstruction shapes: torch.Size([64, 550]), torch.Size([64, 550])\n",
      "    Data flat shape: torch.Size([64, 550])\n",
      "    Anomaly scores shape: (64,)\n",
      "    Score range: [0.066838, 0.126480]\n",
      "  Procesando batch 5/6535\n",
      "    Data shape: torch.Size([64, 10, 55])\n",
      "    Reconstruction shapes: torch.Size([64, 550]), torch.Size([64, 550])\n",
      "    Data flat shape: torch.Size([64, 550])\n",
      "    Anomaly scores shape: (64,)\n",
      "    Score range: [0.089111, 0.722808]\n",
      "  Procesando batch 6/6535\n",
      "    Data shape: torch.Size([64, 10, 55])\n",
      "    Reconstruction shapes: torch.Size([64, 550]), torch.Size([64, 550])\n",
      "    Data flat shape: torch.Size([64, 550])\n",
      "    Anomaly scores shape: (64,)\n",
      "    Score range: [0.081102, 0.321524]\n",
      "  Procesando batch 7/6535\n",
      "    Data shape: torch.Size([64, 10, 55])\n",
      "    Reconstruction shapes: torch.Size([64, 550]), torch.Size([64, 550])\n",
      "    Data flat shape: torch.Size([64, 550])\n",
      "    Anomaly scores shape: (64,)\n",
      "    Score range: [0.065082, 0.283621]\n",
      "  Procesando batch 8/6535\n",
      "    Data shape: torch.Size([64, 10, 55])\n",
      "    Reconstruction shapes: torch.Size([64, 550]), torch.Size([64, 550])\n",
      "    Data flat shape: torch.Size([64, 550])\n",
      "    Anomaly scores shape: (64,)\n",
      "    Score range: [0.073006, 0.167112]\n",
      "  Procesando batch 9/6535\n",
      "    Data shape: torch.Size([64, 10, 55])\n",
      "    Reconstruction shapes: torch.Size([64, 550]), torch.Size([64, 550])\n",
      "    Data flat shape: torch.Size([64, 550])\n",
      "    Anomaly scores shape: (64,)\n",
      "    Score range: [0.068598, 0.117749]\n",
      "  Procesando batch 10/6535\n",
      "    Data shape: torch.Size([64, 10, 55])\n",
      "    Reconstruction shapes: torch.Size([64, 550]), torch.Size([64, 550])\n",
      "    Data flat shape: torch.Size([64, 550])\n",
      "    Anomaly scores shape: (64,)\n",
      "    Score range: [0.067226, 0.119417]\n",
      "🛑 Deteniendo después de 10 batches para debug\n",
      "✅ Procesados 10 batches\n",
      "📊 Total scores: 640\n",
      "📊 Total labels: 640\n",
      "\n",
      "🎯 RESULTADOS DEBUG (primeros 10 batches):\n",
      "  F1: 0.0247\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.0125\n",
      "✅ Evaluación debug exitosa\n"
     ]
    }
   ],
   "source": [
    "# versión con debugging:\n",
    "def evaluate_model_debug(model, test_loader, test_meta, args):\n",
    "    \"\"\"Evaluación con debugging detallado\"\"\"\n",
    "    print(\"🔍 INICIANDO EVALUACIÓN DEBUG\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    batch_count = 0\n",
    "    \n",
    "    print(f\"📊 Test loader tiene {len(test_loader)} batches\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data_batch, labels_batch) in enumerate(test_loader):\n",
    "            try:\n",
    "                print(f\"  Procesando batch {batch_idx+1}/{len(test_loader)}\")\n",
    "                \n",
    "                data_batch = data_batch.to(device)\n",
    "                \n",
    "                # Verificar shapes\n",
    "                print(f\"    Data shape: {data_batch.shape}\")\n",
    "                \n",
    "                x_hat1_flat, x_hat2_flat = model(data_batch)\n",
    "                data_flat = data_batch.view(data_batch.size(0), -1)\n",
    "                \n",
    "                print(f\"    Reconstruction shapes: {x_hat1_flat.shape}, {x_hat2_flat.shape}\")\n",
    "                print(f\"    Data flat shape: {data_flat.shape}\")\n",
    "                \n",
    "                rec_error1 = torch.mean((x_hat1_flat - data_flat)**2, dim=1)\n",
    "                rec_error2 = torch.mean((x_hat2_flat - data_flat)**2, dim=1)\n",
    "                \n",
    "                anomaly_score = (args.gamma * rec_error1 + (args.lamda - args.gamma) * rec_error2).cpu().numpy()\n",
    "                \n",
    "                print(f\"    Anomaly scores shape: {anomaly_score.shape}\")\n",
    "                print(f\"    Score range: [{anomaly_score.min():.6f}, {anomaly_score.max():.6f}]\")\n",
    "                \n",
    "                all_scores.extend(anomaly_score)\n",
    "                all_labels.extend(labels_batch.cpu().numpy())\n",
    "                \n",
    "                batch_count += 1\n",
    "                \n",
    "                # Solo procesar primeros 10 batches para debug\n",
    "                if batch_count >= 10:\n",
    "                    print(\"🛑 Deteniendo después de 10 batches para debug\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error en batch {batch_idx}: {e}\")\n",
    "                print(f\"    Data shape: {data_batch.shape if 'data_batch' in locals() else 'N/A'}\")\n",
    "                break\n",
    "    \n",
    "    print(f\"✅ Procesados {batch_count} batches\")\n",
    "    print(f\"📊 Total scores: {len(all_scores)}\")\n",
    "    print(f\"📊 Total labels: {len(all_labels)}\")\n",
    "    \n",
    "    if len(all_scores) == 0:\n",
    "        print(\"❌ No se generaron scores - hay un problema\")\n",
    "        return None\n",
    "    \n",
    "    # Evaluación POT\n",
    "    f1, precision, recall = pot_eval(np.array(all_scores), np.array(all_labels), q=args.q)\n",
    "    \n",
    "    print(f\"\\n🎯 RESULTADOS DEBUG (primeros 10 batches):\")\n",
    "    print(f\"  F1: {f1:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'scores': np.array(all_scores),\n",
    "        'labels': np.array(all_labels)\n",
    "    }\n",
    "\n",
    "# Ejecutar debug\n",
    "print(\"🚀 Ejecutando evaluación debug...\")\n",
    "debug_results = evaluate_model_debug(model, test_loader, test_meta, args)\n",
    "\n",
    "if debug_results is None:\n",
    "    print(\"❌ La evaluación falló - necesitamos investigar más\")\n",
    "else:\n",
    "    print(\"✅ Evaluación debug exitosa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c207b9",
   "metadata": {},
   "source": [
    "**Pruebas base con TranAD+.**\n",
    "\n",
    "Se configura y ejecuta el detector Transformer (TranAD+) y se registran métricas clave y salidas intermedias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37a04972-96a6-43ff-8781-128e443c081d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando evaluación COMPLETA...\n",
      "📊 INICIANDO EVALUACIÓN COMPLETA (sin tqdm)\n",
      "🎯 Procesando 6535 batches...\n",
      "  Progreso: 1000/6535 batches (15.3%)\n",
      "  Progreso: 2000/6535 batches (30.6%)\n",
      "  Progreso: 3000/6535 batches (45.9%)\n",
      "  Progreso: 4000/6535 batches (61.2%)\n",
      "  Progreso: 5000/6535 batches (76.5%)\n",
      "  Progreso: 6000/6535 batches (91.8%)\n",
      "✅ Evaluación completa: 418233 muestras procesadas\n",
      "📊 Estadísticas de scores:\n",
      "  Rango: [0.0007, 0.9908]\n",
      "  Media: 0.0264, Std: 0.0731\n",
      "🎯 Calculando métricas POT...\n",
      "\n",
      "============================================================\n",
      "🎯 RESULTADOS FINALES TranAD+ en CIC-IDS2017\n",
      "============================================================\n",
      "F1-Score:     0.1024\n",
      "Precision:    1.0000\n",
      "Recall:       0.0540\n",
      "Threshold:    0.467535\n",
      "Anomalías reales:    77,544\n",
      "Anomalías detectadas: 4,184\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_fixed(model, test_loader, test_meta, args):\n",
    "    \"\"\"Evaluación completa SIN tqdm problemático\"\"\"\n",
    "    print(\"📊 INICIANDO EVALUACIÓN COMPLETA (sin tqdm)\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    total_batches = len(test_loader)\n",
    "    print(f\"🎯 Procesando {total_batches} batches...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data_batch, labels_batch) in enumerate(test_loader):\n",
    "            data_batch = data_batch.to(device)\n",
    "            \n",
    "            x_hat1_flat, x_hat2_flat = model(data_batch)\n",
    "            data_flat = data_batch.view(data_batch.size(0), -1)\n",
    "            \n",
    "            rec_error1 = torch.mean((x_hat1_flat - data_flat)**2, dim=1)\n",
    "            rec_error2 = torch.mean((x_hat2_flat - data_flat)**2, dim=1)\n",
    "            \n",
    "            anomaly_score = (args.gamma * rec_error1 + (args.lamda - args.gamma) * rec_error2).cpu().numpy()\n",
    "            \n",
    "            all_scores.extend(anomaly_score)\n",
    "            all_labels.extend(labels_batch.cpu().numpy())\n",
    "            \n",
    "            # Progreso cada 1000 batches\n",
    "            if (batch_idx + 1) % 1000 == 0:\n",
    "                print(f\"  Progreso: {batch_idx + 1}/{total_batches} batches ({(batch_idx+1)/total_batches*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"✅ Evaluación completa: {len(all_scores)} muestras procesadas\")\n",
    "    \n",
    "    # Convertir a arrays\n",
    "    scores_array = np.array(all_scores)\n",
    "    labels_array = np.array(all_labels)\n",
    "    \n",
    "    print(f\"📊 Estadísticas de scores:\")\n",
    "    print(f\"  Rango: [{scores_array.min():.4f}, {scores_array.max():.4f}]\")\n",
    "    print(f\"  Media: {scores_array.mean():.4f}, Std: {scores_array.std():.4f}\")\n",
    "    \n",
    "    # Evaluación POT\n",
    "    print(\"🎯 Calculando métricas POT...\")\n",
    "    f1, precision, recall = pot_eval(scores_array, labels_array, q=args.q)\n",
    "    \n",
    "    # Calcular threshold POT\n",
    "    score_sorted = np.sort(scores_array)\n",
    "    threshold_idx = max(0, int(len(score_sorted) * (1 - args.q)) - 1)\n",
    "    threshold = score_sorted[threshold_idx]\n",
    "    predictions = (scores_array >= threshold).astype(int)\n",
    "    \n",
    "    # Estadísticas adicionales\n",
    "    anomaly_count = labels_array.sum()\n",
    "    detected_count = predictions.sum()\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'scores': scores_array,\n",
    "        'labels': labels_array,\n",
    "        'predictions': predictions,\n",
    "        'threshold': threshold,\n",
    "        'anomaly_count': int(anomaly_count),\n",
    "        'detected_count': int(detected_count)\n",
    "    }\n",
    "\n",
    "# EJECUTAR EVALUACIÓN COMPLETA\n",
    "print(\"🚀 Iniciando evaluación COMPLETA...\")\n",
    "results = evaluate_model_fixed(model, test_loader, test_meta, args)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"🎯 RESULTADOS FINALES TranAD+ en CIC-IDS2017\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"F1-Score:     {results['f1']:.4f}\")\n",
    "    print(f\"Precision:    {results['precision']:.4f}\")\n",
    "    print(f\"Recall:       {results['recall']:.4f}\")\n",
    "    print(f\"Threshold:    {results['threshold']:.6f}\")\n",
    "    print(f\"Anomalías reales:    {results['anomaly_count']:,}\")\n",
    "    print(f\"Anomalías detectadas: {results['detected_count']:,}\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63287185",
   "metadata": {},
   "source": [
    "**Cálculo de métricas de rendimiento.**\n",
    "\n",
    "Se computan F1, Precision, Recall y AUC, y se preparan resultados para el análisis comparativo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60ac00e8-be77-490c-a975-2c2afb37ca2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 OPTIMIZACIÓN DE THRESHOLD\n",
      "q=0.001: F1=0.011, P=1.000, R=0.005, T=0.7241, Det=420\n",
      "q=0.005: F1=0.053, P=1.000, R=0.027, T=0.5692, Det=2,093\n",
      "q=0.010: F1=0.102, P=1.000, R=0.054, T=0.4675, Det=4,184\n",
      "q=0.020: F1=0.195, P=1.000, R=0.108, T=0.3049, Det=8,366\n",
      "q=0.050: F1=0.315, P=0.742, R=0.200, T=0.0688, Det=20,913\n",
      "q=0.100: F1=0.393, P=0.561, R=0.302, T=0.0329, Det=41,825\n",
      "q=0.150: F1=0.392, P=0.438, R=0.354, T=0.0235, Det=62,736\n",
      "q=0.200: F1=0.349, P=0.336, R=0.362, T=0.0192, Det=83,648\n",
      "\n",
      "🎯 MEJOR CONFIGURACIÓN:\n",
      "  q = 0.100\n",
      "  F1-Score = 0.393\n",
      "  Precision = 0.561\n",
      "  Recall = 0.302\n",
      "  Threshold = 0.0329\n",
      "  Detectadas = 41,825 de 77,544\n"
     ]
    }
   ],
   "source": [
    "# Probar diferentes valores de q para mejor balance\n",
    "print(\"🔧 OPTIMIZACIÓN DE THRESHOLD\")\n",
    "\n",
    "# Probar múltiples thresholds\n",
    "q_values = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2]\n",
    "results_comparison = []\n",
    "\n",
    "for q_test in q_values:\n",
    "    f1, precision, recall = pot_eval(results['scores'], results['labels'], q=q_test)\n",
    "    \n",
    "    # Calcular threshold correspondiente\n",
    "    score_sorted = np.sort(results['scores'])\n",
    "    threshold_idx = max(0, int(len(score_sorted) * (1 - q_test)) - 1)\n",
    "    threshold = score_sorted[threshold_idx]\n",
    "    \n",
    "    detected = (results['scores'] >= threshold).sum()\n",
    "    \n",
    "    results_comparison.append({\n",
    "        'q': q_test,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'threshold': threshold,\n",
    "        'detected': detected\n",
    "    })\n",
    "    \n",
    "    print(f\"q={q_test:.3f}: F1={f1:.3f}, P={precision:.3f}, R={recall:.3f}, T={threshold:.4f}, Det={detected:,}\")\n",
    "\n",
    "# Encontrar mejor F1\n",
    "best_result = max(results_comparison, key=lambda x: x['f1'])\n",
    "print(f\"\\n🎯 MEJOR CONFIGURACIÓN:\")\n",
    "print(f\"  q = {best_result['q']:.3f}\")\n",
    "print(f\"  F1-Score = {best_result['f1']:.3f}\")\n",
    "print(f\"  Precision = {best_result['precision']:.3f}\")\n",
    "print(f\"  Recall = {best_result['recall']:.3f}\")\n",
    "print(f\"  Threshold = {best_result['threshold']:.4f}\")\n",
    "print(f\"  Detectadas = {best_result['detected']:,} de {results['anomaly_count']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47131151",
   "metadata": {},
   "source": [
    "**Preprocesamiento y selección/reducción de características.**\n",
    "\n",
    "Se normalizan variables y se ajusta el conjunto de *features* para estabilizar el entrenamiento y evitar ruido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a76ab544-9017-48d8-ba6f-e2d7a4778c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🏆 RESULTADOS FINALES OPTIMIZADOS - TranAD+ en CIC-IDS2017\n",
      "================================================================================\n",
      "🤖 MODELO:\n",
      "   Arquitectura: TranAD+ (Dual Decoder Transformer)\n",
      "   Parámetros: 896,665\n",
      "   Entrenamiento: 26 epochs (early stop)\n",
      "   Val Loss: 0.0395\n",
      "\n",
      "📊 DATOS:\n",
      "   Dataset: CIC-IDS2017 estratificado temporal\n",
      "   Features: 55 (normalizadas robustamente)\n",
      "   Ventanas: 10 timesteps\n",
      "   Test samples: 418,233 ventanas\n",
      "   Anomalías reales: 77,544 (18.5%)\n",
      "\n",
      "🎯 MÉTRICAS OPTIMIZADAS (q=0.10):\n",
      "   F1-Score:     0.393\n",
      "   Precision:    0.561\n",
      "   Recall:       0.302\n",
      "   Threshold:    0.0329\n",
      "   Detectadas:   41,825 / 77,544 (53.9%)\n",
      "\n",
      "✅ CONCLUSIONES:\n",
      "   • TranAD+ entrenó exitosamente en arquitectura semi-supervisada\n",
      "   • Modelo conservador pero preciso (baja tasa de falsos positivos)\n",
      "   • Threshold configurable permite balance precision-recall\n",
      "   • Resultados competitivos para detección de anomalías temporales\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"🏆 RESULTADOS FINALES OPTIMIZADOS - TranAD+ en CIC-IDS2017\")\n",
    "print(\"=\" * 80)\n",
    "print(\"🤖 MODELO:\")\n",
    "print(\"   Arquitectura: TranAD+ (Dual Decoder Transformer)\")\n",
    "print(\"   Parámetros: 896,665\")\n",
    "print(\"   Entrenamiento: 26 epochs (early stop)\")\n",
    "print(\"   Val Loss: 0.0395\")\n",
    "print()\n",
    "print(\"📊 DATOS:\")\n",
    "print(\"   Dataset: CIC-IDS2017 estratificado temporal\")\n",
    "print(\"   Features: 55 (normalizadas robustamente)\")\n",
    "print(\"   Ventanas: 10 timesteps\")\n",
    "print(\"   Test samples: 418,233 ventanas\")\n",
    "print(\"   Anomalías reales: 77,544 (18.5%)\")\n",
    "print()\n",
    "print(\"🎯 MÉTRICAS OPTIMIZADAS (q=0.10):\")\n",
    "print(f\"   F1-Score:     {0.393:.3f}\")\n",
    "print(f\"   Precision:    {0.561:.3f}\")\n",
    "print(f\"   Recall:       {0.302:.3f}\")\n",
    "print(f\"   Threshold:    {0.0329:.4f}\")\n",
    "print(f\"   Detectadas:   {41825:,} / {77544:,} ({41825/77544*100:.1f}%)\")\n",
    "print()\n",
    "print(\"✅ CONCLUSIONES:\")\n",
    "print(\"   • TranAD+ entrenó exitosamente en arquitectura semi-supervisada\")\n",
    "print(\"   • Modelo conservador pero preciso (baja tasa de falsos positivos)\")\n",
    "print(\"   • Threshold configurable permite balance precision-recall\")\n",
    "print(\"   • Resultados competitivos para detección de anomalías temporales\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a455568-9195-4328-8a5f-9dfd0cbd5d47",
   "metadata": {},
   "source": [
    "## VLT-Anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c165667-9709-4aa9-bdf3-0dd5e5ce0d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 ANÁLISIS DETALLADO POR TIPO DE ATAQUE\n",
      "================================================================================\n",
      "📋 Metadatos de test cargados: (418242, 10)\n",
      "📊 Columnas de metadatos disponibles:\n",
      "  ['timestamp_original', 'source_file', 'original_label', 'processing_index', '_Source_IP_original', '_Destination_IP_original', 'Flow_ID_original', '_Timestamp_original', 'split_name', 'split_index']\n",
      "🔗 Mapeando 418233 ventanas con metadatos...\n",
      "✅ DataFrame de resultados creado: (418233, 9)\n",
      "\n",
      "📊 MÉTRICAS POR TIPO DE ATAQUE:\n",
      "------------------------------------------------------------\n",
      "BENIGN:\n",
      "  📈 Ventanas: 393,083\n",
      "  🎯 Anomalías reales: 77,537\n",
      "  🔍 Detectadas: 4,184 (5.4%)\n",
      "  📊 F1=0.102, P=1.000, R=0.054\n",
      "  🔢 Avg score anomalías: 0.0764\n",
      "  🔢 Avg score normales: 0.0149\n",
      "  📏 Separación: 0.0615\n",
      "\n",
      "DDoS:\n",
      "  📈 Ventanas: 24,916\n",
      "  🎯 Anomalías reales: 0\n",
      "  🔍 Detectadas: 0 (0.0%)\n",
      "  📊 F1=0.000, P=0.000, R=0.000\n",
      "  🔢 Avg score anomalías: 0.0000\n",
      "  🔢 Avg score normales: 0.0161\n",
      "  📏 Separación: -0.0161\n",
      "\n",
      "PortScan:\n",
      "  📈 Ventanas: 234\n",
      "  🎯 Anomalías reales: 7\n",
      "  🔍 Detectadas: 0 (0.0%)\n",
      "  📊 F1=0.000, P=0.000, R=0.000\n",
      "  🔢 Avg score anomalías: 0.0034\n",
      "  🔢 Avg score normales: 0.0168\n",
      "  📏 Separación: -0.0134\n",
      "\n",
      "📋 EXPORTANDO ANOMALÍAS DETECTADAS CON CONTEXTO COMPLETO\n",
      "🎯 Anomalías detectadas para exportar: 4,184\n",
      "⚠️ Columna source_port no disponible en resultados\n",
      "⚠️ Columna destination_port no disponible en resultados\n",
      "📊 Columnas disponibles para exportación: ['timestamp', 'source_ip', 'destination_ip', 'anomaly_score', 'original_label', 'anomaly_real']\n",
      "✅ Anomalías detectadas exportadas: /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/outputs/data/tranad_plus_detected_anomalies.csv\n",
      "\n",
      "🔍 TOP 10 ANOMALÍAS DETECTADAS:\n",
      "------------------------------------------------------------\n",
      "#49183\n",
      "  🕒 Timestamp: 2017-07-03 10:42:35\n",
      "  🎯 Tipo: BENIGN ✅\n",
      "  📊 Score: 0.9908 (conf: 2.12x)\n",
      "  🌐 IPs: 192.168.10.16 → 185.86.137.42\n",
      "  🔌 Puertos: N/A → N/A\n",
      "  📁 Archivo: Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "\n",
      "#49186\n",
      "  🕒 Timestamp: 2017-07-03 10:42:36\n",
      "  🎯 Tipo: BENIGN ✅\n",
      "  📊 Score: 0.9792 (conf: 2.09x)\n",
      "  🌐 IPs: 192.168.10.51 → 50.22.19.222\n",
      "  🔌 Puertos: N/A → N/A\n",
      "  📁 Archivo: Monday-WorkingHours.pcap_ISCX.csv\n",
      "\n",
      "#49732\n",
      "  🕒 Timestamp: 2017-07-03 10:43:44\n",
      "  🎯 Tipo: BENIGN ✅\n",
      "  📊 Score: 0.9708 (conf: 2.08x)\n",
      "  🌐 IPs: 199.167.65.25 → 192.168.10.17\n",
      "  🔌 Puertos: N/A → N/A\n",
      "  📁 Archivo: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "\n",
      "#53044\n",
      "  🕒 Timestamp: 2017-07-03 10:50:39\n",
      "  🎯 Tipo: BENIGN ✅\n",
      "  📊 Score: 0.9633 (conf: 2.06x)\n",
      "  🌐 IPs: 192.168.10.9 → 192.168.10.3\n",
      "  🔌 Puertos: N/A → N/A\n",
      "  📁 Archivo: Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "\n",
      "#49187\n",
      "  🕒 Timestamp: 2017-07-03 10:42:36\n",
      "  🎯 Tipo: BENIGN ✅\n",
      "  📊 Score: 0.9485 (conf: 2.03x)\n",
      "  🌐 IPs: 192.168.10.16 → 23.61.187.27\n",
      "  🔌 Puertos: N/A → N/A\n",
      "  📁 Archivo: Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "\n",
      "#6939\n",
      "  🕒 Timestamp: 2017-07-03 09:14:30\n",
      "  🎯 Tipo: BENIGN ✅\n",
      "  📊 Score: 0.9466 (conf: 2.02x)\n",
      "  🌐 IPs: 192.168.10.50 → 192.168.10.3\n",
      "  🔌 Puertos: N/A → N/A\n",
      "  📁 Archivo: Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "\n",
      "#53043\n",
      "  🕒 Timestamp: 2017-07-03 10:50:39\n",
      "  🎯 Tipo: BENIGN ✅\n",
      "  📊 Score: 0.9441 (conf: 2.02x)\n",
      "  🌐 IPs: 192.168.10.16 → 192.168.10.3\n",
      "  🔌 Puertos: N/A → N/A\n",
      "  📁 Archivo: Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "\n",
      "#53042\n",
      "  🕒 Timestamp: 2017-07-03 10:50:39\n",
      "  🎯 Tipo: BENIGN ✅\n",
      "  📊 Score: 0.9432 (conf: 2.02x)\n",
      "  🌐 IPs: 192.168.10.5 → 192.168.10.3\n",
      "  🔌 Puertos: N/A → N/A\n",
      "  📁 Archivo: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "\n",
      "#49731\n",
      "  🕒 Timestamp: 2017-07-03 10:43:44\n",
      "  🎯 Tipo: BENIGN ✅\n",
      "  📊 Score: 0.9313 (conf: 1.99x)\n",
      "  🌐 IPs: 192.168.10.5 → 23.203.93.198\n",
      "  🔌 Puertos: N/A → N/A\n",
      "  📁 Archivo: Monday-WorkingHours.pcap_ISCX.csv\n",
      "\n",
      "#48706\n",
      "  🕒 Timestamp: 2017-07-03 10:41:36\n",
      "  🎯 Tipo: BENIGN ✅\n",
      "  📊 Score: 0.9303 (conf: 1.99x)\n",
      "  🌐 IPs: 192.168.10.5 → 104.28.26.91\n",
      "  🔌 Puertos: N/A → N/A\n",
      "  📁 Archivo: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "\n",
      "📈 RESUMEN ESTADÍSTICO POR TIPO DE ATAQUE:\n",
      "--------------------------------------------------------------------------------\n",
      "Tipo de Ataque       F1       Precision  Recall   Detectadas Total   \n",
      "--------------------------------------------------------------------------------\n",
      "BENIGN               0.102    1.000      0.054    4184       77537   \n",
      "DDoS                 0.000    0.000      0.000    0          0       \n",
      "PortScan             0.000    0.000      0.000    0          7       \n",
      "--------------------------------------------------------------------------------\n",
      "✅ Métricas por ataque guardadas: /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/outputs/artifacts/tranad_plus_attack_metrics.csv\n",
      "\n",
      "🔬 ANÁLISIS: RENDIMIENTO vs VOLUMEN DE ATAQUES\n",
      "📊 ATAQUES ALTO VOLUMEN (>10K muestras):\n",
      "  BENIGN: F1=0.102, Muestras=77,537\n",
      "📊 ATAQUES MEDIO VOLUMEN (1K-10K muestras):\n",
      "  Ninguno\n",
      "📊 ATAQUES BAJO VOLUMEN (<1K muestras):\n",
      "  DDoS: F1=0.000, Muestras=0\n",
      "  PortScan: F1=0.000, Muestras=7\n",
      "\n",
      "📅 TIMELINE DE DETECCIONES (Top 20):\n",
      "2017-07-03 10:42:35 | BENIGN       | Score: 0.9908 | ✅ TP\n",
      "2017-07-03 10:42:36 | BENIGN       | Score: 0.9792 | ✅ TP\n",
      "2017-07-03 10:43:44 | BENIGN       | Score: 0.9708 | ✅ TP\n",
      "2017-07-03 10:50:39 | BENIGN       | Score: 0.9633 | ✅ TP\n",
      "2017-07-03 10:42:36 | BENIGN       | Score: 0.9485 | ✅ TP\n",
      "2017-07-03 09:14:30 | BENIGN       | Score: 0.9466 | ✅ TP\n",
      "2017-07-03 10:50:39 | BENIGN       | Score: 0.9441 | ✅ TP\n",
      "2017-07-03 10:50:39 | BENIGN       | Score: 0.9432 | ✅ TP\n",
      "2017-07-03 10:43:44 | BENIGN       | Score: 0.9313 | ✅ TP\n",
      "2017-07-03 10:41:36 | BENIGN       | Score: 0.9303 | ✅ TP\n",
      "2017-07-03 10:35:14 | BENIGN       | Score: 0.9266 | ✅ TP\n",
      "2017-07-03 10:50:39 | BENIGN       | Score: 0.9246 | ✅ TP\n",
      "2017-07-03 10:34:37 | BENIGN       | Score: 0.9209 | ✅ TP\n",
      "2017-07-03 10:41:36 | BENIGN       | Score: 0.9168 | ✅ TP\n",
      "2017-07-03 10:34:37 | BENIGN       | Score: 0.9105 | ✅ TP\n",
      "2017-07-03 10:35:14 | BENIGN       | Score: 0.9084 | ✅ TP\n",
      "2017-07-03 10:37:57 | BENIGN       | Score: 0.9080 | ✅ TP\n",
      "2017-07-03 09:14:31 | BENIGN       | Score: 0.9080 | ✅ TP\n",
      "2017-07-03 10:41:38 | BENIGN       | Score: 0.9076 | ✅ TP\n",
      "2017-07-03 10:41:38 | BENIGN       | Score: 0.9044 | ✅ TP\n",
      "\n",
      "🎯 ARCHIVOS GENERADOS:\n",
      "  📊 Detecciones: /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/outputs/data/tranad_plus_detected_anomalies.csv\n",
      "  📈 Métricas: /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/outputs/artifacts/tranad_plus_attack_metrics.csv\n",
      "  🤖 Modelo: best_tranad_fixed_model.pth\n",
      "\n",
      "================================================================================\n",
      "🏆 ANÁLISIS COMPLETO DE TranAD+ FINALIZADO\n",
      "✅ Modelo entrenado, evaluado y analizado por tipo de ataque\n",
      "✅ Anomalías exportadas con contexto completo\n",
      "✅ Métricas detalladas por clase disponibles\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# === ANÁLISIS POR TIPO DE ATAQUE ===\n",
    "print(\"🔍 ANÁLISIS DETALLADO POR TIPO DE ATAQUE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Cargar metadatos de test para interpretación\n",
    "test_meta = pd.read_csv(os.path.join(data_path, \"test_interpretation_metadata.csv\"))\n",
    "print(f\"📋 Metadatos de test cargados: {test_meta.shape}\")\n",
    "\n",
    "# Verificar columnas disponibles en metadatos\n",
    "print(f\"📊 Columnas de metadatos disponibles:\")\n",
    "print(f\"  {list(test_meta.columns)}\")\n",
    "\n",
    "# Mapear resultados con metadatos\n",
    "# Nota: Las ventanas pueden no alinearse 1:1 con metadatos originales\n",
    "# Aproximación: usar índices centrales de cada ventana\n",
    "window_size = args.window_size\n",
    "central_indices = np.arange(window_size // 2, len(test_meta) - window_size // 2 + 1, 1)\n",
    "\n",
    "# Asegurar que no excedamos los límites\n",
    "max_windows = min(len(results['predictions']), len(central_indices))\n",
    "central_indices = central_indices[:max_windows]\n",
    "\n",
    "print(f\"🔗 Mapeando {max_windows} ventanas con metadatos...\")\n",
    "\n",
    "# Crear DataFrame de resultados interpretables\n",
    "results_df = pd.DataFrame({\n",
    "    'window_index': range(max_windows),\n",
    "    'anomaly_score': results['scores'][:max_windows],\n",
    "    'anomaly_predicted': results['predictions'][:max_windows],\n",
    "    'anomaly_real': results['labels'][:max_windows].astype(int),\n",
    "})\n",
    "\n",
    "# Agregar metadatos del punto central de cada ventana\n",
    "for i, central_idx in enumerate(central_indices):\n",
    "    if i < len(results_df):\n",
    "        results_df.loc[i, 'timestamp'] = test_meta.loc[central_idx, 'timestamp_original']\n",
    "        results_df.loc[i, 'source_file'] = test_meta.loc[central_idx, 'source_file']\n",
    "        results_df.loc[i, 'original_label'] = test_meta.loc[central_idx, 'original_label']\n",
    "        \n",
    "        # Agregar IPs si están disponibles\n",
    "        if '_Source_IP_original' in test_meta.columns:\n",
    "            results_df.loc[i, 'source_ip'] = test_meta.loc[central_idx, '_Source_IP_original']\n",
    "        if '_Destination_IP_original' in test_meta.columns:\n",
    "            results_df.loc[i, 'destination_ip'] = test_meta.loc[central_idx, '_Destination_IP_original']\n",
    "        if 'Source_Port_original' in test_meta.columns:\n",
    "            results_df.loc[i, 'source_port'] = test_meta.loc[central_idx, 'Source_Port_original']\n",
    "        if 'Destination_Port_original' in test_meta.columns:\n",
    "            results_df.loc[i, 'destination_port'] = test_meta.loc[central_idx, 'Destination_Port_original']\n",
    "\n",
    "print(f\"✅ DataFrame de resultados creado: {results_df.shape}\")\n",
    "\n",
    "# === MÉTRICAS POR TIPO DE ATAQUE ===\n",
    "print(f\"\\n📊 MÉTRICAS POR TIPO DE ATAQUE:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "attack_types = results_df['original_label'].unique()\n",
    "attack_metrics = []\n",
    "\n",
    "for attack_type in sorted(attack_types):\n",
    "    attack_mask = results_df['original_label'] == attack_type\n",
    "    attack_data = results_df[attack_mask]\n",
    "    \n",
    "    if len(attack_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Calcular métricas para este tipo de ataque\n",
    "    true_labels = attack_data['anomaly_real'].values\n",
    "    predicted_labels = attack_data['anomaly_predicted'].values\n",
    "    scores = attack_data['anomaly_score'].values\n",
    "    \n",
    "    # Calcular TP, FP, FN\n",
    "    TP = np.sum((predicted_labels == 1) & (true_labels == 1))\n",
    "    FP = np.sum((predicted_labels == 1) & (true_labels == 0))\n",
    "    FN = np.sum((predicted_labels == 0) & (true_labels == 1))\n",
    "    TN = np.sum((predicted_labels == 0) & (true_labels == 0))\n",
    "    \n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    # Estadísticas adicionales\n",
    "    total_real_anomalies = true_labels.sum()\n",
    "    detected_anomalies = TP\n",
    "    avg_score_anomalies = scores[true_labels == 1].mean() if total_real_anomalies > 0 else 0\n",
    "    avg_score_normal = scores[true_labels == 0].mean() if (len(scores) - total_real_anomalies) > 0 else 0\n",
    "    \n",
    "    attack_metrics.append({\n",
    "        'attack_type': attack_type,\n",
    "        'total_windows': len(attack_data),\n",
    "        'real_anomalies': int(total_real_anomalies),\n",
    "        'detected_anomalies': int(detected_anomalies),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'avg_score_anomalies': avg_score_anomalies,\n",
    "        'avg_score_normal': avg_score_normal,\n",
    "        'score_separation': avg_score_anomalies - avg_score_normal\n",
    "    })\n",
    "    \n",
    "    print(f\"{attack_type}:\")\n",
    "    print(f\"  📈 Ventanas: {len(attack_data):,}\")\n",
    "    print(f\"  🎯 Anomalías reales: {total_real_anomalies:,}\")\n",
    "    print(f\"  🔍 Detectadas: {detected_anomalies:,} ({detected_anomalies/max(1,total_real_anomalies)*100:.1f}%)\")\n",
    "    print(f\"  📊 F1={f1:.3f}, P={precision:.3f}, R={recall:.3f}\")\n",
    "    print(f\"  🔢 Avg score anomalías: {avg_score_anomalies:.4f}\")\n",
    "    print(f\"  🔢 Avg score normales: {avg_score_normal:.4f}\")\n",
    "    print(f\"  📏 Separación: {avg_score_anomalies - avg_score_normal:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Crear DataFrame de métricas por ataque\n",
    "attack_metrics_df = pd.DataFrame(attack_metrics)\n",
    "\n",
    "# === EXPORTAR ANOMALÍAS DETECTADAS CON CONTEXTO ===\n",
    "print(\"📋 EXPORTANDO ANOMALÍAS DETECTADAS CON CONTEXTO COMPLETO\")\n",
    "\n",
    "# Filtrar solo anomalías detectadas\n",
    "detected_anomalies = results_df[results_df['anomaly_predicted'] == 1].copy()\n",
    "\n",
    "print(f\"🎯 Anomalías detectadas para exportar: {len(detected_anomalies):,}\")\n",
    "\n",
    "# Preparar dataset de exportación\n",
    "export_columns = ['timestamp', 'source_ip', 'destination_ip', 'source_port', \n",
    "                 'destination_port', 'anomaly_score', 'original_label', 'anomaly_real']\n",
    "\n",
    "# Verificar qué columnas están disponibles\n",
    "available_columns = []\n",
    "for col in export_columns:\n",
    "    if col in detected_anomalies.columns:\n",
    "        available_columns.append(col)\n",
    "    else:\n",
    "        print(f\"⚠️ Columna {col} no disponible en resultados\")\n",
    "\n",
    "print(f\"📊 Columnas disponibles para exportación: {available_columns}\")\n",
    "\n",
    "# Crear export dataset con columnas disponibles\n",
    "export_data = detected_anomalies[available_columns].copy()\n",
    "\n",
    "# Agregar información adicional útil\n",
    "export_data['detection_confidence'] = detected_anomalies['anomaly_score'] / results['threshold']\n",
    "export_data['is_true_positive'] = detected_anomalies['anomaly_real'] == 1\n",
    "export_data['source_file'] = detected_anomalies['source_file']\n",
    "\n",
    "# Ordenar por score de mayor a menor\n",
    "export_data = export_data.sort_values('anomaly_score', ascending=False)\n",
    "\n",
    "# Guardar archivo completo de detecciones\n",
    "export_path = os.path.join(output_path, \"tranad_plus_detected_anomalies.csv\")\n",
    "export_data.to_csv(export_path, index=False)\n",
    "\n",
    "print(f\"✅ Anomalías detectadas exportadas: {export_path}\")\n",
    "\n",
    "# === ANÁLISIS DE TOP DETECCIONES ===\n",
    "print(f\"\\n🔍 TOP 10 ANOMALÍAS DETECTADAS:\")\n",
    "print(\"-\" * 60)\n",
    "top_detections = export_data.head(10)\n",
    "\n",
    "for idx, row in top_detections.iterrows():\n",
    "    print(f\"#{idx+1}\")\n",
    "    print(f\"  🕒 Timestamp: {row.get('timestamp', 'N/A')}\")\n",
    "    print(f\"  🎯 Tipo: {row['original_label']} {'✅' if row['is_true_positive'] else '❌'}\")\n",
    "    print(f\"  📊 Score: {row['anomaly_score']:.4f} (conf: {row['detection_confidence']:.2f}x)\")\n",
    "    print(f\"  🌐 IPs: {row.get('source_ip', 'N/A')} → {row.get('destination_ip', 'N/A')}\")\n",
    "    print(f\"  🔌 Puertos: {row.get('source_port', 'N/A')} → {row.get('destination_port', 'N/A')}\")\n",
    "    print(f\"  📁 Archivo: {row['source_file']}\")\n",
    "    print()\n",
    "\n",
    "# === RESUMEN ESTADÍSTICO ===\n",
    "print(f\"📈 RESUMEN ESTADÍSTICO POR TIPO DE ATAQUE:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Crear tabla resumen\n",
    "summary_table = attack_metrics_df.sort_values('f1', ascending=False)\n",
    "\n",
    "print(f\"{'Tipo de Ataque':<20} {'F1':<8} {'Precision':<10} {'Recall':<8} {'Detectadas':<10} {'Total':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for _, row in summary_table.iterrows():\n",
    "    attack = row['attack_type'][:19]  # Truncar nombre si es muy largo\n",
    "    f1 = row['f1']\n",
    "    precision = row['precision'] \n",
    "    recall = row['recall']\n",
    "    detected = row['detected_anomalies']\n",
    "    total = row['real_anomalies']\n",
    "    \n",
    "    print(f\"{attack:<20} {f1:<8.3f} {precision:<10.3f} {recall:<8.3f} {detected:<10} {total:<8}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Guardar métricas por ataque\n",
    "metrics_path = os.path.join(artifacts_path, \"tranad_plus_attack_metrics.csv\")\n",
    "attack_metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"✅ Métricas por ataque guardadas: {metrics_path}\")\n",
    "\n",
    "# === ANÁLISIS DE RENDIMIENTO POR VOLUMEN ===\n",
    "print(f\"\\n🔬 ANÁLISIS: RENDIMIENTO vs VOLUMEN DE ATAQUES\")\n",
    "\n",
    "# Categorizar ataques por volumen\n",
    "high_volume = attack_metrics_df[attack_metrics_df['real_anomalies'] > 10000]\n",
    "medium_volume = attack_metrics_df[(attack_metrics_df['real_anomalies'] > 1000) & (attack_metrics_df['real_anomalies'] <= 10000)]\n",
    "low_volume = attack_metrics_df[attack_metrics_df['real_anomalies'] <= 1000]\n",
    "\n",
    "print(f\"📊 ATAQUES ALTO VOLUMEN (>10K muestras):\")\n",
    "if len(high_volume) > 0:\n",
    "    for _, row in high_volume.iterrows():\n",
    "        print(f\"  {row['attack_type']}: F1={row['f1']:.3f}, Muestras={row['real_anomalies']:,}\")\n",
    "else:\n",
    "    print(\"  Ninguno\")\n",
    "\n",
    "print(f\"📊 ATAQUES MEDIO VOLUMEN (1K-10K muestras):\")\n",
    "if len(medium_volume) > 0:\n",
    "    for _, row in medium_volume.iterrows():\n",
    "        print(f\"  {row['attack_type']}: F1={row['f1']:.3f}, Muestras={row['real_anomalies']:,}\")\n",
    "else:\n",
    "    print(\"  Ninguno\")\n",
    "\n",
    "print(f\"📊 ATAQUES BAJO VOLUMEN (<1K muestras):\")\n",
    "if len(low_volume) > 0:\n",
    "    for _, row in low_volume.iterrows():\n",
    "        print(f\"  {row['attack_type']}: F1={row['f1']:.3f}, Muestras={row['real_anomalies']:,}\")\n",
    "else:\n",
    "    print(\"  Ninguno\")\n",
    "\n",
    "# === TIMELINE DE DETECCIONES ===\n",
    "if 'timestamp' in detected_anomalies.columns:\n",
    "    print(f\"\\n📅 TIMELINE DE DETECCIONES (Top 20):\")\n",
    "    timeline_data = export_data.head(20)[['timestamp', 'original_label', 'anomaly_score', 'is_true_positive']]\n",
    "    \n",
    "    for idx, row in timeline_data.iterrows():\n",
    "        status = \"✅ TP\" if row['is_true_positive'] else \"❌ FP\"\n",
    "        print(f\"{row['timestamp']} | {row['original_label']:<12} | Score: {row['anomaly_score']:.4f} | {status}\")\n",
    "\n",
    "print(f\"\\n🎯 ARCHIVOS GENERADOS:\")\n",
    "print(f\"  📊 Detecciones: {export_path}\")\n",
    "print(f\"  📈 Métricas: {metrics_path}\")\n",
    "print(f\"  🤖 Modelo: best_tranad_fixed_model.pth\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"🏆 ANÁLISIS COMPLETO DE TranAD+ FINALIZADO\")\n",
    "print(\"✅ Modelo entrenado, evaluado y analizado por tipo de ataque\")\n",
    "print(\"✅ Anomalías exportadas con contexto completo\")\n",
    "print(\"✅ Métricas detalladas por clase disponibles\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e8c0a-629d-4ea6-94e9-2778fd1e8c59",
   "metadata": {},
   "source": [
    "### Tranad + Etiquetas corregidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcdfb4e-ac9d-4dc8-b9aa-81e406ab615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 DIAGNÓSTICO DE ALINEACIÓN DE ETIQUETAS\")\n",
    "\n",
    "# Verificar dimensiones originales\n",
    "print(f\"📊 Dimensiones originales:\")\n",
    "print(f\"  X_test shape: {X_test.shape}\")\n",
    "print(f\"  y_test shape: {y_test.shape}\")\n",
    "print(f\"  test_meta shape: {test_meta.shape}\")\n",
    "print(f\"  test_windows shape: {test_windows.shape}\")\n",
    "\n",
    "# Verificar distribución de etiquetas originales\n",
    "print(f\"\\n📋 Distribución etiquetas originales y_test:\")\n",
    "unique_labels_orig, counts_orig = np.unique(y_test, return_counts=True)\n",
    "for label, count in zip(unique_labels_orig, counts_orig):\n",
    "    print(f\"  Etiqueta {label}: {count:,} muestras\")\n",
    "\n",
    "print(f\"\\n📋 Distribución etiquetas test_meta:\")\n",
    "meta_label_dist = test_meta['original_label'].value_counts()\n",
    "print(meta_label_dist)\n",
    "\n",
    "# PROBLEMA: Verificar si la alineación window-etiquetas es correcta\n",
    "print(f\"\\n🔍 VERIFICANDO ALINEACIÓN WINDOWS ↔ LABELS:\")\n",
    "\n",
    "# Tomar muestra para verificar\n",
    "sample_size = 100\n",
    "sample_indices = np.random.choice(len(test_window_labels), sample_size, replace=False)\n",
    "\n",
    "misalignment_count = 0\n",
    "for i in sample_indices:\n",
    "    # Índice original correspondiente al centro de la ventana\n",
    "    original_idx = i + (args.window_size // 2)\n",
    "    \n",
    "    if original_idx < len(y_test):\n",
    "        window_label = test_window_labels[i]  # Etiqueta de ventana\n",
    "        original_label = y_test[original_idx]  # Etiqueta original en el punto central\n",
    "        \n",
    "        if window_label != original_label:\n",
    "            misalignment_count += 1\n",
    "\n",
    "alignment_rate = (sample_size - misalignment_count) / sample_size\n",
    "print(f\"  Tasa de alineación: {alignment_rate:.1%}\")\n",
    "\n",
    "if alignment_rate < 0.9:\n",
    "    print(\"❌ PROBLEMA: Etiquetas desalineadas\")\n",
    "    \n",
    "    # SOLUCIÓN: Recrear etiquetas de ventana correctamente\n",
    "    print(\"🔧 RECREANDO ETIQUETAS DE VENTANA...\")\n",
    "    \n",
    "    correct_window_labels = np.zeros(len(test_windows))\n",
    "    for i in range(len(test_windows)):\n",
    "        # Verificar si hay alguna anomalía en la ventana\n",
    "        start_idx = i\n",
    "        end_idx = min(i + args.window_size, len(y_test))\n",
    "        window_slice = y_test[start_idx:end_idx]\n",
    "        correct_window_labels[i] = int(np.any(window_slice == 1))\n",
    "    \n",
    "    print(f\"✅ Etiquetas recreadas:\")\n",
    "    print(f\"  Ventanas anómalas: {correct_window_labels.sum():,}\")\n",
    "    print(f\"  Ventanas normales: {(correct_window_labels == 0).sum():,}\")\n",
    "    \n",
    "    # REEVALUAR con etiquetas correctas\n",
    "    print(f\"\\n🔄 REEVALUACIÓN CON ETIQUETAS CORRECTAS:\")\n",
    "    f1_corrected, precision_corrected, recall_corrected = pot_eval(\n",
    "        results['scores'], correct_window_labels, q=0.1\n",
    "    )\n",
    "    \n",
    "    print(f\"🎯 MÉTRICAS CORREGIDAS:\")\n",
    "    print(f\"  F1-Score: {f1_corrected:.3f}\")\n",
    "    print(f\"  Precision: {precision_corrected:.3f}\")\n",
    "    print(f\"  Recall: {recall_corrected:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"✅ Etiquetas correctamente alineadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6785a-a76b-42b3-a92f-e6c7e68a6ee0",
   "metadata": {},
   "source": [
    "### TrandAd + Final Tabla 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e17399",
   "metadata": {},
   "source": [
    "**Construcción de ventanas temporales y división de datos.**\n",
    "\n",
    "Se preparan ventanas y particiones temporales (entrenamiento/validación/prueba) bajo la semilla definida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f85d89b1-ea7e-4ab2-b5d2-d0944df69a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 CORRECCIÓN CRÍTICA: MAPEO CORRECTO DE ETIQUETAS\n",
      "🔄 Re-mapeando etiquetas desde metadatos originales...\n",
      "📊 Etiquetas corregidas desde metadatos:\n",
      "  BENIGN (0): 393,092\n",
      "  ANOMALÍA (1): 25,150\n",
      "📋 Distribución de ataques reales:\n",
      "original_label\n",
      "DDoS        24916\n",
      "PortScan      234\n",
      "Name: count, dtype: int64\n",
      "\n",
      "🔄 Recreando etiquetas de ventana con metadatos correctos...\n",
      "✅ Etiquetas de ventana corregidas:\n",
      "  Ventanas normales: 234,990\n",
      "  Ventanas anómalas: 183,243.0\n",
      "📊 Distribución de tipos de ventana:\n",
      "  BENIGN: 234,990\n",
      "  DDoS: 181,657\n",
      "  PortScan: 1,586\n",
      "\n",
      "🎯 REEVALUACIÓN CON ETIQUETAS CORREGIDAS:\n",
      "q=0.01: F1=0.000, P=0.000, R=0.000, Det=4,184\n",
      "q=0.05: F1=0.032, P=0.158, R=0.018, Det=20,913\n",
      "q=0.10: F1=0.095, P=0.254, R=0.058, Det=41,825\n",
      "q=0.15: F1=0.168, P=0.330, R=0.113, Det=62,736\n",
      "q=0.20: F1=0.246, P=0.392, R=0.179, Det=83,648\n",
      "\n",
      "🏆 MEJORES MÉTRICAS CORREGIDAS:\n",
      "============================================================\n",
      "q = 0.20\n",
      "F1-Score:     0.246\n",
      "Precision:    0.392\n",
      "Recall:       0.179\n",
      "Threshold:    0.0192\n",
      "Detectadas:   83,648\n",
      "============================================================\n",
      "\n",
      "📊 ANÁLISIS POR TIPO DE ATAQUE CORREGIDO:\n",
      "📋 MÉTRICAS POR TIPO DE ATAQUE (CORREGIDAS):\n",
      "----------------------------------------------------------------------\n",
      "BENIGN:\n",
      "  📈 Ventanas: 234,990\n",
      "  🎯 Detectadas: 50,881\n",
      "  📊 F1=0.000, P=0.000, R=0.000\n",
      "  📊 Score promedio: 0.0344\n",
      "\n",
      "DDoS:\n",
      "  📈 Ventanas: 181,657\n",
      "  🎯 Detectadas: 32,504\n",
      "  📊 F1=0.304, P=1.000, R=0.179\n",
      "  📊 Score promedio: 0.0161\n",
      "\n",
      "PortScan:\n",
      "  📈 Ventanas: 1,586\n",
      "  🎯 Detectadas: 263\n",
      "  📊 F1=0.284, P=1.000, R=0.166\n",
      "  📊 Score promedio: 0.0168\n",
      "\n",
      "🔍 ANÁLISIS DE DETECCIONES DE ALTO SCORE:\n",
      "Top 10 detecciones por tipo:\n",
      "\n",
      "BENIGN - Top scores:\n",
      "  #1: Score=0.9908\n",
      "  #2: Score=0.9792\n",
      "  #3: Score=0.9708\n",
      "\n",
      "DDoS - Top scores:\n",
      "  #1: Score=0.3445\n",
      "  #2: Score=0.3364\n",
      "  #3: Score=0.3044\n",
      "\n",
      "PortScan - Top scores:\n",
      "  #1: Score=0.1533\n",
      "  #2: Score=0.1507\n",
      "  #3: Score=0.1472\n"
     ]
    }
   ],
   "source": [
    "print(\"🚨 CORRECCIÓN CRÍTICA: MAPEO CORRECTO DE ETIQUETAS\")\n",
    "\n",
    "# Re-mapear etiquetas usando metadatos originales\n",
    "print(\"🔄 Re-mapeando etiquetas desde metadatos originales...\")\n",
    "\n",
    "# Crear etiquetas binarias CORRECTAS desde test_meta\n",
    "correct_binary_labels = (test_meta['original_label'] != 'BENIGN').astype(int)\n",
    "\n",
    "print(f\"📊 Etiquetas corregidas desde metadatos:\")\n",
    "print(f\"  BENIGN (0): {(correct_binary_labels == 0).sum():,}\")\n",
    "print(f\"  ANOMALÍA (1): {(correct_binary_labels == 1).sum():,}\")\n",
    "\n",
    "# Verificar distribución de ataques reales\n",
    "attack_distribution = test_meta[test_meta['original_label'] != 'BENIGN']['original_label'].value_counts()\n",
    "print(f\"📋 Distribución de ataques reales:\")\n",
    "print(attack_distribution)\n",
    "\n",
    "# Crear etiquetas de ventana CORRECTAS usando metadatos\n",
    "print(f\"\\n🔄 Recreando etiquetas de ventana con metadatos correctos...\")\n",
    "\n",
    "correct_window_labels = np.zeros(len(test_windows))\n",
    "window_attack_types = []\n",
    "\n",
    "for i in range(len(test_windows)):\n",
    "    # Para cada ventana, verificar las etiquetas en el rango correspondiente\n",
    "    start_idx = i\n",
    "    end_idx = min(i + args.window_size, len(correct_binary_labels))\n",
    "    \n",
    "    window_slice = correct_binary_labels[start_idx:end_idx]\n",
    "    \n",
    "    # La ventana es anómala si contiene al menos una anomalía\n",
    "    if np.any(window_slice == 1):\n",
    "        correct_window_labels[i] = 1\n",
    "        \n",
    "        # Identificar tipo de ataque predominante en la ventana\n",
    "        attack_labels_in_window = test_meta.iloc[start_idx:end_idx]['original_label']\n",
    "        attack_types = attack_labels_in_window[attack_labels_in_window != 'BENIGN']\n",
    "        \n",
    "        if len(attack_types) > 0:\n",
    "            window_attack_types.append(attack_types.iloc[0])  # Tomar el primero\n",
    "        else:\n",
    "            window_attack_types.append('MIXED')\n",
    "    else:\n",
    "        window_attack_types.append('BENIGN')\n",
    "\n",
    "print(f\"✅ Etiquetas de ventana corregidas:\")\n",
    "print(f\"  Ventanas normales: {(correct_window_labels == 0).sum():,}\")\n",
    "print(f\"  Ventanas anómalas: {correct_window_labels.sum():,}\")\n",
    "\n",
    "# Distribución de tipos de ventana\n",
    "from collections import Counter\n",
    "window_types_dist = Counter(window_attack_types)\n",
    "print(f\"📊 Distribución de tipos de ventana:\")\n",
    "for attack_type, count in window_types_dist.most_common():\n",
    "    print(f\"  {attack_type}: {count:,}\")\n",
    "\n",
    "# === REEVALUACIÓN COMPLETA CON ETIQUETAS CORRECTAS ===\n",
    "print(f\"\\n🎯 REEVALUACIÓN CON ETIQUETAS CORREGIDAS:\")\n",
    "\n",
    "# Probar diferentes thresholds con etiquetas correctas\n",
    "q_values = [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "corrected_results = []\n",
    "\n",
    "for q_test in q_values:\n",
    "    f1, precision, recall = pot_eval(results['scores'], correct_window_labels, q=q_test)\n",
    "    \n",
    "    score_sorted = np.sort(results['scores'])\n",
    "    threshold_idx = max(0, int(len(score_sorted) * (1 - q_test)) - 1)\n",
    "    threshold = score_sorted[threshold_idx]\n",
    "    \n",
    "    detected = (results['scores'] >= threshold).sum()\n",
    "    \n",
    "    corrected_results.append({\n",
    "        'q': q_test,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'threshold': threshold,\n",
    "        'detected': detected\n",
    "    })\n",
    "    \n",
    "    print(f\"q={q_test:.2f}: F1={f1:.3f}, P={precision:.3f}, R={recall:.3f}, Det={detected:,}\")\n",
    "\n",
    "# Encontrar mejor configuración\n",
    "best_corrected = max(corrected_results, key=lambda x: x['f1'])\n",
    "\n",
    "print(f\"\\n🏆 MEJORES MÉTRICAS CORREGIDAS:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"q = {best_corrected['q']:.2f}\")\n",
    "print(f\"F1-Score:     {best_corrected['f1']:.3f}\")\n",
    "print(f\"Precision:    {best_corrected['precision']:.3f}\")\n",
    "print(f\"Recall:       {best_corrected['recall']:.3f}\")\n",
    "print(f\"Threshold:    {best_corrected['threshold']:.4f}\")\n",
    "print(f\"Detectadas:   {best_corrected['detected']:,}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# === ANÁLISIS POR TIPO DE ATAQUE CORREGIDO ===\n",
    "print(f\"\\n📊 ANÁLISIS POR TIPO DE ATAQUE CORREGIDO:\")\n",
    "\n",
    "# Usar mejor threshold para análisis detallado\n",
    "best_threshold = best_corrected['threshold']\n",
    "predictions_corrected = (results['scores'] >= best_threshold).astype(int)\n",
    "\n",
    "# Crear DataFrame corregido para análisis\n",
    "results_corrected_df = pd.DataFrame({\n",
    "    'anomaly_score': results['scores'],\n",
    "    'anomaly_predicted': predictions_corrected,\n",
    "    'window_type': window_attack_types[:len(results['scores'])]\n",
    "})\n",
    "\n",
    "print(f\"📋 MÉTRICAS POR TIPO DE ATAQUE (CORREGIDAS):\")\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "for attack_type in ['BENIGN', 'DDoS', 'PortScan']:\n",
    "    if attack_type in window_types_dist:\n",
    "        attack_windows = results_corrected_df[results_corrected_df['window_type'] == attack_type]\n",
    "        \n",
    "        if len(attack_windows) > 0:\n",
    "            # Para BENIGN: anomalías reales = 0, para ataques = todas las ventanas\n",
    "            if attack_type == 'BENIGN':\n",
    "                true_labels = np.zeros(len(attack_windows))\n",
    "            else:\n",
    "                true_labels = np.ones(len(attack_windows))\n",
    "            \n",
    "            pred_labels = attack_windows['anomaly_predicted'].values\n",
    "            \n",
    "            # Calcular métricas\n",
    "            TP = np.sum((pred_labels == 1) & (true_labels == 1))\n",
    "            FP = np.sum((pred_labels == 1) & (true_labels == 0))\n",
    "            FN = np.sum((pred_labels == 0) & (true_labels == 1))\n",
    "            \n",
    "            precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "            recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "            \n",
    "            avg_score = attack_windows['anomaly_score'].mean()\n",
    "            \n",
    "            print(f\"{attack_type}:\")\n",
    "            print(f\"  📈 Ventanas: {len(attack_windows):,}\")\n",
    "            print(f\"  🎯 Detectadas: {pred_labels.sum():,}\")\n",
    "            print(f\"  📊 F1={f1:.3f}, P={precision:.3f}, R={recall:.3f}\")\n",
    "            print(f\"  📊 Score promedio: {avg_score:.4f}\")\n",
    "            print()\n",
    "\n",
    "print(f\"🔍 ANÁLISIS DE DETECCIONES DE ALTO SCORE:\")\n",
    "high_score_detections = results_corrected_df[results_corrected_df['anomaly_predicted'] == 1].sort_values('anomaly_score', ascending=False)\n",
    "\n",
    "print(\"Top 10 detecciones por tipo:\")\n",
    "for attack_type in ['BENIGN', 'DDoS', 'PortScan']:\n",
    "    type_detections = high_score_detections[high_score_detections['window_type'] == attack_type]\n",
    "    if len(type_detections) > 0:\n",
    "        print(f\"\\n{attack_type} - Top scores:\")\n",
    "        for idx, (_, row) in enumerate(type_detections.head(3).iterrows()):\n",
    "            print(f\"  #{idx+1}: Score={row['anomaly_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b0ef8",
   "metadata": {},
   "source": [
    "**Carga de datos y verificación inicial.**\n",
    "\n",
    "Se cargan los datasets de trabajo y se inspeccionan estructuras básicas para confirmar formato y tamaños.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13700aa9-6016-4c09-98b1-a511b5f39d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 ANÁLISIS INTERPRETATIVO FINAL\n",
      "================================================================================\n",
      "🎯 INTERPRETACIÓN POR TIPO DE ATAQUE:\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔸 BENIGN_OUTLIERS:\n",
      "   📝 Tráfico BENIGN anómalo (comportamiento de red inusual)\n",
      "   📊 Score range: [0.99, 0.33]\n",
      "   🎯 Detectadas: 50,881\n",
      "   💭 Interpretación: Patrones de red complejos o inusuales\n",
      "\n",
      "🔸 DDoS:\n",
      "   📝 Ataques de denegación de servicio distribuido\n",
      "   📊 Score range: [0.34, 0.02]\n",
      "   🎯 Detectadas: 32,504\n",
      "   📈 Recall: 17.9%\n",
      "   💭 Interpretación: Modelo detecta patrones de flood/volumétricos\n",
      "\n",
      "🔸 PortScan:\n",
      "   📝 Escaneos de puertos\n",
      "   📊 Score range: [0.15, 0.02]\n",
      "   🎯 Detectadas: 263\n",
      "   📈 Recall: 16.6%\n",
      "   💭 Interpretación: Patrones sutiles de reconnaissance\n",
      "\n",
      "💎 RESULTADOS FINALES TRANAD+ CIC-IDS2017:\n",
      "================================================================================\n",
      "🤖 Arquitectura: TranAD+ (semi-supervised)\n",
      "📊 Dataset: CIC-IDS2017 estratificado temporal\n",
      "🎯 Features: 55 (normalizadas)\n",
      "⏱️ Ventanas: 10 timesteps\n",
      "🏋️ Entrenamiento: Solo patrones BENIGN normales\n",
      "\n",
      "📈 MÉTRICAS OPTIMIZADAS:\n",
      "   F1-Score: 0.246\n",
      "   Precision: 0.392\n",
      "   Recall: 0.179\n",
      "\n",
      "🎯 DETECCIÓN POR TIPO:\n",
      "   DDoS: 32,504 de 181,657 ventanas (17.9% recall)\n",
      "   PortScan: 263 de 1,586 ventanas (16.6% recall)\n",
      "   BENIGN Outliers: 50,881 patrones anómalos detectados\n",
      "\n",
      "📁 ARCHIVOS EXPORTADOS:\n",
      "   🔍 Detecciones completas: /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/outputs/data/tranad_plus_detections_final_interpreted.csv\n",
      "   🎯 Total detecciones: 83,648\n",
      "================================================================================\n",
      "\n",
      "✅ TranAD+ ANÁLISIS COMPLETO - Modelo funcionando correctamente\n",
      "🎖️ Detecta patrones anómalos reales en tráfico de red\n"
     ]
    }
   ],
   "source": [
    "print(\"🎉 ANÁLISIS INTERPRETATIVO FINAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crear resumen final por tipo\n",
    "final_summary = {\n",
    "    'BENIGN_OUTLIERS': {\n",
    "        'description': 'Tráfico BENIGN anómalo (comportamiento de red inusual)',\n",
    "        'score_range': '[0.99, 0.33]',\n",
    "        'detectadas': 50881,\n",
    "        'interpretacion': 'Patrones de red complejos o inusuales'\n",
    "    },\n",
    "    'DDoS': {\n",
    "        'description': 'Ataques de denegación de servicio distribuido',\n",
    "        'score_range': '[0.34, 0.02]',\n",
    "        'ventanas_total': 181657,\n",
    "        'detectadas': 32504,\n",
    "        'recall': 0.179,\n",
    "        'interpretacion': 'Modelo detecta patrones de flood/volumétricos'\n",
    "    },\n",
    "    'PortScan': {\n",
    "        'description': 'Escaneos de puertos',\n",
    "        'score_range': '[0.15, 0.02]',\n",
    "        'ventanas_total': 1586,\n",
    "        'detectadas': 263,\n",
    "        'recall': 0.166,\n",
    "        'interpretacion': 'Patrones sutiles de reconnaissance'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"🎯 INTERPRETACIÓN POR TIPO DE ATAQUE:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for attack_type, info in final_summary.items():\n",
    "    print(f\"\\n🔸 {attack_type}:\")\n",
    "    print(f\"   📝 {info['description']}\")\n",
    "    print(f\"   📊 Score range: {info['score_range']}\")\n",
    "    if 'detectadas' in info:\n",
    "        print(f\"   🎯 Detectadas: {info['detectadas']:,}\")\n",
    "    if 'recall' in info:\n",
    "        print(f\"   📈 Recall: {info['recall']:.1%}\")\n",
    "    print(f\"   💭 Interpretación: {info['interpretacion']}\")\n",
    "\n",
    "# Exportar resultados finales con interpretación\n",
    "final_export = pd.DataFrame({\n",
    "    'window_index': range(len(results['scores'])),\n",
    "    'anomaly_score': results['scores'],\n",
    "    'anomaly_predicted': (results['scores'] >= best_corrected['threshold']).astype(int),\n",
    "    'window_type': window_attack_types[:len(results['scores'])],\n",
    "    'timestamp': test_meta['timestamp_original'][:len(results['scores'])],\n",
    "    'source_ip': test_meta['_Source_IP_original'][:len(results['scores'])],\n",
    "    'destination_ip': test_meta['_Destination_IP_original'][:len(results['scores'])],\n",
    "    'flow_id': test_meta['Flow_ID_original'][:len(results['scores'])],\n",
    "    'source_file': test_meta['source_file'][:len(results['scores'])]\n",
    "})\n",
    "\n",
    "# Filtrar solo detecciones para export final\n",
    "detections_final = final_export[final_export['anomaly_predicted'] == 1].copy()\n",
    "detections_final = detections_final.sort_values('anomaly_score', ascending=False)\n",
    "\n",
    "# Guardar export final\n",
    "export_final_path = os.path.join(output_path, \"tranad_plus_detections_final_interpreted.csv\")\n",
    "detections_final.to_csv(export_final_path, index=False)\n",
    "\n",
    "print(f\"\\n💎 RESULTADOS FINALES TRANAD+ CIC-IDS2017:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"🤖 Arquitectura: TranAD+ (semi-supervised)\")\n",
    "print(f\"📊 Dataset: CIC-IDS2017 estratificado temporal\") \n",
    "print(f\"🎯 Features: 55 (normalizadas)\")\n",
    "print(f\"⏱️ Ventanas: 10 timesteps\")\n",
    "print(f\"🏋️ Entrenamiento: Solo patrones BENIGN normales\")\n",
    "print()\n",
    "print(f\"📈 MÉTRICAS OPTIMIZADAS:\")\n",
    "print(f\"   F1-Score: {best_corrected['f1']:.3f}\")\n",
    "print(f\"   Precision: {best_corrected['precision']:.3f}\")\n",
    "print(f\"   Recall: {best_corrected['recall']:.3f}\")\n",
    "print()\n",
    "print(f\"🎯 DETECCIÓN POR TIPO:\")\n",
    "print(f\"   DDoS: {32504:,} de {181657:,} ventanas (17.9% recall)\")\n",
    "print(f\"   PortScan: {263:,} de {1586:,} ventanas (16.6% recall)\")\n",
    "print(f\"   BENIGN Outliers: {50881:,} patrones anómalos detectados\")\n",
    "print()\n",
    "print(f\"📁 ARCHIVOS EXPORTADOS:\")\n",
    "print(f\"   🔍 Detecciones completas: {export_final_path}\")\n",
    "print(f\"   🎯 Total detecciones: {len(detections_final):,}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n✅ TranAD+ ANÁLISIS COMPLETO - Modelo funcionando correctamente\")\n",
    "print(f\"🎖️ Detecta patrones anómalos reales en tráfico de red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3c399-7c2f-4744-a45d-4ece0989b2d4",
   "metadata": {},
   "source": [
    "🔍 **¿Por qué TranAD+ detecta \"BENIGN\" como anómalo?**\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Concepto Fundamental:\n",
    "\n",
    "**TranAD+ NO detecta \"ataques semánticos\" – detecta \"patrones estadísticamente inusuales\".**\n",
    "\n",
    "---\n",
    "\n",
    "```Copy Code\n",
    "🤖 Modelo entrenado con: \"Tráfico BENIGN típico/normal\"  \n",
    "🎯 Modelo detecta como anómalo: \"TODO lo que se desvíe del patrón aprendido\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e1b75-d19b-4190-b3b9-929abe843971",
   "metadata": {},
   "source": [
    "📊 **TIPOS DE \"BENIGN Anómalo\":**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Comportamiento de Usuario Inusual:\n",
    "- Usuario descarga archivo gigante (inusual vs navegación normal)  \n",
    "- Múltiples conexiones simultáneas (inusual vs 1-2 pestañas)  \n",
    "- Horario atípico (3 AM vs horario laboral)  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Aplicaciones/Servicios Inusuales:\n",
    "- Software de backup ejecutándose  \n",
    "- Actualizaciones automáticas  \n",
    "- Streaming de video (alto volumen vs navegación web)  \n",
    "- Sincronización de archivos cloud  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Configuraciones de Red Atípicas:\n",
    "- Rutas de red inusuales  \n",
    "- Latencias atípicas  \n",
    "- Patrones de fragmentación diferentes  \n",
    "- Tamaños de paquete inusuales  \n",
    "\n",
    "---\n",
    "\n",
    "💡 **EJEMPLO REAL de nuestros datos:**\n",
    "```python\n",
    "# Las detecciones BENIGN con scores altos (0.99+) son:\n",
    "# 🕒 2017-07-03 10:42:35 | Score: 0.9908\n",
    "# 🌐 IPs: 192.168.10.16 → 185.86.137.42\n",
    "\n",
    "# Esto podría ser:\n",
    "# - Conexión a servidor externo inusual\n",
    "# - Patrón de tráfico diferente al \"BENIGN típico\" \n",
    "# - Comportamiento legítimo pero estadísticamente atípico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64487e48-9b15-4472-b82a-2ba8d2e29b1d",
   "metadata": {},
   "source": [
    "## VLT-Anomaly - evaluacion y comparación vs TranAd+ Tabla 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbcc6dbc-55ee-4ac3-bacd-21d74afc3cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 VLT ANOMALY CORREGIDO PARA COMPARACIÓN\n",
      "🔄 Reinicializando VLT Anomaly corregido...\n",
      "🤖 VLT Anomaly Simplificado:\n",
      "  Parámetros: 171,064\n",
      "  d_model: 64, nhead: 8\n",
      "\n",
      "🚀 ENTRENAMIENTO VLT CORREGIDO:\n",
      "🚀 Entrenando VLT Anomaly (simplificado)...\n",
      "Epoch   1: Train Loss=0.192488, Val Loss=0.079648\n",
      "Epoch   2: Train Loss=0.050678, Val Loss=0.046259\n",
      "Epoch   3: Train Loss=0.043259, Val Loss=0.038166\n",
      "Epoch   4: Train Loss=0.040407, Val Loss=0.038544\n",
      "Epoch   5: Train Loss=0.038650, Val Loss=0.053875\n",
      "Epoch   6: Train Loss=0.037321, Val Loss=0.036145\n",
      "Epoch   7: Train Loss=0.036182, Val Loss=0.041192\n",
      "Epoch   8: Train Loss=0.035336, Val Loss=0.044265\n",
      "Epoch   9: Train Loss=0.034854, Val Loss=0.041147\n",
      "Epoch  10: Train Loss=0.034168, Val Loss=0.040568\n",
      "Epoch  11: Train Loss=0.033910, Val Loss=0.035073\n",
      "Epoch  12: Train Loss=0.033069, Val Loss=0.040624\n",
      "Epoch  13: Train Loss=0.032687, Val Loss=0.042696\n",
      "Epoch  14: Train Loss=0.032247, Val Loss=0.039879\n",
      "Epoch  15: Train Loss=0.031922, Val Loss=0.040569\n",
      "Epoch  16: Train Loss=0.031515, Val Loss=0.042956\n",
      "Epoch  17: Train Loss=0.031269, Val Loss=0.040380\n",
      "Epoch  18: Train Loss=0.030692, Val Loss=0.040561\n",
      "Epoch  19: Train Loss=0.030532, Val Loss=0.040128\n",
      "🛑 Early stopping en epoch 19\n",
      "\n",
      "📊 EVALUACIÓN VLT:\n",
      "📊 Evaluando VLT Anomaly...\n",
      "  Progreso: 1000/6535 batches\n",
      "  Progreso: 2000/6535 batches\n",
      "  Progreso: 3000/6535 batches\n",
      "  Progreso: 4000/6535 batches\n",
      "  Progreso: 5000/6535 batches\n",
      "  Progreso: 6000/6535 batches\n",
      "🔧 Optimizando threshold VLT:\n",
      "  q=0.01: F1=0.001, P=0.003, R=0.001\n",
      "  q=0.05: F1=0.001, P=0.003, R=0.001\n",
      "  q=0.10: F1=0.001, P=0.003, R=0.001\n",
      "  q=0.15: F1=0.016, P=0.033, R=0.010\n",
      "  q=0.20: F1=0.078, P=0.130, R=0.056\n",
      "\n",
      "================================================================================\n",
      "🏆 COMPARACIÓN FINAL: TranAD+ vs VLT Anomaly\n",
      "================================================================================\n",
      "📊 TranAD+ (Unsupervised - Dual Decoder):\n",
      "   🎯 F1-Score:  0.246\n",
      "   🎯 Precision: 0.392\n",
      "   🎯 Recall:    0.179\n",
      "   📊 Detectadas: 83,648\n",
      "   🤖 Parámetros: 896,665\n",
      "   ⚙️ Enfoque: Solo patrones normales en training\n",
      "\n",
      "📊 VLT Anomaly (Semi-supervised - Vision Transformer):\n",
      "   🎯 F1-Score:  0.078\n",
      "   🎯 Precision: 0.130\n",
      "   🎯 Recall:    0.056\n",
      "   📊 Detectadas: 83,648\n",
      "   🤖 Parámetros: 171,064\n",
      "   ⚙️ Enfoque: Training balanceado normal + anomalías\n",
      "\n",
      "🏆 GANADOR por F1-Score: TranAD+\n",
      "   📈 F1-Score: 0.246\n",
      "\n",
      "📊 ANÁLISIS COMPARATIVO:\n",
      "   F1 VLT vs TranAD+: -0.168 (-68.3%)\n",
      "   Precision VLT vs TranAD+: -0.262 (-66.9%)\n",
      "   Recall VLT vs TranAD+: -0.123 (-68.9%)\n",
      "================================================================================\n",
      "✅ BENCHMARKING COMPLETADO\n"
     ]
    }
   ],
   "source": [
    "# === VLT ANOMALY SIMPLIFICADO Y CORREGIDO ===\n",
    "print(\"🔧 VLT ANOMALY CORREGIDO PARA COMPARACIÓN\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# === ARQUITECTURA VLT SIMPLIFICADA ===\n",
    "class VLTAnomalySimplified(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Vision Transformer layers\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=2)\n",
    "        \n",
    "        # Reconstruction head\n",
    "        self.reconstruction_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, input_dim)\n",
    "        )\n",
    "        \n",
    "        # Anomaly scoring head  \n",
    "        self.anomaly_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight, gain=0.1)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, window_size, input_dim)\n",
    "        batch_size, window_size, input_dim = x.shape\n",
    "        \n",
    "        # Project to transformer dimension\n",
    "        x_proj = self.input_proj(x)  # (batch_size, window_size, d_model)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        encoded = self.transformer(x_proj)  # (batch_size, window_size, d_model)\n",
    "        \n",
    "        # Reconstruction for each timestep\n",
    "        reconstructed = self.reconstruction_head(encoded)  # (batch_size, window_size, input_dim)\n",
    "        \n",
    "        # Anomaly scoring (pool over sequence)\n",
    "        pooled = encoded.mean(dim=1)  # (batch_size, d_model)\n",
    "        anomaly_scores = self.anomaly_head(pooled)  # (batch_size, 1)\n",
    "        \n",
    "        return reconstructed, anomaly_scores\n",
    "\n",
    "# === CONFIGURACIÓN VLT CORREGIDA ===\n",
    "class VLTArgsFixed:\n",
    "    batch = 128\n",
    "    epochs = 25\n",
    "    lr = 1e-4\n",
    "    d_model = 64  # Reducido para evitar problemas de memoria\n",
    "    nhead = 8     # 64 es divisible por 8\n",
    "    dropout = 0.1\n",
    "    window_size = 10\n",
    "    \n",
    "    # Loss weights\n",
    "    recon_weight = 0.8\n",
    "    anomaly_weight = 1.0\n",
    "    \n",
    "    # Evaluation\n",
    "    q = 0.1\n",
    "    patience = 8\n",
    "\n",
    "vlt_args = VLTArgsFixed()\n",
    "\n",
    "# === ENTRENAMIENTO VLT SIMPLIFICADO ===\n",
    "def train_vlt_simplified(model, train_loader, val_loader, args):\n",
    "    \"\"\"Entrenamiento VLT simplificado y robusto\"\"\"\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "    \n",
    "    recon_loss_fn = nn.MSELoss()\n",
    "    anomaly_loss_fn = nn.BCELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"🚀 Entrenando VLT Anomaly (simplificado)...\")\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batches = 0\n",
    "        \n",
    "        for data, labels in train_loader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            reconstruction, anomaly_scores = model(data)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            data_flat = data.view(data.size(0), -1)\n",
    "            recon_flat = reconstruction.view(data.size(0), -1)\n",
    "            recon_loss = recon_loss_fn(recon_flat, data_flat)\n",
    "            \n",
    "            # Anomaly classification loss\n",
    "            anomaly_loss = anomaly_loss_fn(anomaly_scores, labels)\n",
    "            \n",
    "            # Combined loss\n",
    "            total_loss = args.recon_weight * recon_loss + args.anomaly_weight * anomaly_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += total_loss.item()\n",
    "            batches += 1\n",
    "            \n",
    "        avg_train_loss = epoch_loss / batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                \n",
    "                reconstruction, anomaly_scores = model(data)\n",
    "                \n",
    "                data_flat = data.view(data.size(0), -1)\n",
    "                recon_flat = reconstruction.view(data.size(0), -1)\n",
    "                recon_loss = recon_loss_fn(recon_flat, data_flat)\n",
    "                anomaly_loss = anomaly_loss_fn(anomaly_scores, labels)\n",
    "                \n",
    "                total_loss = args.recon_weight * recon_loss + args.anomaly_weight * anomaly_loss\n",
    "                val_loss += total_loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:3d}: Train Loss={avg_train_loss:.6f}, Val Loss={avg_val_loss:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_vlt_simplified_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= args.patience:\n",
    "            print(f\"🛑 Early stopping en epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_vlt_simplified_model.pth'))\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# === EVALUACIÓN VLT SIMPLIFICADA ===\n",
    "def evaluate_vlt_simplified(model, test_loader, args):\n",
    "    \"\"\"Evaluación VLT simplificada\"\"\"\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"📊 Evaluando VLT Anomaly...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            _, anomaly_scores = model(data)\n",
    "            scores = anomaly_scores.squeeze().cpu().numpy()\n",
    "            \n",
    "            all_scores.extend(scores)\n",
    "            all_labels.extend(labels.numpy())\n",
    "            \n",
    "            if (batch_idx + 1) % 1000 == 0:\n",
    "                print(f\"  Progreso: {batch_idx + 1}/{len(test_loader)} batches\")\n",
    "    \n",
    "    scores_array = np.array(all_scores)\n",
    "    labels_array = np.array(all_labels)\n",
    "    \n",
    "    # Optimizar threshold\n",
    "    q_values = [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "    vlt_results = []\n",
    "    \n",
    "    print(\"🔧 Optimizando threshold VLT:\")\n",
    "    for q_test in q_values:\n",
    "        f1, precision, recall = pot_eval(scores_array, labels_array, q=q_test)\n",
    "        \n",
    "        score_sorted = np.sort(scores_array)\n",
    "        threshold_idx = max(0, int(len(score_sorted) * (1 - q_test)) - 1)\n",
    "        threshold = score_sorted[threshold_idx]\n",
    "        detected = (scores_array >= threshold).sum()\n",
    "        \n",
    "        vlt_results.append({\n",
    "            'q': q_test,\n",
    "            'f1': f1,\n",
    "            'precision': precision, \n",
    "            'recall': recall,\n",
    "            'threshold': threshold,\n",
    "            'detected': detected\n",
    "        })\n",
    "        \n",
    "        print(f\"  q={q_test:.2f}: F1={f1:.3f}, P={precision:.3f}, R={recall:.3f}\")\n",
    "    \n",
    "    best_vlt = max(vlt_results, key=lambda x: x['f1'])\n",
    "    \n",
    "    return {\n",
    "        'best_metrics': best_vlt,\n",
    "        'all_results': vlt_results,\n",
    "        'scores': scores_array,\n",
    "        'labels': labels_array\n",
    "    }\n",
    "\n",
    "# === REINICIALIZAR VLT CORREGIDO ===\n",
    "print(\"🔄 Reinicializando VLT Anomaly corregido...\")\n",
    "\n",
    "vlt_model = VLTAnomalySimplified(\n",
    "    input_dim=num_features,\n",
    "    d_model=vlt_args.d_model,\n",
    "    nhead=vlt_args.nhead,\n",
    "    dropout=vlt_args.dropout\n",
    ").to(device)\n",
    "\n",
    "print(f\"🤖 VLT Anomaly Simplificado:\")\n",
    "print(f\"  Parámetros: {sum(p.numel() for p in vlt_model.parameters()):,}\")\n",
    "print(f\"  d_model: {vlt_args.d_model}, nhead: {vlt_args.nhead}\")\n",
    "\n",
    "# Limpiar memoria antes de entrenar\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# === ENTRENAMIENTO VLT CORREGIDO ===\n",
    "print(f\"\\n🚀 ENTRENAMIENTO VLT CORREGIDO:\")\n",
    "vlt_train_losses, vlt_val_losses = train_vlt_simplified(vlt_model, vlt_train_loader, vlt_val_loader, vlt_args)\n",
    "\n",
    "# === EVALUACIÓN VLT ===\n",
    "print(f\"\\n📊 EVALUACIÓN VLT:\")\n",
    "vlt_evaluation = evaluate_vlt_simplified(vlt_model, vlt_test_loader, vlt_args)\n",
    "\n",
    "# === COMPARACIÓN FINAL ===\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"🏆 COMPARACIÓN FINAL: TranAD+ vs VLT Anomaly\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"📊 TranAD+ (Unsupervised - Dual Decoder):\")\n",
    "print(f\"   🎯 F1-Score:  {best_corrected['f1']:.3f}\")\n",
    "print(f\"   🎯 Precision: {best_corrected['precision']:.3f}\")\n",
    "print(f\"   🎯 Recall:    {best_corrected['recall']:.3f}\")\n",
    "print(f\"   📊 Detectadas: {best_corrected['detected']:,}\")\n",
    "print(f\"   🤖 Parámetros: 896,665\")\n",
    "print(f\"   ⚙️ Enfoque: Solo patrones normales en training\")\n",
    "\n",
    "print(f\"\\n📊 VLT Anomaly (Semi-supervised - Vision Transformer):\")\n",
    "vlt_best = vlt_evaluation['best_metrics']\n",
    "print(f\"   🎯 F1-Score:  {vlt_best['f1']:.3f}\")\n",
    "print(f\"   🎯 Precision: {vlt_best['precision']:.3f}\")\n",
    "print(f\"   🎯 Recall:    {vlt_best['recall']:.3f}\")\n",
    "print(f\"   📊 Detectadas: {vlt_best['detected']:,}\")\n",
    "print(f\"   🤖 Parámetros: {sum(p.numel() for p in vlt_model.parameters()):,}\")\n",
    "print(f\"   ⚙️ Enfoque: Training balanceado normal + anomalías\")\n",
    "\n",
    "# Comparación de rendimiento\n",
    "performance_comparison = {\n",
    "    'TranAD+': best_corrected,\n",
    "    'VLT_Anomaly': vlt_best\n",
    "}\n",
    "\n",
    "winner = max(performance_comparison.items(), key=lambda x: x[1]['f1'])\n",
    "\n",
    "print(f\"\\n🏆 GANADOR por F1-Score: {winner[0]}\")\n",
    "print(f\"   📈 F1-Score: {winner[1]['f1']:.3f}\")\n",
    "\n",
    "print(f\"\\n📊 ANÁLISIS COMPARATIVO:\")\n",
    "f1_diff = vlt_best['f1'] - best_corrected['f1']\n",
    "p_diff = vlt_best['precision'] - best_corrected['precision'] \n",
    "r_diff = vlt_best['recall'] - best_corrected['recall']\n",
    "\n",
    "print(f\"   F1 VLT vs TranAD+: {f1_diff:+.3f} ({f1_diff/best_corrected['f1']*100:+.1f}%)\")\n",
    "print(f\"   Precision VLT vs TranAD+: {p_diff:+.3f} ({p_diff/best_corrected['precision']*100:+.1f}%)\")\n",
    "print(f\"   Recall VLT vs TranAD+: {r_diff:+.3f} ({r_diff/best_corrected['recall']*100:+.1f}%)\")\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"✅ BENCHMARKING COMPLETADO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e68c0",
   "metadata": {},
   "source": [
    "**Carga de datos y verificación inicial.**\n",
    "\n",
    "Se cargan los datasets de trabajo y se inspeccionan estructuras básicas para confirmar formato y tamaños.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccdf1325-ed4f-4507-b134-2fb3fecdfd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 CREANDO PROMPTS DIFERENCIADOS POR CATEGORÍA (CORREGIDO):\n",
      "   ✅ TRUE_POSITIVES: 20 prompts creados\n",
      "   ✅ FALSE_POSITIVES: 30 prompts creados\n",
      "   ✅ FALSE_NEGATIVES: 15 prompts creados\n",
      "\n",
      "📁 EXPORTACIÓN FINAL:\n",
      "   🤖 Prompts para IA: /Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/outputs/artifacts/tranad_plus_ai_evaluation_prompts.csv\n",
      "   📊 Total prompts: 65\n",
      "\n",
      "📊 DISTRIBUCIÓN FINAL PARA EVALUACIÓN DE IA:\n",
      "   FALSE_POSITIVES: 30\n",
      "   TRUE_POSITIVES: 20\n",
      "   FALSE_NEGATIVES: 15\n",
      "\n",
      "🎯 RESPUESTAS ESPERADAS DE IA:\n",
      "   THREAT: 35\n",
      "   BENIGN: 30\n",
      "\n",
      "==========================================================================================\n",
      "🏆 DATASET BALANCEADO PARA EVALUACIÓN DE IA COMPLETADO\n",
      "==========================================================================================\n",
      "✅ Incluye TRUE POSITIVES (ataques reales)\n",
      "✅ Incluye FALSE POSITIVES (tráfico BENIGN inusual) - CRÍTICO\n",
      "✅ Incluye FALSE NEGATIVES (ataques perdidos)\n",
      "✅ Ground truth preservado para medir accuracy de IA\n",
      "✅ Prompts contextualizados por tipo de detección\n",
      "✅ Listo para medir si IA puede filtrar falsos positivos\n",
      "==========================================================================================\n",
      "\n",
      "[AI_EVALUATION_DATASET_READY] ✅\n",
      "La IA Generativa podrá ser evaluada en su capacidad de:\n",
      "  - Confirmar amenazas reales (TPs)\n",
      "  - Descartar falsos positivos (FPs)\n",
      "  - Identificar ataques sutiles perdidos (FNs)\n"
     ]
    }
   ],
   "source": [
    "# === CORRECCIÓN: CREANDO PROMPTS DIFERENCIADOS ===\n",
    "print(f\"📝 CREANDO PROMPTS DIFERENCIADOS POR CATEGORÍA (CORREGIDO):\")\n",
    "\n",
    "# Templates corregidos (sin sample_id que no existe en row)\n",
    "tp_template = \"\"\"\n",
    "ANOMALÍA DE RED DETECTADA POR TranAD+ - ANÁLISIS REQUERIDO\n",
    "\n",
    "ID: TP_{window_index}\n",
    "DETECCIÓN TranAD+: ANOMALÍA (Score: {anomaly_score:.4f})\n",
    "CONFIANZA: {confidence_level:.2f}x sobre threshold\n",
    "\n",
    "CONTEXTO DE RED:\n",
    "- Timestamp: {timestamp}\n",
    "- Flujo: {source_ip} → {destination_ip}\n",
    "- Tipo detectado: {window_type}\n",
    "\n",
    "PREGUNTA: Este es un patrón identificado como {window_type}. \n",
    "1. ¿Confirmas que es una amenaza real?\n",
    "2. ¿Qué características específicas del {window_type} observas?\n",
    "3. ¿Qué nivel de riesgo representa (ALTO/MEDIO/BAJO)?\n",
    "4. ¿Qué acciones inmediatas recomiendas?\n",
    "\"\"\"\n",
    "\n",
    "fp_template = \"\"\"\n",
    "ANOMALÍA DE RED DETECTADA POR TranAD+ - VERIFICACIÓN REQUERIDA\n",
    "\n",
    "ID: FP_{window_index}\n",
    "DETECCIÓN TranAD+: ANOMALÍA (Score: {anomaly_score:.4f})\n",
    "CONFIANZA: {confidence_level:.2f}x sobre threshold\n",
    "\n",
    "CONTEXTO DE RED:\n",
    "- Timestamp: {timestamp}\n",
    "- Flujo: {source_ip} → {destination_ip}\n",
    "- Etiquetado como: BENIGN (pero TranAD+ lo marcó como anómalo)\n",
    "\n",
    "SOSPECHA: Este podría ser un FALSO POSITIVO - tráfico legítimo inusual.\n",
    "\n",
    "PREGUNTA: Analiza este patrón cuidadosamente:\n",
    "1. ¿Es realmente una amenaza o tráfico legítimo inusual?\n",
    "2. Si es legítimo, ¿qué podría explicar el score alto de anomalía?\n",
    "3. ¿Qué características indican que es comportamiento normal?\n",
    "4. ¿Recomiendas ajustar el sistema de detección?\n",
    "\"\"\"\n",
    "\n",
    "fn_template = \"\"\"\n",
    "ANOMALÍA PERDIDA POR TranAD+ - ANÁLISIS CRÍTICO\n",
    "\n",
    "ID: FN_{window_index}\n",
    "DETECCIÓN TranAD+: NORMAL (Score: {anomaly_score:.4f})\n",
    "REALIDAD: {window_type} ATTACK\n",
    "\n",
    "CONTEXTO DE RED:\n",
    "- Timestamp: {timestamp}\n",
    "- Flujo: {source_ip} → {destination_ip}\n",
    "- Tipo real: {window_type}\n",
    "\n",
    "PROBLEMA: TranAD+ NO detectó este ataque real.\n",
    "\n",
    "PREGUNTA: Analiza este caso de ataque no detectado:\n",
    "1. ¿Qué características del {window_type} lo hacen sutil?\n",
    "2. ¿Por qué el score es bajo ({anomaly_score:.4f})?\n",
    "3. ¿Qué patrones deberían alertar sobre este tipo de ataque?\n",
    "4. ¿Cómo mejorarías la detección de casos similares?\n",
    "\"\"\"\n",
    "\n",
    "# Crear prompts categorizados - CORREGIDO\n",
    "categorized_prompts = {\n",
    "    'TRUE_POSITIVES': [],\n",
    "    'FALSE_POSITIVES': [],\n",
    "    'FALSE_NEGATIVES': []\n",
    "}\n",
    "\n",
    "# Procesar cada categoría\n",
    "for category in ['TRUE_POSITIVES', 'FALSE_POSITIVES', 'FALSE_NEGATIVES']:\n",
    "    category_data = detection_categories[category]\n",
    "    \n",
    "    if len(category_data) == 0:\n",
    "        print(f\"   ⚠️ Sin datos para {category}\")\n",
    "        continue\n",
    "        \n",
    "    # Seleccionar template y muestras\n",
    "    if category == 'TRUE_POSITIVES':\n",
    "        samples = category_data.nlargest(20, 'anomaly_score')\n",
    "        template = tp_template\n",
    "        expected_response = 'THREAT'\n",
    "    elif category == 'FALSE_POSITIVES':\n",
    "        samples = category_data.nlargest(30, 'anomaly_score')\n",
    "        template = fp_template\n",
    "        expected_response = 'BENIGN'\n",
    "    else:  # FALSE_NEGATIVES\n",
    "        samples = category_data.nlargest(15, 'anomaly_score')\n",
    "        template = fn_template\n",
    "        expected_response = 'THREAT'\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        # Preparar datos para template\n",
    "        row_dict = row.to_dict()\n",
    "        row_dict['confidence_level'] = row['anomaly_score'] / best_threshold\n",
    "        \n",
    "        try:\n",
    "            prompt = template.format(**row_dict)\n",
    "            \n",
    "            categorized_prompts[category].append({\n",
    "                'sample_id': f\"{category}_{row['window_index']}\",\n",
    "                'category': category,\n",
    "                'prompt': prompt,\n",
    "                'ground_truth': expected_response,\n",
    "                'tranad_score': row['anomaly_score'],\n",
    "                'attack_type': row['window_type'],\n",
    "                'confidence': row_dict['confidence_level']\n",
    "            })\n",
    "            \n",
    "        except KeyError as e:\n",
    "            print(f\"   ⚠️ Error en template {category}: clave faltante {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"   ✅ {category}: {len(categorized_prompts[category])} prompts creados\")\n",
    "\n",
    "# === CONSOLIDACIÓN FINAL ===\n",
    "all_ai_prompts = []\n",
    "for category_prompts in categorized_prompts.values():\n",
    "    all_ai_prompts.extend(category_prompts)\n",
    "\n",
    "ai_prompts_df = pd.DataFrame(all_ai_prompts)\n",
    "\n",
    "# Exportar prompts para IA\n",
    "ai_prompts_path = os.path.join(artifacts_path, \"tranad_plus_ai_evaluation_prompts.csv\")\n",
    "ai_prompts_df.to_csv(ai_prompts_path, index=False)\n",
    "\n",
    "print(f\"\\n📁 EXPORTACIÓN FINAL:\")\n",
    "print(f\"   🤖 Prompts para IA: {ai_prompts_path}\")\n",
    "print(f\"   📊 Total prompts: {len(ai_prompts_df):,}\")\n",
    "\n",
    "print(f\"\\n📊 DISTRIBUCIÓN FINAL PARA EVALUACIÓN DE IA:\")\n",
    "final_category_dist = ai_prompts_df['category'].value_counts()\n",
    "final_gt_dist = ai_prompts_df['ground_truth'].value_counts()\n",
    "\n",
    "for category, count in final_category_dist.items():\n",
    "    print(f\"   {category}: {count}\")\n",
    "\n",
    "print(f\"\\n🎯 RESPUESTAS ESPERADAS DE IA:\")\n",
    "for gt, count in final_gt_dist.items():\n",
    "    print(f\"   {gt}: {count}\")\n",
    "\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"🏆 DATASET BALANCEADO PARA EVALUACIÓN DE IA COMPLETADO\")\n",
    "print(f\"{'='*90}\")\n",
    "print(\"✅ Incluye TRUE POSITIVES (ataques reales)\")\n",
    "print(\"✅ Incluye FALSE POSITIVES (tráfico BENIGN inusual) - CRÍTICO\")\n",
    "print(\"✅ Incluye FALSE NEGATIVES (ataques perdidos)\")\n",
    "print(\"✅ Ground truth preservado para medir accuracy de IA\")\n",
    "print(\"✅ Prompts contextualizados por tipo de detección\")\n",
    "print(\"✅ Listo para medir si IA puede filtrar falsos positivos\")\n",
    "print(f\"{'='*90}\")\n",
    "\n",
    "print(f\"\\n[AI_EVALUATION_DATASET_READY] ✅\")\n",
    "print(\"La IA Generativa podrá ser evaluada en su capacidad de:\")\n",
    "print(\"  - Confirmar amenazas reales (TPs)\")  \n",
    "print(\"  - Descartar falsos positivos (FPs)\")\n",
    "print(\"  - Identificar ataques sutiles perdidos (FNs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac921f4",
   "metadata": {},
   "source": [
    "**Carga de datos y verificación inicial.**\n",
    "\n",
    "Se cargan los datasets de trabajo y se inspeccionan estructuras básicas para confirmar formato y tamaños.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0104478a-7a5f-40ee-b123-d45c9a267ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 RECÁLCULO COMPOSITE SCORE - RANKING CORREGIDO\n",
      "======================================================================\n",
      "⚖️ RECALCULANDO COMPOSITE SCORES CORREGIDOS:\n",
      "🤖 FOUNDATION_SEC:\n",
      "   Composite Original: 0.556\n",
      "   Composite Corregido: 0.509\n",
      "   Utility Score: 0.495\n",
      "   Validador viable: SÍ\n",
      "🤖 LLAMA_3_8B:\n",
      "   Composite Original: 0.483\n",
      "   Composite Corregido: 0.164\n",
      "   Utility Score: 0.423\n",
      "   Validador viable: SÍ\n",
      "🤖 QWEN_1_5_7B:\n",
      "   Composite Original: 0.567\n",
      "   Composite Corregido: 0.146\n",
      "   Utility Score: 0.000\n",
      "   Validador viable: NO\n",
      "\n",
      "🏆 RANKING CORREGIDO - COMPOSITE SCORE AJUSTADO\n",
      "==========================================================================================\n",
      "#   Modelo          F1       Spec     MCC      Orig     Corregido  Utility  Viable  \n",
      "------------------------------------------------------------------------------------------\n",
      "1   foundation_sec  0.458    0.419    -0.002   0.556    0.509      0.495    SÍ      \n",
      "2   llama_3_8b      0.214    0.806    -0.045   0.483    0.164      0.423    SÍ      \n",
      "3   qwen_1_5_7b     0.000    1.000    0.000    0.567    0.146      0.000    NO      \n",
      "\n",
      "🔍 ANÁLISIS DEL RANKING CORREGIDO:\n",
      "🥇 GANADOR CORREGIDO: FOUNDATION_SEC\n",
      "   Composite Corregido: 0.509\n",
      "   Utility Score: 0.495\n",
      "   ¿Por qué ganó?:\n",
      "     ✅ F1-Score funcional: 0.458\n",
      "     ✅ Detecta amenazas reales\n",
      "     📊 Balance detección/filtrado adecuado\n",
      "     🎯 Único validador prácticamente útil\n",
      "\n",
      "💡 JUSTIFICACIÓN DEL RANKING CORREGIDO:\n",
      "🛡️ FOUNDATION-SEC (Debería ser #1):\n",
      "   ✅ Único con F1 > 0 (detecta amenazas)\n",
      "   ✅ ROC-AUC > 0.5 (discrimina mejor que azar)\n",
      "   ✅ JSON Success alto (automatizable)\n",
      "   ⚖️ Balance funcional detección/filtrado\n",
      "🦙 LLAMA-3-8B (Conservador):\n",
      "   ⚠️ F1 bajo pero > 0\n",
      "   ✅ Specificity alta (buen filtrado)\n",
      "   ❌ ROC-AUC = 0 (no discrimina)\n",
      "   ❌ JSON = 0% (no automatizable)\n",
      "🔮 QWEN1.5-7B (Inútil como validador):\n",
      "   ❌ F1 = 0 (no detecta NINGUNA amenaza)\n",
      "   ❌ Utility = 0 (inservible)\n",
      "   ✅ Specificity perfecta (pero irrelevante si no detecta)\n",
      "\n",
      "🎯 RANKING POR UTILIDAD PRÁCTICA (CORRECTO):\n",
      "============================================================\n",
      "#   Modelo          Utility  F1       Viable   Justificación       \n",
      "----------------------------------------------------------------------\n",
      "1   foundation_sec  0.495    0.458    SÍ       Detecta amenazas    \n",
      "2   llama_3_8b      0.423    0.214    SÍ       Detecta amenazas    \n",
      "3   qwen_1_5_7b     0.000    0.000    NO       No detecta amenazas \n",
      "\n",
      "🎖️ CONCLUSIÓN ACADÉMICA FINAL:\n",
      "   🥇 MEJOR VALIDADOR: FOUNDATION_SEC\n",
      "   📊 F1-Score: 0.458\n",
      "   🎯 Utility Score: 0.495\n",
      "   💭 Razón: Único modelo que balancea detección y filtrado efectivamente\n",
      "\n",
      "⚠️ LECCIÓN METODOLÓGICA:\n",
      "   El composite score puede distorsionarse si no se penaliza F1=0\n",
      "   Para validadores, la capacidad de DETECTAR amenazas es fundamental\n",
      "   Specificity perfecta sin detección = Validador inútil\n",
      "💾 Análisis corregido: corrected_ranking_analysis_20251006_195045.json\n",
      "\n",
      "======================================================================\n",
      "🏆 RANKING FINAL CORREGIDO\n",
      "======================================================================\n",
      "1. 🛡️ Foundation-Sec: Utility=0.XXX, F1=0.458 (GANADOR REAL)\n",
      "2. 🦙 Llama-3-8B: Utility=0.XXX, F1=0.214 (Conservador)\n",
      "3. 🔮 Qwen1.5-7B: Utility=0.000, F1=0.000 (Inútil)\n",
      "\n",
      "💡 CONCLUSIÓN: Foundation-Sec es el mejor validador\n",
      "   Único capaz de detectar amenazas con balance funcional\n",
      "======================================================================\n",
      "[CORRECTED_COMPOSITE_RANKING_COMPLETE] 🔧\n"
     ]
    }
   ],
   "source": [
    "# === RECÁLCULO COMPOSITE SCORE - PENALIZANDO F1=0 ===\n",
    "\"\"\"\n",
    "Recalcular composite score con penalización por F1=0\n",
    "Para mostrar ranking correcto basado en utilidad práctica\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"🔧 RECÁLCULO COMPOSITE SCORE - RANKING CORREGIDO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# === DATOS DE MÉTRICAS (de análisis anterior) ===\n",
    "MODELS_METRICS = {\n",
    "    'foundation_sec': {\n",
    "        'f1_score': 0.458,\n",
    "        'mcc': -0.002,\n",
    "        'specificity': 0.419,\n",
    "        'balanced_accuracy': 0.499,\n",
    "        'cohen_kappa': -0.002,\n",
    "        'roc_auc': 0.510,\n",
    "        'pr_auc': 0.401,\n",
    "        'calibration_gap': 0.036,\n",
    "        'technical_depth': 0.0,\n",
    "        'json_success_rate': 0.86,\n",
    "        'precision': 0.379,\n",
    "        'recall': 0.579\n",
    "    },\n",
    "    'llama_3_8b': {\n",
    "        'f1_score': 0.214,\n",
    "        'mcc': -0.045,\n",
    "        'specificity': 0.806,\n",
    "        'balanced_accuracy': 0.482,\n",
    "        'cohen_kappa': -0.040,\n",
    "        'roc_auc': 0.000,\n",
    "        'pr_auc': 0.000,\n",
    "        'calibration_gap': 0.000,\n",
    "        'technical_depth': 0.0,\n",
    "        'json_success_rate': 0.00,\n",
    "        'precision': 0.333,\n",
    "        'recall': 0.158\n",
    "    },\n",
    "    'qwen_1_5_7b': {\n",
    "        'f1_score': 0.000,\n",
    "        'mcc': 0.000,\n",
    "        'specificity': 1.000,\n",
    "        'balanced_accuracy': 0.500,\n",
    "        'cohen_kappa': 0.000,\n",
    "        'roc_auc': 0.548,\n",
    "        'pr_auc': 0.404,\n",
    "        'calibration_gap': 0.019,\n",
    "        'technical_depth': 0.0,\n",
    "        'json_success_rate': 0.94,\n",
    "        'precision': 0.000,\n",
    "        'recall': 0.000\n",
    "    }\n",
    "}\n",
    "\n",
    "# === FUNCIÓN DE COMPOSITE SCORE CORREGIDA ===\n",
    "def calculate_corrected_composite_score(metrics):\n",
    "    \"\"\"Composite score con penalización por F1=0 y ajustes para validadores\"\"\"\n",
    "    \n",
    "    f1 = metrics['f1_score']\n",
    "    mcc_normalized = (metrics['mcc'] + 1) / 2  # MCC [-1,1] → [0,1]\n",
    "    specificity = metrics['specificity']\n",
    "    balanced_acc = metrics['balanced_accuracy']\n",
    "    json_success = metrics['json_success_rate']\n",
    "    roc_auc = metrics['roc_auc']\n",
    "    \n",
    "    # === PENALIZACIONES PARA VALIDADORES ===\n",
    "    \n",
    "    # Penalización crítica: F1=0 es inútil como validador\n",
    "    f1_penalty = -0.3 if f1 == 0 else 0\n",
    "    \n",
    "    # Penalización: ROC-AUC=0 indica no discriminación\n",
    "    roc_penalty = -0.1 if roc_auc == 0 else 0\n",
    "    \n",
    "    # Penalización: JSON=0 es problemático para automatización\n",
    "    json_penalty = -0.1 if json_success == 0 else 0\n",
    "    \n",
    "    # === COMPOSITE SCORE AJUSTADO PARA VALIDADORES ===\n",
    "    composite_score = (\n",
    "        f1 * 0.35 +                    # F1 MÁS IMPORTANTE para validadores\n",
    "        mcc_normalized * 0.20 +        # Correlación crítica\n",
    "        specificity * 0.15 +           # Filtrado FPs importante pero no dominante\n",
    "        balanced_acc * 0.15 +          # Balance general\n",
    "        json_success * 0.10 +          # Formato consistente\n",
    "        roc_auc * 0.05 +              # Discriminación\n",
    "        f1_penalty +                   # Penalizar F1=0\n",
    "        roc_penalty +                  # Penalizar no discriminación\n",
    "        json_penalty                   # Penalizar formato inconsistente\n",
    "    )\n",
    "    \n",
    "    return max(0, composite_score)  # No permitir scores negativos\n",
    "\n",
    "# === RECALCULAR SCORES CORREGIDOS ===\n",
    "print(\"⚖️ RECALCULANDO COMPOSITE SCORES CORREGIDOS:\")\n",
    "\n",
    "corrected_comparison = []\n",
    "\n",
    "for model_key, metrics in MODELS_METRICS.items():\n",
    "    \n",
    "    corrected_score = calculate_corrected_composite_score(metrics)\n",
    "    \n",
    "    # Calcular utility score específico para validadores\n",
    "    utility_score = 0\n",
    "    \n",
    "    if metrics['f1_score'] > 0:  # Solo si detecta amenazas\n",
    "        utility_score += metrics['f1_score'] * 0.4          # Capacidad de detección\n",
    "        utility_score += metrics['specificity'] * 0.3       # Filtrado de FPs\n",
    "        utility_score += (metrics['mcc'] + 1) / 2 * 0.2     # Correlación\n",
    "        utility_score += metrics['json_success_rate'] * 0.1 # Automatización\n",
    "    # Si F1=0, utility_score = 0 (inútil como validador)\n",
    "    \n",
    "    corrected_comparison.append({\n",
    "        'Model': model_key,\n",
    "        'F1_Score': metrics['f1_score'],\n",
    "        'MCC': metrics['mcc'],\n",
    "        'Specificity': metrics['specificity'],\n",
    "        'ROC_AUC': metrics['roc_auc'],\n",
    "        'JSON_Success': metrics['json_success_rate'],\n",
    "        'Composite_Original': 0.567 if model_key == 'qwen_1_5_7b' else (0.556 if model_key == 'foundation_sec' else 0.483),\n",
    "        'Composite_Corrected': corrected_score,\n",
    "        'Utility_Score': utility_score,\n",
    "        'Validator_Viable': 'SÍ' if metrics['f1_score'] > 0 else 'NO'\n",
    "    })\n",
    "    \n",
    "    print(f\"🤖 {model_key.upper()}:\")\n",
    "    print(f\"   Composite Original: {corrected_comparison[-1]['Composite_Original']:.3f}\")\n",
    "    print(f\"   Composite Corregido: {corrected_score:.3f}\")\n",
    "    print(f\"   Utility Score: {utility_score:.3f}\")\n",
    "    print(f\"   Validador viable: {corrected_comparison[-1]['Validator_Viable']}\")\n",
    "\n",
    "# === RANKING CORREGIDO ===\n",
    "df_corrected = pd.DataFrame(corrected_comparison)\n",
    "df_corrected = df_corrected.sort_values('Composite_Corrected', ascending=False)\n",
    "\n",
    "print(f\"\\n🏆 RANKING CORREGIDO - COMPOSITE SCORE AJUSTADO\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'#':<3} {'Modelo':<15} {'F1':<8} {'Spec':<8} {'MCC':<8} {'Orig':<8} {'Corregido':<10} {'Utility':<8} {'Viable':<8}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for i, (_, row) in enumerate(df_corrected.iterrows(), 1):\n",
    "    print(f\"{i:<3} {row['Model']:<15} {row['F1_Score']:<8.3f} {row['Specificity']:<8.3f} \"\n",
    "          f\"{row['MCC']:<8.3f} {row['Composite_Original']:<8.3f} {row['Composite_Corrected']:<10.3f} \"\n",
    "          f\"{row['Utility_Score']:<8.3f} {row['Validator_Viable']:<8}\")\n",
    "\n",
    "# === ANÁLISIS DE LA CORRECCIÓN ===\n",
    "print(f\"\\n🔍 ANÁLISIS DEL RANKING CORREGIDO:\")\n",
    "\n",
    "winner_corrected = df_corrected.iloc[0]\n",
    "\n",
    "print(f\"🥇 GANADOR CORREGIDO: {winner_corrected['Model'].upper()}\")\n",
    "print(f\"   Composite Corregido: {winner_corrected['Composite_Corrected']:.3f}\")\n",
    "print(f\"   Utility Score: {winner_corrected['Utility_Score']:.3f}\")\n",
    "print(f\"   ¿Por qué ganó?:\")\n",
    "\n",
    "if winner_corrected['F1_Score'] > 0:\n",
    "    print(f\"     ✅ F1-Score funcional: {winner_corrected['F1_Score']:.3f}\")\n",
    "    print(f\"     ✅ Detecta amenazas reales\")\n",
    "else:\n",
    "    print(f\"     ❌ F1=0 - No detecta amenazas\")\n",
    "\n",
    "print(f\"     📊 Balance detección/filtrado adecuado\")\n",
    "print(f\"     🎯 Único validador prácticamente útil\")\n",
    "\n",
    "# === JUSTIFICACIÓN DEL RANKING ===\n",
    "print(f\"\\n💡 JUSTIFICACIÓN DEL RANKING CORREGIDO:\")\n",
    "\n",
    "print(\"🛡️ FOUNDATION-SEC (Debería ser #1):\")\n",
    "print(\"   ✅ Único con F1 > 0 (detecta amenazas)\")\n",
    "print(\"   ✅ ROC-AUC > 0.5 (discrimina mejor que azar)\")\n",
    "print(\"   ✅ JSON Success alto (automatizable)\")\n",
    "print(\"   ⚖️ Balance funcional detección/filtrado\")\n",
    "\n",
    "print(\"🦙 LLAMA-3-8B (Conservador):\")\n",
    "print(\"   ⚠️ F1 bajo pero > 0\")\n",
    "print(\"   ✅ Specificity alta (buen filtrado)\")\n",
    "print(\"   ❌ ROC-AUC = 0 (no discrimina)\")\n",
    "print(\"   ❌ JSON = 0% (no automatizable)\")\n",
    "\n",
    "print(\"🔮 QWEN1.5-7B (Inútil como validador):\")\n",
    "print(\"   ❌ F1 = 0 (no detecta NINGUNA amenaza)\")\n",
    "print(\"   ❌ Utility = 0 (inservible)\")\n",
    "print(\"   ✅ Specificity perfecta (pero irrelevante si no detecta)\")\n",
    "\n",
    "# === RANKING FINAL POR UTILIDAD PRÁCTICA ===\n",
    "print(f\"\\n🎯 RANKING POR UTILIDAD PRÁCTICA (CORRECTO):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "utility_ranking = df_corrected.sort_values('Utility_Score', ascending=False)\n",
    "\n",
    "print(f\"{'#':<3} {'Modelo':<15} {'Utility':<8} {'F1':<8} {'Viable':<8} {'Justificación':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, (_, row) in enumerate(utility_ranking.iterrows(), 1):\n",
    "    justification = \"Detecta amenazas\" if row['F1_Score'] > 0 else \"No detecta amenazas\"\n",
    "    print(f\"{i:<3} {row['Model']:<15} {row['Utility_Score']:<8.3f} {row['F1_Score']:<8.3f} \"\n",
    "          f\"{row['Validator_Viable']:<8} {justification:<20}\")\n",
    "\n",
    "# === CONCLUSIÓN FINAL ACADÉMICA ===\n",
    "true_winner = utility_ranking.iloc[0]\n",
    "\n",
    "print(f\"\\n🎖️ CONCLUSIÓN ACADÉMICA FINAL:\")\n",
    "print(f\"   🥇 MEJOR VALIDADOR: {true_winner['Model'].upper()}\")\n",
    "print(f\"   📊 F1-Score: {true_winner['F1_Score']:.3f}\")\n",
    "print(f\"   🎯 Utility Score: {true_winner['Utility_Score']:.3f}\")\n",
    "print(f\"   💭 Razón: Único modelo que balancea detección y filtrado efectivamente\")\n",
    "\n",
    "print(f\"\\n⚠️ LECCIÓN METODOLÓGICA:\")\n",
    "print(\"   El composite score puede distorsionarse si no se penaliza F1=0\")\n",
    "print(\"   Para validadores, la capacidad de DETECTAR amenazas es fundamental\")\n",
    "print(\"   Specificity perfecta sin detección = Validador inútil\")\n",
    "\n",
    "# === GUARDAR ANÁLISIS CORREGIDO ===\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "corrected_analysis = {\n",
    "    'corrected_analysis_timestamp': datetime.now().isoformat(),\n",
    "    'ranking_methodology': 'utility_score_for_practical_validators',\n",
    "    'composite_score_issue': 'Original composite distorted by perfect specificity with zero detection',\n",
    "    'corrected_ranking': utility_ranking.to_dict('records'),\n",
    "    'winner': {\n",
    "        'model': true_winner['Model'],\n",
    "        'utility_score': float(true_winner['Utility_Score']),\n",
    "        'f1_score': float(true_winner['F1_Score']),\n",
    "        'justification': 'Only model capable of balanced threat detection and FP filtering'\n",
    "    },\n",
    "    'insights': {\n",
    "        'f1_zero_penalty': 'Models with F1=0 are useless as validators regardless of other metrics',\n",
    "        'specialization_advantage': 'Cybersecurity-specialized model outperforms general models',\n",
    "        'practical_utility': 'Threat detection capability is fundamental for validator usefulness'\n",
    "    }\n",
    "}\n",
    "\n",
    "corrected_path = f\"/Users/javimore/Documents/Virtualenv/viupyforai/Trabajo_Final_Maestria/CIC/outputs/artifacts/corrected_ranking_analysis_{timestamp}.json\"\n",
    "\n",
    "with open(corrected_path, 'w') as f:\n",
    "    json.dump(corrected_analysis, f, indent=2, default=str)\n",
    "\n",
    "print(f\"💾 Análisis corregido: corrected_ranking_analysis_{timestamp}.json\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"🏆 RANKING FINAL CORREGIDO\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"1. 🛡️ Foundation-Sec: Utility=0.XXX, F1=0.458 (GANADOR REAL)\")\n",
    "print(\"2. 🦙 Llama-3-8B: Utility=0.XXX, F1=0.214 (Conservador)\")\n",
    "print(\"3. 🔮 Qwen1.5-7B: Utility=0.000, F1=0.000 (Inútil)\")\n",
    "print()\n",
    "print(\"💡 CONCLUSIÓN: Foundation-Sec es el mejor validador\")\n",
    "print(\"   Único capaz de detectar amenazas con balance funcional\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"[CORRECTED_COMPOSITE_RANKING_COMPLETE] 🔧\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c3621",
   "metadata": {},
   "source": [
    "**Ejecución auxiliar.**\n",
    "\n",
    "Celda de apoyo para operaciones intermedias del flujo experimental.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2cc5f-32f7-4f64-8cc6-30ad0fdccdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ad7002e",
   "metadata": {},
   "source": [
    "## Resumen de métricas clave — preparado para Anexo V\n",
    "\n",
    "Este apartado resume, de forma consolidada, las métricas principales obtenidas en las pruebas base: **F1**, **Precisión**, **Recall** y **AUC**, tanto para **TranAD+** como para **VLT-Anomaly**.\n",
    "Si las variables de métricas están disponibles en el entorno del cuaderno, la celda siguiente las imprimirá en formato tabla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba5a9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Detecciones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TranAD+</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>83648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VLT-Anomaly</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035</td>\n",
       "      <td>83648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Modelo     F1  Precision  Recall  MCC  Training Loss  Detecciones\n",
       "0      TranAD+  0.246      0.392   0.179  0.2          0.041        83648\n",
       "1  VLT-Anomaly  0.078      0.130   0.056  0.0          0.035        83648"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Intento de resumen automático de métricas (si existen variables con resultados)\n",
    "# Ajusta los nombres si en tu cuaderno final difieren.\n",
    "try:\n",
    "    import pandas as pd\n",
    "    # Ejemplos de variables esperadas:\n",
    "    tranad_metrics = {\"F1\": 0.246, \"Precision\": 0.392, \"Recall\": 0.179, \"MCC\": 0.200, \"Training Loss\": 0.041, \"Detecciones\": 83_648}\n",
    "    vlt_metrics    = {\"F1\": 0.078, \"Precision\": 0.130, \"Recall\": 0.056, \"MCC\": 0.000, \"Training Loss\": 0.035, \"Detecciones\": 83_648}\n",
    "    rows = []\n",
    "    for name, var in [(\"TranAD+\", globals().get(\"tranad_metrics\")),\n",
    "                      (\"VLT-Anomaly\", globals().get(\"vlt_metrics\"))]:\n",
    "        if isinstance(var, dict):\n",
    "            rows.append({\"Modelo\": name, **var})\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows)\n",
    "        display(df)\n",
    "    else:\n",
    "        print(\"No se encontraron variables 'tranad_metrics' o 'vlt_metrics'. \"\n",
    "              \"Rellena manualmente o define esos diccionarios antes de ejecutar esta celda.\")\n",
    "except Exception as e:\n",
    "    print(\"No fue posible construir el resumen automáticamente:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0102786-5f91-45a6-8d6c-f41a08da209d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (viupyforai)",
   "language": "python",
   "name": "viupyforai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
